id	sentiment	review
EN13	0	﻿Digital technology has become widespread and encompasses virtually all aspects of our everyday lives. We could see it being used in computers and related gadgets, entertainment, automation (robotics), medical etc. Though physical quantities measured in the real world are analogue, most of these are processed by digital means. In order to do this, we have to convert the measured analogue quantity into digital, process the digital quantity using digital circuitry and then reconvert to analogue. The contents of this book concentrate on the digital circuit design to enable the processing of the digital quantity. But before we look into the principles of such designs, we need to understand the basics of number systems. Decimal number system is the commonly used number system that has ten digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. It is also known as base (or radix) ten system since it has ten digits that can be used to represent any number. Figure 1.1 shows the positional values or weights of the decimal number system for an integer. The digit with least weight (i.e. the one on the foremost right) is known as the least significant digit (LSD) while the highest weight digit is known as the most significant digit (MSD). In the example shown in Figure 1.1, the MSD is digit 6 while the LSD is digit 3. Figure 1.2 shows the case for fractional decimal number. While decimal number system is the commonly used number system in everyday lives, digital devices uses only binary number system that consists of 0 and 1. The base is two for this system and Figure 1.3 show an example of binary number for decimal equivalent of 6.2510 Similarly, octal and hexadecimal (hex in short) number systems have number bases of 8 and 16. For octal number system, the eight digits are 0, 1, 2, 3, 4, 5, 6, and 7 while hexadecimal number system has 16 digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, and F. Figure 1.4 gives examples on these number systems. It is often necessary to convert a number from one base system to another. Converting a number to decimal is rather straightforward as we have seen in the previous examples. The weights or positional values (for the appropriate base) are multiplied with the digit and summed to give the decimal value. In this section, we will look at methods to convert numbers from decimal to binary, octal and hex. Other conversions such as octal to binary (and vice versa), binary to hex, hex to binary, octal to hex and hex to octal are also possible. There are two methods that can be used to achieve decimal to binary conversion. The first method is by presenting the decimal value in units, tens, hundreds etc. For example: The problem with this method is that certain positional values (such as 22 and 20 in the example above) can easily be forgotten. There is another method called repeated division that is more frequently employed. Figure 1.5 illustrates this method. It works by repeated division with a value of 2 (until the quotient is 0) and the remainder digits from each step represent the binary number (in reverse order). Similarly, we can convert a decimal number to octal and hex. Figures 1.6 and 1.7 illustrate the steps for these conversions. Do remember that the final answer is in the reverse order! Any binary number can be converted to octal simply by grouping them in groups of three digits. For example, 1001011108 can be converted to 4568 as shown in Figure 1.8 (a). The reverse procedure of converting an octal number to binary can be done by writing three binary digit equivalent for each octal digit. This is shown in Figure 1.8 (b). Similar to octal number, binary number can be converted to hex simply by grouping them in groups of four digits. For example, 100101112 can be converted to 9716 as shown in Figure 1.9 (a). A hex number can be converted to binary by writing four binary digit equivalent for each hex digit. This is shown in Figure 1.9 (b). In this section, several other commonly used codes will be discussed. ASCII stands for American Standard Code for Information Interchange. Characters such as ‘a’, ‘A’, ‘@’, ‘$’ each have a code that is recognised by the computer. Standard ASCII has 128 characters (represented by 7 binary digits; 27=128), though the first 32 is no longer used. Extended ASCII has another 128 characters, mostly to represent special characters and mathematical symbols such as ‘ÿ’, ‘ė’, ‘Σ’, and ‘σ’. Table 1.1 shows the standard ASCII code. BCD is actually a set of binary numbers where a group of four binary numbers represent a decimal digit. As there are 10 basic digits in the decimal number system, four binary digits (bits) are required1. Figure 1.10 shows an example, while Table 1.2 gives the BCD code. Gray code is another commonly encountered code system. The main feature of this code is that only one bit changes between two successive values. This system is less prone to errors and is considered very useful for practical applications such as mechanical switches and error correction in digital communication as compared to the standard binary system. Table 1.3 gives the BCD code with 4 bits (i.e. up to decimal value of 15). The basic building blocks for digital circuits are logic gates. Most logic gates are binary logic, i.e. have two states of 0 or 1. The input or output of these logic gates can only exist in one of these states, where a positive logic system treats 0 as FALSE value and 1 as TRUE value and conversely for the negative logic system. Figure 2.1 shows a logic waveform that is logic 1 between time t1 and t2 and is logic 0 at other times. Positive logic will be assumed throughout the book except where denoted otherwise. ﻿Basically AND gate is composed of two inputs and a single output as shown in Figure 2.5 with algebraic representation4 BAF⋅=or simply . The traditional symbol shown in Figure 2.5(a) is more commonly employed in text books. However, the IEEE/ANSI symbol as shown in Figure 2.5(b) is gaining popularity and has the advantage of containing qualifying symbols inside the logic-symbol that describes the operation of the gate. The truth table that gives the output F for inputs A and B is given in Table 2.1. It can be seen that the output is LOW (FALSE) when any one of the inputs is LOW (FALSE) and the output is only HIGH (TRUE) when all the inputs are HIGH (TRUE). AND gate inputs do not have to be limited to two; there can be any number of inputs greater than one as shown in Figure 2.6. Timing diagram is useful in describing the relationship between the inputs and output of a logic gate. The inputs of a digital logic gate can be shown diagrammatically as a waveform that represents the changing values over time. A waveform corresponding to the changing values of the inputs over time will be generated at the output of the logic gate. Figure 2.7 show examples of timing diagram waveform for equal and unequal mark-space cycles. The mark represents the time for logic level HIGH, while the space represents the time for logic level LOW. Equal mark-space requires periodic clock pulse5. All the discussion in this book will be using equal mark-space timing waveforms only. Figure 2.8 shows an example of a timing diagram for a two-input AND gate. At each time block, the inputs A and B affect the output F. For example, in time block t0 to t1, both inputs are LOW, so the output is also LOW. Similarly, the entire timing waveform for the output can be obtained using AND operation of inputs in each time block. OR gate as shown in Figure 2.9 has algebraic representation,BAF+=. The truth table that gives the output F for inputs A and B is given in Table 2.2. It can be seen that the output is HIGH when any one of the inputs is HIGH and the output is only LOW when all the inputs are LOW. Similar to AND gate, there can be any number of inputs greater than one as shown in Figure 2.10. Figure 2.11 shows an example of a timing diagram for a two-input OR gate. At each time block, the inputs A and B affect the output F. For example, in time block t5 to t6, one input is HIGH, so the output is HIGH. Similarly, the entire timing waveform for the output can be obtained using OR operation of inputs in each time block. NOT gate is also known as INVERTER as it inverts (complements) the input logic level. It is shown in Figure 2.12 and has only one input and one output with algebraic representation of AF= or 'AF=. The bubble in the symbol denotes inversion (without it, the symbol will represent a buffer gate that does not alter the logic level; in IEEE/ANSI standard, the bubble is replaced by a triangle). The truth table for NOT gate is given in Table 2.3. NOT gate can also be connected in cascade and a few examples are shown in Figure 2.13. It should be obvious that odd number of NOT gate connections give output logic level that is complement to the input logic level and an even number of NOT gates connections give output logic level that is the same as the input logic level. It is useful to know that AND gate logic can be easily implemented using OR gate and vice versa through a simple process using additional NOT gates. For example, an AND gate equivalent can be constructed with an OR gate with both the inputs and outputs inverted through NOT gates. Figure 2.14 shows an example with equivalent truth table in Table 2.4. This is actually DeMorgan’s first theorem, which will be discussed in detail in Chapter Three. It is mentioned here so that the reader is aware that it is possible to implement one gate logic with another gate(s). NAND and NOR gates that will be discussed in the following section are known as universal gates as combinations of these gates are sufficient to obtain equivalent operation of OR, AND or NOT gates. However, this is different to the implementation discussed in Section 2.4 as either NAND or NOR gates on their own will be sufficient to implement logic function of any of the other gates. NAND gate logic symbol is shown in Figure 2.15 (note the addition of a bubble when compared to AND gate) and its truth table is shown in Table 2.5. A NAND gate operation can also be obtained through cascade operation of AND and NOT gates as shown in Figure 2.16. Algebraically, the operation can be defined as. Figure 2.17 shows an example for implementing an AND gate using NAND gates only. The blue shaded tiny bubble represents branch-off of the signal and should not be confused with the empty bubble that is used to represent inversion operation. Similarly, other gates such as OR and NOT can be implemented using NAND gates and these are left as exercises for the reader. NOR gate is basically an OR gate with the output inverted. Figure 2.18 shows the logic symbol with truth table shown in Table 2.6. Algebraically, the operation can be defined asBAF+=. Similar to NAND gate, several NOR gates can be used to implement AND, OR or NOT gates. An example of this is shown in Figure 2.19 and the reader can easily verify through the use of truth tables that All the gates that we have discussed in this chapter are manufactured as integrated circuit (IC) with several gates in one IC. For example, 74LS00 is a transistor-transistor logic (TTL) technology based IC that has four (quad) two-input NAND gates. Complementary Metal-Oxide Semiconductor (CMOS) is another technology that is widely used for manufacturing IC but TTL devices are more commonly employed for laboratory experiments as they are more robust to electrostatic noise. Figure 2.20 shows the pin configuration of 74LS00 and Figure 2.21 shows an example of pin configurations to implement NOT operation. ﻿In the previous chapter, operation and truth tables of single gates were discussed. However, in practise, single gates are seldom useful and combinations of several gates are employed for a particular application. For example, see Figure 3.1 where different gates are used to obtain the output F. Very often, there is the need to simplify logic circuits (whenever possible). For example, the circuit shown in Figure 3.1 requires four gates but equivalent logic output can be obtained with just two gates by simplifying the expression as follows: BBA is zero due to the presence of BB as shown in the truth table given in Table 3.1. The simplified circuit is given in Figure 3.2. Table 3.2 gives the truth table and it can be seen that the outputs given by expressions )(CBBAF+=and CBAF= are the same. The above simplification may not be clear at this stage but that will be the purpose of the following sections to look into Boolean algebra that will be useful to simplify logic circuits. Not only will the simplification result in lower cost, smaller and simpler design (since fewer gates will be used), it will also reduce other complications such as overheating and propagation delay. Basic axioms of Boolean algebra are shown in Table 3.3, while Table 3.4 shows the Boolean theorems for operation of a single variable and a constant (either 0 or1). Boolean algebra satisfies commutative and associative laws. Therefore, the order of variables in a product or sum does not matter and the order of evaluating sub-expression in brackets does not matter. For example: Boolean algebra also satisfies the distributive law where the expression can be expanded by multiplying out the terms. For example: It should be evident by now that when an expression contains AND and OR, AND operator takes precedence over OR operator. DeMorgan’s theorem is very useful to simplify expressions when they contain a bar (inversion) over more than a single variable. It states that an inverted expression can be replaced by its individual inverted variables but with AND replaced by OR and vice versa. For example: The following examples show the usefulness of using DeMorgan’s theorem. Note that from now on, the use of óeò˜AND (⋅) sign in the expression will be dropped for simplicity sake unless noted otherwise, so CBAF⋅⋅=will be written as . In this section, several examples are given to illustrate simplification using Boolean algebra and DeMorgan’s theorem: As another example, consider the circuit diagram given in Figure 3.4 which can be simplified as The correctness of the simplified expression can be verified by constructing a truth table and comparing the output from both expressions. The simplified logic circuit diagram is shown in Figure 3.5 where only five gates are required as opposed to six gates in the original circuit. It can be seen that there is no input A as its logic value does not affect the output based on the simplified expression. While the expression for the logic circuit shown in Figure 3.5 is simplified to single literals, it is interesting to note that another equivalent logic circuit shown in Figure 3.6 only requires four gates asDCBDCB If complement inputs are available, then the simplified circuit shown in Figure 3.5 will only require two gates as shown in Figure 3.7. To conclude the chapter, it is useful to look at two more frequently used gates: Exclusive OR (XOR) and Exclusive NOR (XNOR). These gates would be useful when circuitry such as half adders and full adders are discussed in later chapters. XOR gate as shown in Figure 3.8 has algebraic representation,BABAF+=or more commonly written as BAF⊕=. The truth table that gives the output F for inputs A and B is given in Table 3.5. It can be seen that when both inputs have the same logic value, the output is LOW. The output is HIGH when the input logic values are dissimilar, i.e. one LOW and one HIGH. XNOR gate is simply XOR with an inversion. The gate is shown in Figure 3.9 and has algebraic representation, The truth table is given in Table 3.6. The output is HIGH when both inputs have the same logic value. The output is LOW when the input logic values are dissimilar, i.e. one LOW and one HIGH. Table 3.7 shows the Boolean algebra for XOR operation. XOR operation is also both commutative and associative: ABBA⊕=⊕ and CBACBACBA⊕⊕=⊕⊕=⊕⊕)()(. As mentioned earlier, XOR gates are useful when designing more advanced circuitry such as adders, but these are also used in parity checker devices. Parity checker is used to reduce errors from transmitting a binary code across a communication channel. For example, if the seven bit ASCII code for W, 1010111 (see Table 1.1) is to be transmitted, an eight parity bit is appended at the beginning of the code. This parity bit will either be 0 or 1 depending on whether even or odd parity is required. Assuming that it is even parity checker, then the total number of bits will be even. In this case, the parity bit will be 1 and code to be transmitted will be 11010111. XOR gates can be used as even parity checker. For example, with three inputs, the expression will be CBAF⊕⊕= and the output is HIGH if one of the inputs or all three inputs are HIGH. Similarly, for eight inputs, the output is HIGH when odd number of inputs is HIGH. Figure 3.10 shows the logic circuit using seven two-input XOR gates where the bits representing the code are A0, A1,…., A6 and the parity bit is P. The output F will be HIGH when odd number of inputs is HIGH. So if the code is not transmitted correctly (say resulting in odd number of 1s), then the LED will light-up to show that an error has occured. On the other hand, with correct transmission, the number of 1s will be even and the output will be low (i.e. LED will not light-up). It should be obvious that XNOR gates can be used as odd parity checker as the output will be HIGH only when even number of inputs is HIGH. ﻿For a digital signal x[n], the discrete Fourier transform (DFT) is defined as where the DFT X[k] is a discrete periodic function of period N. Therefore one period of distinct values are only taken at k = 0,1,2,..., N −1. Note that the DFT Eq.(6.1) only has defined the transform over 0 ≤ n ≤ N −1, otherwise not known or not cared. This is different from Fourier series in which the signal is strictly periodic or the discrete version of Fourier transform in which the signal is non-periodic but defined over − ∞ < n < ∞. The comparison is made in Table 6.1. The DFT matches most of the practical cases in which only limited record is available from a certain measurement. The inverse discrete Fourier transform (IDFT) is where x[n] is a periodic function of period N. Distinct values can be taken from one period at n = 0,1,2,..., N −1 . For this reason, we can assume the original signal x[n] in the above DFT formula has been extended to a signal of periodic of N. i.e., the DFT considers a non-periodic signal x[n] to be periodic only for the purpose of mathematic convenience. Otherwise, the summation in the DFT formula is not to run for N samples 0 ≤ n ≤ N −1 but the whole axis − ∞ < n < ∞ and to obtain a continuous spectrum X (W) . Table 6.1 has listed the definitions of Fourier series, discrete version of Fourier Transform and DFT for comparison. In the following Figure 6.2, the difference between the DFT and discrete version of FT are compared. The upper left is a non-periodic signal with N samples in 0 ≤ n ≤ N −1 in which zeros are given to all outside the N records. The upper right is its discrete version of Fourier transform which is a continuous function. The lower left is the signal in which the N samples are regarded as one period and the record has been extended to the whole axis − ∞ < n < ∞. Therefore, like Fourier series, its periodic discrete spectrum is shown in the lower right figure. In essence, applying the DFT is to decompose a periodic signal to a series of cosine and sine functions represented by where k is the frequency of the sinusoidal function which runs through all possibilities from 0 (direct current) to N-1. The following figure shows the first few sinusoidal components. where r is an arbitrary integer and N is the period. This property says that the shape of the signal stays the same when it is shifted to left or right by integer number of N samples. where ↔ represents the pair of DFT and IDFT, and A and B are constants. This property includes an equal magnification rule, and a superposition rule between the input and output. The relationship of convolution between two signals in time domain can be simplified to a multiplication in the frequency domain. In the formula, the convolution is defined on one period. Likewise to the property 4), the relationship of convolution between two spectra in the frequency domain can be simplified to a multiplication in the time domain. When x[n] is real signal, a) if x[n] is an even function, Im(X[k]) =0 (6.9) b) if x[n] is an even function, Re(X[k]) =0 (6.10) This property can be used to simplify and save the calculation. Only X[0], X[N/2] and X(k), k=1,2,N/2-1 are needed to represent the whole X[k] (k=0,1,2,…,N-1). i.e. there are a total of 2 real and N/2-1 complex coefficients. It can also be proved This property tells that the modules of the DFT is symmetrical about the vertical Line If the signal x[n] is complex, there is no spectral symmetry, and all N coefficients are distinct in general. James W. Cooley and JohnW. Tukey in 1965 made a revolutionary invention in calculating the DFT (published in J.W.Cooley and J.W. Tukey in Math. Comput., vol. 19, April 1965, pp297-301). In the algorithm known as FFT, redundancy in direct calculating complex DFT due to periodicity in sinusoidal functions has been removed, therefore the computing time has been remarkably reduced. The principle can be explained in the following. If separating x[n] to an eve and an odd sequences Eq.(6.14) means a DFT of length N can be equivalent to 2 DFTs of length N/2. As an immediate result, the number of distinct complex numbers can be reduced from N kn N W to N/2 rk N W / 2 in the above DFT summation; thus complex multiplications can be greatly reduced in computation. The heart of implementing FFT is to make above division further until each DFT has only 2 samples. A requirement is the length of data N is an integer power of 2. Therefore, N is chosen to be an integer power of 2, N/2 is even. 2 N/2-point sequences can be decomposed into 2 shorter N/4-point sequences. This decomposition continues until all sequences are 2-point sub-sequences, each of which requires only a simple 2-point DFT. This procedure produces a radix-2 FFT algorithm. For example: Let N=8, the DFT is The number of direct calculation of its DFT will be 8 64 2 , approximately. However, it can be divided into 2 length N=4 sequences: Further, they can be divided in to 4 length N=2 sequences: In original DFT, there are approximately N 2 multiplications in (there are some unities when k or n=0). However, in the FFT algorithm, redundant computation in multiplying kn N W are reduced by re-arranging samples to shorter sequences to enable multiplication by much fewer distinct 1 2 0 / 2 / 4 2 W ,W ,W ,...W and W kn N kn N kn N in a butterfly shaped flow chart. Figure 6.4 illustrates 2 N multiplications in a length N=8 DFT. In the following Figure 6.5, two boxes represents 2 length N=4 DFTs. The solid lines represent moves and the doted lines represent complex multiplications.$$$﻿For a digital signal x[n], the discrete Fourier transform (DFT) is defined as where the DFT X[k] is a discrete periodic function of period N. Therefore one period of distinct values are only taken at k = 0,1,2,..., N −1. Note that the DFT Eq.(6.1) only has defined the transform over 0 ≤ n ≤ N −1, otherwise not known or not cared. This is different from Fourier series in which the signal is strictly periodic or the discrete version of Fourier transform in which the signal is non-periodic but defined over − ∞ < n < ∞. The comparison is made in Table 6.1. The DFT matches most of the practical cases in which only limited record is available from a certain measurement. The inverse discrete Fourier transform (IDFT) is where x[n] is a periodic function of period N. Distinct values can be taken from one period at n = 0,1,2,..., N −1 . For this reason, we can assume the original signal x[n] in the above DFT formula has been extended to a signal of periodic of N. i.e., the DFT considers a non-periodic signal x[n] to be periodic only for the purpose of mathematic convenience. Otherwise, the summation in the DFT formula is not to run for N samples 0 ≤ n ≤ N −1 but the whole axis − ∞ < n < ∞ and to obtain a continuous spectrum X (W) . Table 6.1 has listed the definitions of Fourier series, discrete version of Fourier Transform and DFT for comparison. In the following Figure 6.2, the difference between the DFT and discrete version of FT are compared. The upper left is a non-periodic signal with N samples in 0 ≤ n ≤ N −1 in which zeros are given to all outside the N records. The upper right is its discrete version of Fourier transform which is a continuous function. The lower left is the signal in which the N samples are regarded as one period and the record has been extended to the whole axis − ∞ < n < ∞. Therefore, like Fourier series, its periodic discrete spectrum is shown in the lower right figure. In essence, applying the DFT is to decompose a periodic signal to a series of cosine and sine functions represented by where k is the frequency of the sinusoidal function which runs through all possibilities from 0 (direct current) to N-1. The following figure shows the first few sinusoidal components. where r is an arbitrary integer and N is the period. This property says that the shape of the signal stays the same when it is shifted to left or right by integer number of N samples. where ↔ represents the pair of DFT and IDFT, and A and B are constants. This property includes an equal magnification rule, and a superposition rule between the input and output. The relationship of convolution between two signals in time domain can be simplified to a multiplication in the frequency domain. In the formula, the convolution is defined on one period. Likewise to the property 4), the relationship of convolution between two spectra in the frequency domain can be simplified to a multiplication in the time domain. When x[n] is real signal, a) if x[n] is an even function, Im(X[k]) =0 (6.9) b) if x[n] is an even function, Re(X[k]) =0 (6.10) This property can be used to simplify and save the calculation. Only X[0], X[N/2] and X(k), k=1,2,N/2-1 are needed to represent the whole X[k] (k=0,1,2,…,N-1). i.e. there are a total of 2 real and N/2-1 complex coefficients. It can also be proved This property tells that the modules of the DFT is symmetrical about the vertical Line If the signal x[n] is complex, there is no spectral symmetry, and all N coefficients are distinct in general. James W. Cooley and JohnW. Tukey in 1965 made a revolutionary invention in calculating the DFT (published in J.W.Cooley and J.W. Tukey in Math. Comput., vol. 19, April 1965, pp297-301). In the algorithm known as FFT, redundancy in direct calculating complex DFT due to periodicity in sinusoidal functions has been removed, therefore the computing time has been remarkably reduced. The principle can be explained in the following. If separating x[n] to an eve and an odd sequences Eq.(6.14) means a DFT of length N can be equivalent to 2 DFTs of length N/2. As an immediate result, the number of distinct complex numbers can be reduced from N kn N W to N/2 rk N W / 2 in the above DFT summation; thus complex multiplications can be greatly reduced in computation. The heart of implementing FFT is to make above division further until each DFT has only 2 samples. A requirement is the length of data N is an integer power of 2. Therefore, N is chosen to be an integer power of 2, N/2 is even. 2 N/2-point sequences can be decomposed into 2 shorter N/4-point sequences. This decomposition continues until all sequences are 2-point sub-sequences, each of which requires only a simple 2-point DFT. This procedure produces a radix-2 FFT algorithm. For example: Let N=8, the DFT is The number of direct calculation of its DFT will be 8 64 2 , approximately. However, it can be divided into 2 length N=4 sequences: Further, they can be divided in to 4 length N=2 sequences: In original DFT, there are approximately N 2 multiplications in (there are some unities when k or n=0). However, in the FFT algorithm, redundant computation in multiplying kn N W are reduced by re-arranging samples to shorter sequences to enable multiplication by much fewer distinct 1 2 0 / 2 / 4 2 W ,W ,W ,...W and W kn N kn N kn N in a butterfly shaped flow chart. Figure 6.4 illustrates 2 N multiplications in a length N=8 DFT. In the following Figure 6.5, two boxes represents 2 length N=4 DFTs. The solid lines represent moves and the doted lines represent complex multiplications.
EN11	1	﻿Digital signal processing (DSP) has become a common tool for many disciplines. The topic includes the methods of dealing with digital signals and digital systems. The techniques are useful for all the branches of natural and social sciences which involve data acquisition, analysis and management, such as engineering, physics, chemistry, meteorology, information systems, financial and social services. Before the digital era, signal processing devices were dominated by analogue type. The major reason for DSP advancement and shift from analogue is the extraordinary growth and popularization of digital microelectronics and computing technology. The reason that digital becomes a trend to replace analogue systems, apart from it is a format that microprocessors can be easily used to carry out functions, high quality data storage, transmission and sophisticated data management are the other advantages. In addition, only 0s and 1s are used to represent a digital signal, noise can easily be suppressed or removed. The quality of reproduction is high and independent of the medium used or the number of reproduction. Digital images are two dimensional digital signals, which represent another wide application of digital signals. Digital machine vision, photographing and videoing are already widely used in various areas. In the field of signal processing, a signal is defined as a quantity which carries information. An analogue signal is a signal represented by a continuous varying quantity. A digital signal is a signal represented by a sequence of discrete values of a quantity. The digital signal is the only form for which the modern microprocessor can take and exercise its powerful functions. Examples of digital signals which are in common use include digital sound and imaging, digital television, digital communications, audio and video devices. To process a signal is to make numerical manipulation for signal samples. The objective of processing a signal can be to detect the trend, to extract a wanted signal from a mixture of various signal components including unwanted noise, to look at the patterns present in a signal for understanding underlying physical processes in the real world. To analyse a digital system is to find out the relationship between input and output, or to design a processor with pre-defined functions, such as filtering and amplifying under applied certain frequency range requirements. A digital signal or a digital system can be analysed in time domain, frequency domain or complex domain, etc. Representation of digital signals can be specific or generic. A digital signal is refereed to a series of numerical numbers, such as: where 2, 4, 6 are samples and the whole set of samples is called a signal. In a generic form, a digital signal can be represented as time-equally spaced data where -1, 0, 1, 2 etc are the sample numbers, x[0], x[1], x[2], etc are samples. The square brackets represent the digital form. The signal can be represented as a compact form In the signal, x[-1], x[1], x[100], etc, are the samples, n is the sample number. The values of a digital signal are only being defined at the sample number variable n , which indicates the occurrence order of samples and may be given a specific unit of time, such as second, hour, year or even century, in specific applications. We can have many digital signal examples: -- Midday temperature at Brighton city, measured on successive days, -- Daily share price, -- Monthly cost in telephone bills, -- Student number enrolled on a course, -- Numbers of vehicles passing a bridge, etc. Examples of digital signal processing can be given in the following: Example 1.1 To obtain a past 7 day’s average temperature sequence. The averaged temperature sequence for past 7 days is For example, if n=0 represents today, the past 7 days average is where x[0], x[−1], x[−2], ... represent the temperatures of today, yesterday, the day before yesterday, …; y[0] represents the average of past 7 days temperature from today and including today. On the other hand, represents the average of past 7 days temperature observed from tomorrow and including tomorrow, and so on. In a shorter form, the new sequence of averaged temperature can be written as where x[n] is the temperature sequence signal and y[n] is the new averaged temperature sequence. The purpose of average can be used to indicate the trend. The averaging acts as a low-pass filter, in which fast fluctuations have been removed as a result. Therefore, the sequence y[n] will be smoother than x[n]. Example 1.2. To obtain the past M day simple moving averages of share prices, let x[n] denotes the close price, y [n] M the averaged close price over past M days. For example, M=20 day simple moving average is used to indicate 20 day trend of a share price. M=5, 120, 250 (trading days) are usually used for indicating 1 week, half year and one year trends, respectively. Figure 1.1 shows a share’s prices with moving averages of different trading days. Although some signals are originally digital, such as population data, number of vehicles and share prices, many practical signals start off in analogue form. They are continuous signals, such as human’s blood pressure, temperature and heart pulses. A continuous signal can be first converted to a proportional voltage waveform by a suitable transducer, i.e. the analogue signal is generated. Then, for adapting digital processor, the signal has to be converted into digital form by taking samples. Those samples are usually equally spaced in time for easy processing and interpretation. Figure 1.2 shows a analogue signal and its digital signal by sampling with equal time intervals. The upper is the analogue signal x(t) and the lower is the digital signal sampled at time t = nT, where n is the sample number and T is the sampling interval. Therefore, For ease of storage or digital processing, an analogue signal must be sampled into a digital signal. The continuous signal is being taken sample at equal time interval and represented by a set of members. First of all, a major question about it is how often should an analogue signal be sampled, or how frequent the sampling can be enough to represent the details of the original signal. ﻿A digital system is also refereed as a digital processor, which is capable of carrying out a DSP function or operation. The digital system takes variety of forms, such as a microprocessor, a programmed general-purpose computer, a part of digital device or a piece of computing software. Among digital systems, linear time-invariant (LTI) systems are basic and common. For those reasons, it will be restricted to address about only the LTI systems in this whole book. The linearity is an important and realistic assumption in dealing with a large number of digital systems, which satisfies the following relationships between input and output described by Figure 3.1. i.e. a single input [ ] 1 x n produces a single output [ ] 1 y n , Applying sum of inputs [ ] [ ] 1 2 x n + x n produces [ ] [ ] 1 2 y n + y n , and applying input [ ] [ ] 1 2 ax n bx n generates [ ] [ ] 1 2 ay n by n . The linearity can be described as the combination of a scaling rule and a superposition rule. The time-invariance requires the function of the system does not vary with the time. e.g. a cash register at a supermarket adds all costs of purchased items x[n], x[n −1],… at check-out during the period of interest, and the total cost y[n] is given by where y[n] is the total cost, and if x[0] is an item registered at this moment, x[−1] then is the item at the last moment, etc. The calculation method as a simple sum of all those item’s costs is assumed to remain invariant at the supermarket, at least, for the period of interest. Like a differential equation is used to describe the relationship between its input and output of a continuous system, a difference equation can be used to characterise the relationship between the input and output of a digital system. Many systems in real life can be described by a continuous form of differential equations. When a differential equation takes a discrete form, it generates a difference equation. For example, a first order differential equation is commonly a mathematical model for describing a heater’s rising temperature, water level drop of a leaking tank, etc: where x[n] is the input and y[n] is the output. For digital case, the derivative can be described as i.e. the ratio of the difference between the current sample and one backward sample to the time interval of the two samples. Therefore, the differential equation can be approximately represented by a difference equation: yielding a standard form difference equation: For input’s derivative, we have similar digital form as Further, the second order derivative in a differential equation contains can be discretised as When the output can be expressed only by the input and shifted input, the difference equation is called non-recursive equation, such as On the other hand, if the output is expressed by the shifted output, the difference equation is a recursive equation, such as where the output y[n] is expressed by it shifted signals y[n −1] , y[n − 2], etc. In general, an LTI processor can be represented as or a short form A difference equation is not necessarily from the digitization of differential equation. It can originally take digital form, such as the difference equation in Eq.(3.1). Alternatively, equivalent to the difference equation, an LTI system can also be represented by a block diagram, which also characterises the input and output relationship for the system. For example, to draw a block diagram for the digital system described by the difference equation: The output can be rewrite as The block diagram for the system is shown in Figure 3.2. In the bock diagram, T is the sampling interval, which acts as a delay or right-shift by one sample in time. For general cases, instead of Eq.(3.9), Eq. (3.8) is used for drawing a block diagram. It can easily begin with the input, output flows and the summation operator, then add input and output branches. Both the difference equation and block diagram can be used to describe a digital system. Furthermore, the impulse response can also be used to represent the relationship between input and output of a digital system. As the terms suggest, impulse response is the response to the simplest input – unit impulse. Figure 3.2 illustrates a digital LTI system, in which the input is the unit impulse and the output is the impulse response. Once the impulse response of a system is known, it can be expected that the response to other types of input can be derived. An LTI system can be classified as causal or non-causal. A causal system is refereeing to those in which the response is no earlier than input, or h[n] =0 before n=0. This is the case for most of practical systems or the systems in the natural world. However, non-causal system can exist if the response is arranged, such as programmed, to be earlier than the excitation. The impulse response of a system can be evaluated from its difference equation. Following are the examples of finding the values of impulse responses from difference equations Example 3.1 Evaluating the impulse response for the following systems We know that when the input is the simplest unit impulse d[n], the output response will be the impulse response. Therefore, replacing input x[n] by d[n] and response y[n] by h[n], the equation is still holding and has become special: It is easy to evaluate the impulse response by letting n=-1, 0,1,2,3,… Generally for the difference equation: The impulse response can evaluated by the special equation with the simple unit impulse input: The step response is also commonly used to characterize the relationship between the input and output of a system. To find the step response using the impulse response, we know that the unit step can be expressed by unit impulses as The linear system satisfies the superposition rule. Therefore, the step response is a sum of a series of impulse responses excited by a series of shifted unit impulses. i.e., the step response is a sum of impulse responses ﻿Consider a periodic digital signal x[n], n=0,1,2,...,N-1, where N is the number of sample values in each period. From Euler’s complex exponential equation: for each frequency k, Eq. (4.1) contains 2 sinusoidal functions in real and imaginary parts, respectively, with p/2 difference in phase. The frequencies of the functions are k=0,1,2,...,N-1. The fundamental sinusoidal function is when the frequency k=1. The other higher sinusoidal functions are called harmonics. The coefficients of Fourier Series for a digital signal can be calculated by where k a is kth spectral coefficient, indicating the strength of the kth harmonic function. The original digital signal x[n] can be represented by its constituent harmonics as the form of discrete Fourier series: The following Figure 4.1 illustrates how many complex multiplications are required in calculating those coefficients k a and from k a to obtaining x[n]. It is basically N 2 in each occasion. Example 4.1 A periodic signal x[n] = [0 2 4 6] has the period N=4, its coefficients of Fourier series can be calculated by the formula Figure 4.2 illustrates one period of the signal and its spectral coefficients. Example 4.2 A sine wave k=4 and N=64, the sine wave and its Fourier spectrum are shown in the following Figure 4.3. It can be seen that the single sine wave gives a single spectral line at frequencies of 4 hz and 64-4=60 hz, or The Fourier series introduced in previous section is only applicable to periodic signals. In general cases, the signal is not periodic, and the Fourier series formula Eq. (4.2) is not applicable. Therefore, a Fourier Transform for non-periodic signals must be introduced. Let frequency (rad/sample) changes continuously between 0 and 2p. This can be achieved by assuming N is big enough and the fundamental frequency is very small. Define a Fourier transform as where X (W) is the Fourier spectrum, which is a continuous function, therefore, usual round brackets are used. It is also a periodic function with a period of 2p as sinusoidal functions are multiplied in the transform. Assuming N is big and using Eq.(4.3), the inverse Fourier transform can be defined: Consider digital LTI systems, take Fourier transform for the input, impulse response and output: where H(W) is known as the frequency response, which describes the gain of a system at different frequencies and can be obtained by taking Fourier transform to the impulse response. The input-output relationship in the time and frequency domains, and the time-frequency relationships of the three quantities are shown in Figure 4.4. Using the convolution property of the Fourier transform, for the input-output relationship apply the Fourier transform on both sides, it can be obtained The frequency response can be obtained from the transforms of input and output: Find the frequency response and sketch its magnitude and phase over the range 0<W<p. Solution: Re-arrange the difference equation as Let x[n] be a signal discretized with a sampling rate of fs Hz, we are about to find the position of peak for a complex harmonic component with frequency in the frequency Ω (rad/sample) domain. Its Fourier transform is given by In the previous chapters, time domain and frequency domain analysis have been introduced. In each of those domains different insights of digital signals are revealed. It is useful to introduce another domain: the z domain. A digital time signal can be transferred into z domain by the z-transform. The z-transform is defined as where z is a complex variable. The transform defined by Eq. (5.1) is a unilateral transform as defined on one side of the axis 0 ≤ n < ∞. In the transform, each sample x[n] is multiplied by the complex variable z −n , i.e. There is advantage in this unilateral transform definition as it can avoid mathematical inconvenience. One can shift the signal of interest to obtain a required origin in its analysis, thus usually causing no trouble in applications. The inverse z-transform can be found by It involves contour integration, and further discussion is beyond the scope of this basic content. However, an alternative approach is available using partial fractions together with z-transform formulas of basic functions. Table 5.1 lists the basic properties of the z transform and Table 5.2 lists some basic z- transform pairs. Example 5.1 Find the z transform for a signal and reconstruct a signal from its z-transform. Let z = exp( jW) , i.e. the complex variable z is only allowed on the unit circle, the z-transform becomes a unilateral Fourier transform Obviously, apart from on the unit circle, the complex operator z can be specified into other curves or region, if necessary. Later, it will be shown the unit circle is important boundary on the z-domain. Multiplying by z implies a time advance and dividing by z, or multiplying by z −1 , is to cause a time delay. For the unit impulse, The transfer function describes the input-output relationship, or the transmissibility between input and output, in the z-domain. Applying the z-transform to the output of a system, the relationship between the z-transforms of input and output can be found: i.e., the transfer function can be obtained from the z-transforms of input and output. Alternatively, the transfer function H(z) can be obtained by applying z-transform directly to the impulse response h[n] . The relationships of input ( x[n] and X (z) ), output ( y[n] and Y(z) ) and system function ( h[n] and H(z) ) in the time and z domains are depicted in Figure 5.2. Instead of using Eq.(5.2), the inverse z-transform can be made through partial fractions. The following are examples. Example 5.2 A signal has a z-transform find the corresponding original signal x[n]. The z-transform can be represented by partial fractions as There is an alternative way of finding the coefficients for the partial fractions. For the above example, poles and zeros For the z-transform of a digital signal or a transfer function of an LTI system, generally it can be expressed as factorised form for both the$$$﻿Consider a periodic digital signal x[n], n=0,1,2,...,N-1, where N is the number of sample values in each period. From Euler’s complex exponential equation: for each frequency k, Eq. (4.1) contains 2 sinusoidal functions in real and imaginary parts, respectively, with p/2 difference in phase. The frequencies of the functions are k=0,1,2,...,N-1. The fundamental sinusoidal function is when the frequency k=1. The other higher sinusoidal functions are called harmonics. The coefficients of Fourier Series for a digital signal can be calculated by where k a is kth spectral coefficient, indicating the strength of the kth harmonic function. The original digital signal x[n] can be represented by its constituent harmonics as the form of discrete Fourier series: The following Figure 4.1 illustrates how many complex multiplications are required in calculating those coefficients k a and from k a to obtaining x[n]. It is basically N 2 in each occasion. Example 4.1 A periodic signal x[n] = [0 2 4 6] has the period N=4, its coefficients of Fourier series can be calculated by the formula Figure 4.2 illustrates one period of the signal and its spectral coefficients. Example 4.2 A sine wave k=4 and N=64, the sine wave and its Fourier spectrum are shown in the following Figure 4.3. It can be seen that the single sine wave gives a single spectral line at frequencies of 4 hz and 64-4=60 hz, or The Fourier series introduced in previous section is only applicable to periodic signals. In general cases, the signal is not periodic, and the Fourier series formula Eq. (4.2) is not applicable. Therefore, a Fourier Transform for non-periodic signals must be introduced. Let frequency (rad/sample) changes continuously between 0 and 2p. This can be achieved by assuming N is big enough and the fundamental frequency is very small. Define a Fourier transform as where X (W) is the Fourier spectrum, which is a continuous function, therefore, usual round brackets are used. It is also a periodic function with a period of 2p as sinusoidal functions are multiplied in the transform. Assuming N is big and using Eq.(4.3), the inverse Fourier transform can be defined: Consider digital LTI systems, take Fourier transform for the input, impulse response and output: where H(W) is known as the frequency response, which describes the gain of a system at different frequencies and can be obtained by taking Fourier transform to the impulse response. The input-output relationship in the time and frequency domains, and the time-frequency relationships of the three quantities are shown in Figure 4.4. Using the convolution property of the Fourier transform, for the input-output relationship apply the Fourier transform on both sides, it can be obtained The frequency response can be obtained from the transforms of input and output: Find the frequency response and sketch its magnitude and phase over the range 0<W<p. Solution: Re-arrange the difference equation as Let x[n] be a signal discretized with a sampling rate of fs Hz, we are about to find the position of peak for a complex harmonic component with frequency in the frequency Ω (rad/sample) domain. Its Fourier transform is given by In the previous chapters, time domain and frequency domain analysis have been introduced. In each of those domains different insights of digital signals are revealed. It is useful to introduce another domain: the z domain. A digital time signal can be transferred into z domain by the z-transform. The z-transform is defined as where z is a complex variable. The transform defined by Eq. (5.1) is a unilateral transform as defined on one side of the axis 0 ≤ n < ∞. In the transform, each sample x[n] is multiplied by the complex variable z −n , i.e. There is advantage in this unilateral transform definition as it can avoid mathematical inconvenience. One can shift the signal of interest to obtain a required origin in its analysis, thus usually causing no trouble in applications. The inverse z-transform can be found by It involves contour integration, and further discussion is beyond the scope of this basic content. However, an alternative approach is available using partial fractions together with z-transform formulas of basic functions. Table 5.1 lists the basic properties of the z transform and Table 5.2 lists some basic z- transform pairs. Example 5.1 Find the z transform for a signal and reconstruct a signal from its z-transform. Let z = exp( jW) , i.e. the complex variable z is only allowed on the unit circle, the z-transform becomes a unilateral Fourier transform Obviously, apart from on the unit circle, the complex operator z can be specified into other curves or region, if necessary. Later, it will be shown the unit circle is important boundary on the z-domain. Multiplying by z implies a time advance and dividing by z, or multiplying by z −1 , is to cause a time delay. For the unit impulse, The transfer function describes the input-output relationship, or the transmissibility between input and output, in the z-domain. Applying the z-transform to the output of a system, the relationship between the z-transforms of input and output can be found: i.e., the transfer function can be obtained from the z-transforms of input and output. Alternatively, the transfer function H(z) can be obtained by applying z-transform directly to the impulse response h[n] . The relationships of input ( x[n] and X (z) ), output ( y[n] and Y(z) ) and system function ( h[n] and H(z) ) in the time and z domains are depicted in Figure 5.2. Instead of using Eq.(5.2), the inverse z-transform can be made through partial fractions. The following are examples. Example 5.2 A signal has a z-transform find the corresponding original signal x[n]. The z-transform can be represented by partial fractions as There is an alternative way of finding the coefficients for the partial fractions. For the above example, poles and zeros For the z-transform of a digital signal or a transfer function of an LTI system, generally it can be expressed as factorised form for both the
EN23	1	﻿High-content screening can easily generate more than one Terabyte in primary images and metadata per run, that have to be stored and organized, which means an appropriate laboratory information management system (LIMS) has to be established. The LIMS must be able to collect, collate and integrate the data stream to allow at least searching and rapid evaluation of the data. After image acquisition and data transfer, image analysis will be run to extract the metadata. Further evaluation includes testing for process errors. Heat maps along with pattern recognition algorithms help to identify artefacts such as edge-effects, uneven pipetting, or simply to exclude images that are not in focus. All plates should be checked so that the selected positive and negative controls exhibit values in a pre-defined range. Further, data may be normalized against controls before further statistical analysis is run to identify putative hits. Known proteins of the pathway being screened should score, and are a good internal control for the accuracy of the assay and workflow. Hits have to be verified by going back to the original images. Further, results have to be compared between independent runs. After this, an appropriate hit verification strategy has to be applied as discussed above. Target gene expression should be confirmed, for example, by running a microarray analysis of gene expression for the given cell line. Finally, data will be compared to other internal and external data sources. Cluster analysis will assist in identifying networks and correlations. A critical aspect of high content screening is the informatics and data management solution that the user needs to implement to process and store the images. Typically multiple images are collected per microplate well at different magnifications and processed with pre-optimised algorithms (these are the software routines that analyse images, recognize patterns and extract measurements relevant to the biological application, enabling the automated quantitative comparison and ranking of compound effects) to derive numerical data on multiple parameters. This allows for the quantification of detailed cellular measurements that underlie the phenotype observed. From an image analysis perspective the following should not be overlooked when reviewing vendor offerings: the breadth of biology covered; how the software is delivered, does it run quickly, or open a script; is analysis done on-the-fly or offline; have the algorithms been fully validated with biology; the ease of exporting image files to other software packages; and access to new algorithms, is the user dependent on the supplier or is it relatively easy to develop your own or adapt existing algorithms? The key theme and piece of information repeated throughout this chapter is “partnering”. Scientific research and informatics must work together for the mutual benefit of screening like the drug discovery process. To really be part of the winning team in any organization, all areas must bring their collective expertise together and make the extra effort to understand one another and defer where there is lack of knowledge to those on the team with the experience and expertise or to seek external advises. It is necessary to start off by setting the stage concerning where laboratory computing, which includes the data management (we will discuss a bit later in the chapter), has progressed in order to gain the necessary understanding of where it currently is and where we anticipate it will be going in the HCS area in the future. A goal of this chapter is to provide an overview of the key aspects of informatics tools and technologies needed for HCS, including characteristics of HCS data; data models/structures for storing HCS data; HCS informatics system architectures, data management approaches, hardware and network considerations, visualization, data mining technologies, and integrating HCS data with other data and systems. HCS systems scan a multiwell plate with cells or cellular components in each well, acquire multiple images of cells, and extract multiple features (or measurements) relevant to the biological application, resulting in a large quantity of data and images. The amount of data and images generated from a single microtiter plate can range from hundreds of megabytes (MB) to multiple gigabytes (GB). One large-scale HCS experiment, often resulting in billions of features and millions of images that needs multiple terabytes (TB) of storage space. High content informatics tools and infrastructure is needed to manage the large volume of HCS data and images. There are many rules that are common for the image based HCS informatics infrastructure in academic or non academic organization. Answering the following questions analyzed by entire organization tells one exactly which strategy and organization setup has to be taken and what type of work has to assign to experts and researchers. In choosing the strategy and organization setup one needs to answer the following questions: • Is the required analysis software available off-the-shelf or must it be written in-house? This decision has to be taken in collaboration between IT and scientists, based on the defined requirements. • What kind of data will be acquired (how many screens in year)? • How is the data stored, managed, and protected for short-, medium-, and long-term use? • What type of desktop clusters and servers are required for HCS computing? (brand, type, speed, and memory) • How do the computer systems interface with the necessary data collection instrumentation and connect to the network and servers at the same time? • Can allowances and accommodations be made for external collaborations and programs shared among scientists? • Are we interested in setup a safety buffered zone outside of our firewalls to allow this external data exchange? After analysis of those questions one would think to have dedicated IT person from IT department working together with the scientists to allow IT professionals to take over responsibility for informatics tasks. The side-by-side person would allow the informatics organization to understand needs of HCS unit. For example the servers processes could be placed inside of HCS pipeline or infrastructure and not be placed as usual and forced to add extra steps to the workflow. It is also important to decide what will be operated by informatics department and what by HCS unit within organization. It makes better sense for informatics department to ﻿Within the last few years a large number of tools and softwares dealing with different computational problems related to HCS have been developed. Incorporating third party or new tools into existing frameworks needs a flexible, modular and customizable workflow framework. Workflow (Pipeline) systems could become crucial for enabling HCS researchers doing large scale experiments to deal with this data explosion. The workflow is termed abstract in that it is not yet fully functional but the actual components are in place and in the requisite order. In general, workflow systems concentrate on the creation of abstract process workflows to which data can be applied when the design process is complete. In contrast, workflow systems in the life sciences domain are often based on a data-flow model, due to the data-centric and data-driven nature of many scientific analyses. A comprehensive understanding of biological phenomena can be achieved only through the integration of all available biological information and different data analysis tools and applications. In general, an ideal workflow system in HCS can integrate nearly all standard tools and software. For example, for an HCS using small molecules, the workflow system must be able to integrate different image processing software and data mining toolkits with flexibility. The possibility that any single software covers all possible domains and data models is nearly zero. No one vendor or source can provide all the tools needed by HCS informatics. So it is suggested that one uses specialized tools from specialized sources. Also not all softwares components can be integrated with all workflow systems. Workflow environment helps also HCS researchers to perform the integration themselves without involving of any programming. A workflow system allows the construction of complex in silico experiments in the form of workflows and data pipelines. Data pipelining is a relatively simple concept. Visual representation of the workflow process logic is generally carried out using a Graphical User Interface where different types of nodes (data transformation point) or software components are available for connection through edges or pipes that define the workflow process. Graphical User Interfaces provide drag and drop utilities for creating an abstract workflow, also known as “visual programming”. The anatomy of a workflow node or component (Fig. 3) is basically defined by three parameters: input metadata, transformation rules, algorithms or user parameters and output metadata. Nodes can be plugged together only if the output of one, previous (set of) node(s) represents the mandatory input requirements of the following node. Thus, the essential description of a node actually comprises only in -and output that are described fully in terms of data types and their semantics. The user can create workflows using any combination of the available tools, readers, writers or database connections in workflow system by dragging/dropping and linking graphical icons. The component properties are best described by the input metadata, output metadata and user defined parameters or transformation rules. The input ports can be constrained to only accept data of a specific type such as those provided by another component. An HCS workflow design is best carried out in phases. In the first phase, a conceptual workflow is generated. A conceptual workflow, as the name suggests, is a sequential arrangement of different components that the user may require to accomplish the given task. It is possible that some of those steps may in turn be composed of several sub components. The next phase converts the conceptual workflow into an abstract workflow by performing a visual drag and drop of the individual components that were figured to be a part of the workflow in the first phase. The workflow is termed abstract in that it is not yet fully functional but the actual components are in place and in the requisite order. In general, workflow systems concentrate on the creation of abstract process workflows to which data can be applied when the design process is complete. HCS screening workflows are based on a dataflow which integrate most of the available, standard software tools (either commercial or public domain) along with different classes of programmable toolkits. As an example, Figure 3 shows a workflow designed to be run by the HCDCKNIME Workflow Management System ( http://hcdc.ethz.ch). This workflow is used by HCS facilities. It obtains RNAi from databases, annotates them, make dilutions steps, barcode handling, split volume. In this case, the tasks, also known as steps, nodes, activities, processors or components, represent either the invocation of a remote Web service (the databases), or the execution of a local recalculation. Data-flows along data links from the outputs of a task to the inputs of another, is prepared according to a pre-defined graph topology. The workflow defines how the output produced by one task is to be consumed by a subsequent task, a feature referred to as orchestration of a flow of data. Any computational component or node has data inputs and data outputs. Data pipelining views these nodes as being connected together by ‘pipes’ through which the data flows (Figure 4). Workflow technology is a generic mechanism to integrate diverse types of available resources (databases, microscopes, servers, software applications and different services) which facilitates data exchange within screening environment. Users without programming skill can easily incorporate and access diverse instruments, image processing tools and produced data to develop their own screening workflow for analysis. In this section, we will discuss the usage of existing workflow systems in HCS and the trends in applications of workflow based systems. Many free and commercial software packages are now available to analyse HCS data sets using statistical method or classification, although it is still difficult to find a single off-the-shelf software package that answers all the questions of HCS analysis. Statistical open source software packages such as BioConductor (www.bioconductor.org) provide large collections of methods suitable for HCS data analysis. However, their command-line usage can be too demanding for users without adequate computer knowledge. As an alternative, software packages where users can upload their data and receive their processed results are becoming increasingly common: Weka25, CellAnalyzer4, CellHTS3, TreeView21 have all been published within the last year. Unfortunately, these services often allow only limited freedom in the choice and arrangement of processing steps. Other, more flexible tools, such as Eclipse6, KNIME13, JOpera2, operate either stand-alone or require considerable computer knowledge and extra software to run through the web. In order to make use of the vast variety of data analysis methods around, it is essential that such an environment is easy and intuitive to use, allows for quick and interactive changes to the analysis process and enables the user to visually explore the results. ﻿How best to archive and mine the complex data derived from HCS experiments that provides a series of challenges associated with both the methods used to elicit the RNAi response and the functional data gathered? To enable effective data retrieval for HCS experiments, data and images and associated information must be stored with high integrity and in a readable form. HCS data should be stored in a form that takes advantage of the characteristics of this type of data to enable full access, analysis and exploitation of the data. A key factor is the database model which represents data in logical form. The data model (or database structure or database schema) should be flexible to handle the various HCS data types (i.e., compound information, results: image data and derived metadata), experiment simulation and a wide range of changes in the data (e.g., different numbers of wells, cells, features, images, different image sizes and formats, different number of time-points, and so on). The structure of the data model provides a way of describing data and the relationships within the data, enabling data to be organized, cataloged, and searched effectively. Databases where a database model is implemented enable joining of related data to allow meaningful visualization, analysis, and data mining. The data model is also important for integration with other systems. HCS data are containing three types of data: 1. Database of compounds (RNAi or small molecules). 2. Numbers of images that require significant amounts of storage. 3. Numbers of files including image processing parameters. 4. Meta-data. Thus, a large amount of data is collected for just one well of a single plate. In addition, other associated information about the assay or experiment, such as protocol information, is also typically recorded. Having four types of data is easy to define three general categories of HCS data: - Image data: These are the images acquired at each channel for each field within a well and produced thumbnails for visualization purposes - Numeric Results data: these are the measurements that result from performing an analysis on an image with image analysis algorithms. - Metadata: These are the associated data that provide context for the other two categories of data (i.e., metadata are data that describes other data). Examples are: well – compound annotation, assay type, plate information, protocols, operators, calculated data such as dose–response values, as well as annotations imported from other systems. Let’s try to understand how data are produced. HCS microscopes typically scan multiwell plates. These plates typically have 96, 384, or 1536 wells. Each well is a container in the plate that contains an individual sample of cells. Each well is divided into multiple fields. Each field is a region of a well that represents an area to image. Each field consists of multiple images, one for each individual wavelength of light (referred to as a “channel”, “staining”), corresponding to the fluorescent markers/probes used for the biology/dye of interest (e.g., DAPI). There are typically between two and four channels per field (e.g., each channel shows different elements of the cells: 1 channel nuclei, 2 channel: cell membranes, 3 channel: endosomes, and so on). The images produced are immediately analyzed using automated image processing. Experiment results are produced. Each well is seeded with a certain number of cells which has to be detected by image processing algorithms. The cell number counted is a basic parameter used for the quality control of automation, microscopy or assay performance. The number of cells per well varies depending on the experiment, but typically ranges between 5 and 10000 cells. Very often images from well fields are merged into one image using montage function. For each cell, multiple object features (or measurements) are calculated by automated image processing. The cell features include measurements such as size, shape, intensity, and so on. The number of cell features calculated varies depending on the assay, but typically ranges between 5 and 500.Those features have to be carefully investigated, filtered and only parameters should be considered for hit definition. In addition, cell features are often aggregated to the well level to provide well level statistics (one well one row labeled with plate annotation and position as unique identify). The total storage size for experiments is primarily based on the acquired image data, image thumbnails, library information and the numeric results data. The amount of data, acquisition and processing time varies depending on a number of factors including the type of assay, the number of plates, the type of the screen (primary, secondary), available computational hardware, the throughput of the instrument or analysis application and the number of instruments which can work parallel. Table 2 demonstrates example experiments and summarizes necessary time, number of records and require for storage space. The size of the library information and numeric results data are counted in megabytes. Numeric results are estimated by the number of feature records (lines in tables). Image storage depends on the number of images acquired. The number of images depends on plate number, plate type (96, 384, 1536 ), number of fields, number of channels, confocality levels and eventually time points in case of kincetic studies. The typical image size acquired ranges between 500KB and 2 MB (single slice, single tiff file without montage). Thumbnails of those images often are generated using jpeg compression, their size range between 150- 300 kb. For numeric results data are categorized in three types of outputs: cell based, image based and well based. The number of image based record should be equal to the number of acquired images which is also equal to the number of thumbnails produced. The record number of well based results data should be equal to the number of all wells in screening experiment. In high content informatics, the numeric data are supported by the images and the validation of numeric data is based on the visual inspection of images. Any high content informatics solution therefore needs to be able to efficiently handle the relationships between the various levels of numeric results data, library information and the associated images. In the next subsection we will describe a database model (schema) and a database solution for handling library data, images and numeric results data.$$$﻿How best to archive and mine the complex data derived from HCS experiments that provides a series of challenges associated with both the methods used to elicit the RNAi response and the functional data gathered? To enable effective data retrieval for HCS experiments, data and images and associated information must be stored with high integrity and in a readable form. HCS data should be stored in a form that takes advantage of the characteristics of this type of data to enable full access, analysis and exploitation of the data. A key factor is the database model which represents data in logical form. The data model (or database structure or database schema) should be flexible to handle the various HCS data types (i.e., compound information, results: image data and derived metadata), experiment simulation and a wide range of changes in the data (e.g., different numbers of wells, cells, features, images, different image sizes and formats, different number of time-points, and so on). The structure of the data model provides a way of describing data and the relationships within the data, enabling data to be organized, cataloged, and searched effectively. Databases where a database model is implemented enable joining of related data to allow meaningful visualization, analysis, and data mining. The data model is also important for integration with other systems. HCS data are containing three types of data: 1. Database of compounds (RNAi or small molecules). 2. Numbers of images that require significant amounts of storage. 3. Numbers of files including image processing parameters. 4. Meta-data. Thus, a large amount of data is collected for just one well of a single plate. In addition, other associated information about the assay or experiment, such as protocol information, is also typically recorded. Having four types of data is easy to define three general categories of HCS data: - Image data: These are the images acquired at each channel for each field within a well and produced thumbnails for visualization purposes - Numeric Results data: these are the measurements that result from performing an analysis on an image with image analysis algorithms. - Metadata: These are the associated data that provide context for the other two categories of data (i.e., metadata are data that describes other data). Examples are: well – compound annotation, assay type, plate information, protocols, operators, calculated data such as dose–response values, as well as annotations imported from other systems. Let’s try to understand how data are produced. HCS microscopes typically scan multiwell plates. These plates typically have 96, 384, or 1536 wells. Each well is a container in the plate that contains an individual sample of cells. Each well is divided into multiple fields. Each field is a region of a well that represents an area to image. Each field consists of multiple images, one for each individual wavelength of light (referred to as a “channel”, “staining”), corresponding to the fluorescent markers/probes used for the biology/dye of interest (e.g., DAPI). There are typically between two and four channels per field (e.g., each channel shows different elements of the cells: 1 channel nuclei, 2 channel: cell membranes, 3 channel: endosomes, and so on). The images produced are immediately analyzed using automated image processing. Experiment results are produced. Each well is seeded with a certain number of cells which has to be detected by image processing algorithms. The cell number counted is a basic parameter used for the quality control of automation, microscopy or assay performance. The number of cells per well varies depending on the experiment, but typically ranges between 5 and 10000 cells. Very often images from well fields are merged into one image using montage function. For each cell, multiple object features (or measurements) are calculated by automated image processing. The cell features include measurements such as size, shape, intensity, and so on. The number of cell features calculated varies depending on the assay, but typically ranges between 5 and 500.Those features have to be carefully investigated, filtered and only parameters should be considered for hit definition. In addition, cell features are often aggregated to the well level to provide well level statistics (one well one row labeled with plate annotation and position as unique identify). The total storage size for experiments is primarily based on the acquired image data, image thumbnails, library information and the numeric results data. The amount of data, acquisition and processing time varies depending on a number of factors including the type of assay, the number of plates, the type of the screen (primary, secondary), available computational hardware, the throughput of the instrument or analysis application and the number of instruments which can work parallel. Table 2 demonstrates example experiments and summarizes necessary time, number of records and require for storage space. The size of the library information and numeric results data are counted in megabytes. Numeric results are estimated by the number of feature records (lines in tables). Image storage depends on the number of images acquired. The number of images depends on plate number, plate type (96, 384, 1536 ), number of fields, number of channels, confocality levels and eventually time points in case of kincetic studies. The typical image size acquired ranges between 500KB and 2 MB (single slice, single tiff file without montage). Thumbnails of those images often are generated using jpeg compression, their size range between 150- 300 kb. For numeric results data are categorized in three types of outputs: cell based, image based and well based. The number of image based record should be equal to the number of acquired images which is also equal to the number of thumbnails produced. The record number of well based results data should be equal to the number of all wells in screening experiment. In high content informatics, the numeric data are supported by the images and the validation of numeric data is based on the visual inspection of images. Any high content informatics solution therefore needs to be able to efficiently handle the relationships between the various levels of numeric results data, library information and the associated images. In the next subsection we will describe a database model (schema) and a database solution for handling library data, images and numeric results data.
EN21	0	﻿High-content screening can easily generate more than one Terabyte in primary images and metadata per run, that have to be stored and organized, which means an appropriate laboratory information management system (LIMS) has to be established. The LIMS must be able to collect, collate and integrate the data stream to allow at least searching and rapid evaluation of the data. After image acquisition and data transfer, image analysis will be run to extract the metadata. Further evaluation includes testing for process errors. Heat maps along with pattern recognition algorithms help to identify artefacts such as edge-effects, uneven pipetting, or simply to exclude images that are not in focus. All plates should be checked so that the selected positive and negative controls exhibit values in a pre-defined range. Further, data may be normalized against controls before further statistical analysis is run to identify putative hits. Known proteins of the pathway being screened should score, and are a good internal control for the accuracy of the assay and workflow. Hits have to be verified by going back to the original images. Further, results have to be compared between independent runs. After this, an appropriate hit verification strategy has to be applied as discussed above. Target gene expression should be confirmed, for example, by running a microarray analysis of gene expression for the given cell line. Finally, data will be compared to other internal and external data sources. Cluster analysis will assist in identifying networks and correlations. A critical aspect of high content screening is the informatics and data management solution that the user needs to implement to process and store the images. Typically multiple images are collected per microplate well at different magnifications and processed with pre-optimised algorithms (these are the software routines that analyse images, recognize patterns and extract measurements relevant to the biological application, enabling the automated quantitative comparison and ranking of compound effects) to derive numerical data on multiple parameters. This allows for the quantification of detailed cellular measurements that underlie the phenotype observed. From an image analysis perspective the following should not be overlooked when reviewing vendor offerings: the breadth of biology covered; how the software is delivered, does it run quickly, or open a script; is analysis done on-the-fly or offline; have the algorithms been fully validated with biology; the ease of exporting image files to other software packages; and access to new algorithms, is the user dependent on the supplier or is it relatively easy to develop your own or adapt existing algorithms? The key theme and piece of information repeated throughout this chapter is “partnering”. Scientific research and informatics must work together for the mutual benefit of screening like the drug discovery process. To really be part of the winning team in any organization, all areas must bring their collective expertise together and make the extra effort to understand one another and defer where there is lack of knowledge to those on the team with the experience and expertise or to seek external advises. It is necessary to start off by setting the stage concerning where laboratory computing, which includes the data management (we will discuss a bit later in the chapter), has progressed in order to gain the necessary understanding of where it currently is and where we anticipate it will be going in the HCS area in the future. A goal of this chapter is to provide an overview of the key aspects of informatics tools and technologies needed for HCS, including characteristics of HCS data; data models/structures for storing HCS data; HCS informatics system architectures, data management approaches, hardware and network considerations, visualization, data mining technologies, and integrating HCS data with other data and systems. HCS systems scan a multiwell plate with cells or cellular components in each well, acquire multiple images of cells, and extract multiple features (or measurements) relevant to the biological application, resulting in a large quantity of data and images. The amount of data and images generated from a single microtiter plate can range from hundreds of megabytes (MB) to multiple gigabytes (GB). One large-scale HCS experiment, often resulting in billions of features and millions of images that needs multiple terabytes (TB) of storage space. High content informatics tools and infrastructure is needed to manage the large volume of HCS data and images. There are many rules that are common for the image based HCS informatics infrastructure in academic or non academic organization. Answering the following questions analyzed by entire organization tells one exactly which strategy and organization setup has to be taken and what type of work has to assign to experts and researchers. In choosing the strategy and organization setup one needs to answer the following questions: • Is the required analysis software available off-the-shelf or must it be written in-house? This decision has to be taken in collaboration between IT and scientists, based on the defined requirements. • What kind of data will be acquired (how many screens in year)? • How is the data stored, managed, and protected for short-, medium-, and long-term use? • What type of desktop clusters and servers are required for HCS computing? (brand, type, speed, and memory) • How do the computer systems interface with the necessary data collection instrumentation and connect to the network and servers at the same time? • Can allowances and accommodations be made for external collaborations and programs shared among scientists? • Are we interested in setup a safety buffered zone outside of our firewalls to allow this external data exchange? After analysis of those questions one would think to have dedicated IT person from IT department working together with the scientists to allow IT professionals to take over responsibility for informatics tasks. The side-by-side person would allow the informatics organization to understand needs of HCS unit. For example the servers processes could be placed inside of HCS pipeline or infrastructure and not be placed as usual and forced to add extra steps to the workflow. It is also important to decide what will be operated by informatics department and what by HCS unit within organization. It makes better sense for informatics department to ﻿Within the last few years a large number of tools and softwares dealing with different computational problems related to HCS have been developed. Incorporating third party or new tools into existing frameworks needs a flexible, modular and customizable workflow framework. Workflow (Pipeline) systems could become crucial for enabling HCS researchers doing large scale experiments to deal with this data explosion. The workflow is termed abstract in that it is not yet fully functional but the actual components are in place and in the requisite order. In general, workflow systems concentrate on the creation of abstract process workflows to which data can be applied when the design process is complete. In contrast, workflow systems in the life sciences domain are often based on a data-flow model, due to the data-centric and data-driven nature of many scientific analyses. A comprehensive understanding of biological phenomena can be achieved only through the integration of all available biological information and different data analysis tools and applications. In general, an ideal workflow system in HCS can integrate nearly all standard tools and software. For example, for an HCS using small molecules, the workflow system must be able to integrate different image processing software and data mining toolkits with flexibility. The possibility that any single software covers all possible domains and data models is nearly zero. No one vendor or source can provide all the tools needed by HCS informatics. So it is suggested that one uses specialized tools from specialized sources. Also not all softwares components can be integrated with all workflow systems. Workflow environment helps also HCS researchers to perform the integration themselves without involving of any programming. A workflow system allows the construction of complex in silico experiments in the form of workflows and data pipelines. Data pipelining is a relatively simple concept. Visual representation of the workflow process logic is generally carried out using a Graphical User Interface where different types of nodes (data transformation point) or software components are available for connection through edges or pipes that define the workflow process. Graphical User Interfaces provide drag and drop utilities for creating an abstract workflow, also known as “visual programming”. The anatomy of a workflow node or component (Fig. 3) is basically defined by three parameters: input metadata, transformation rules, algorithms or user parameters and output metadata. Nodes can be plugged together only if the output of one, previous (set of) node(s) represents the mandatory input requirements of the following node. Thus, the essential description of a node actually comprises only in -and output that are described fully in terms of data types and their semantics. The user can create workflows using any combination of the available tools, readers, writers or database connections in workflow system by dragging/dropping and linking graphical icons. The component properties are best described by the input metadata, output metadata and user defined parameters or transformation rules. The input ports can be constrained to only accept data of a specific type such as those provided by another component. An HCS workflow design is best carried out in phases. In the first phase, a conceptual workflow is generated. A conceptual workflow, as the name suggests, is a sequential arrangement of different components that the user may require to accomplish the given task. It is possible that some of those steps may in turn be composed of several sub components. The next phase converts the conceptual workflow into an abstract workflow by performing a visual drag and drop of the individual components that were figured to be a part of the workflow in the first phase. The workflow is termed abstract in that it is not yet fully functional but the actual components are in place and in the requisite order. In general, workflow systems concentrate on the creation of abstract process workflows to which data can be applied when the design process is complete. HCS screening workflows are based on a dataflow which integrate most of the available, standard software tools (either commercial or public domain) along with different classes of programmable toolkits. As an example, Figure 3 shows a workflow designed to be run by the HCDCKNIME Workflow Management System ( http://hcdc.ethz.ch). This workflow is used by HCS facilities. It obtains RNAi from databases, annotates them, make dilutions steps, barcode handling, split volume. In this case, the tasks, also known as steps, nodes, activities, processors or components, represent either the invocation of a remote Web service (the databases), or the execution of a local recalculation. Data-flows along data links from the outputs of a task to the inputs of another, is prepared according to a pre-defined graph topology. The workflow defines how the output produced by one task is to be consumed by a subsequent task, a feature referred to as orchestration of a flow of data. Any computational component or node has data inputs and data outputs. Data pipelining views these nodes as being connected together by ‘pipes’ through which the data flows (Figure 4). Workflow technology is a generic mechanism to integrate diverse types of available resources (databases, microscopes, servers, software applications and different services) which facilitates data exchange within screening environment. Users without programming skill can easily incorporate and access diverse instruments, image processing tools and produced data to develop their own screening workflow for analysis. In this section, we will discuss the usage of existing workflow systems in HCS and the trends in applications of workflow based systems. Many free and commercial software packages are now available to analyse HCS data sets using statistical method or classification, although it is still difficult to find a single off-the-shelf software package that answers all the questions of HCS analysis. Statistical open source software packages such as BioConductor (www.bioconductor.org) provide large collections of methods suitable for HCS data analysis. However, their command-line usage can be too demanding for users without adequate computer knowledge. As an alternative, software packages where users can upload their data and receive their processed results are becoming increasingly common: Weka25, CellAnalyzer4, CellHTS3, TreeView21 have all been published within the last year. Unfortunately, these services often allow only limited freedom in the choice and arrangement of processing steps. Other, more flexible tools, such as Eclipse6, KNIME13, JOpera2, operate either stand-alone or require considerable computer knowledge and extra software to run through the web. In order to make use of the vast variety of data analysis methods around, it is essential that such an environment is easy and intuitive to use, allows for quick and interactive changes to the analysis process and enables the user to visually explore the results. ﻿Features are computed values that should be representative of the signal and be reproducible at different times. Other criteria for the features will depend on the application, for example: 􀁸Smaller dimension than the signal; 􀁸High inter-class variance with low intra-class variance; 􀁸Robust/enhanced representation of the signal (i.e. invariant to changes caused by noise, scale factors etc). Examples of simple features are: 􀁸Mean (but not useful if we set it to zero as a pre-processing step for noise removal); 􀁸Standard deviation; 􀁸Energy (which can be computed by using variance after setting mean to zero) - very often, we measure the energy in different spectral bands and use them as features. For example, in the case of electroencephalogram (EEG), bands like delta, theta, alpha, beta and gamma are normally used. To do this, we can filter the signal in the specific band and then compute the energy. For example, using MATLAB code (with IIR Elliptic filter): Correlation of a test signal with a template signal can be used as a feature. It is a measure of similarity between two signals. In MATLAB, we can use R=corrcoef(X,Y) where X is the test signal and Y is the template signal. This is useful when we have a template of signals and need to test the class/category of the test signal. For example, if we have templates for electrocardiogram (ECG) signals from five different heart ailments, then we can use the correlation value from a test ECG signal with each of the templates: The highest correlation will tell us which activity the test ECG signal is likely to belong. This method is more suitable for ECG signals rather than EEG as EEG signals are more ‘random’ as compared to ECG signals which generally have more known patterns (such as sinus rhythm, atrial fibrillation etc). Autoregressive24 (AR) model is another popular linear feature extraction method for biological signals. A real valued, zero mean, stationary, non-deterministic, autoregressive process of order p is given by where p is the model order, x[n] is the data of the signal at sampled point n, ak are the real valued AR coefficients, and e[n] represents the white noise error term independent of past samples. AR modelling could be seen as a process to obtain an equation that fits the signal (like in curve fitting). AR modelling tries to model the signal assuming that a data point is closely related to the previous few data points. This is suitable for modelling biosignals. Many different techniques have been proposed to estimate ak such as Yule-Walker (YW) method. However, YW method is computationally complex and is erroneous for small data segments due to difficulties in properly estimating the autocorrelation function. Hence, recursive algorithms have been proposed to estimate ak with order p using ak of previous order p-1. Examples of such methods are Burg’s and Levinson– Durbin algorithms but the former is more accurate than the latter since it uses more data points simultaneously by minimising not only a forward error but also a backward error [1]. In MATLAB, we can compute the ak coefficients using arburg(x,p) function. A model order which is too high will overfit the data and represent too much noise but a model order which is too small will not sufficiently represent the signal. So a compromise has to be made for the model order. There are many methods to compute the model order like Akaike Information Criterion (AIC), Final Prediction Error, Criterion Autoregressive Transfer, Minimum Description Length etc [1, 2]. AIC is most common and its use will be explained here: where p is the model order, N is the length of the signal and 2 p 􀁖is the variance of the error sequence at order p. The first component of the AIC function represents the fitting ability (higher order, better fit) while the second component represents a penalty function with increasing order. In MATLAB, the code [A,E] = arburg(x,p) returns the ak coefficients of signal x in A and error variance in E (using order p). AIC is computed for order 1 to a certain maximum order (depending on the application, rule of thumb is p <N/3, though the selected order is typically lower than N/3) and then the order p that minimises the AIC function is chosen to be the optimal order. Also, in general, every additional peak in the spectral plot will require increase of two in the model order [2]. So, model order of six will be required when using AR method for a combination of three sinusoidal components, each with distinct frequency. However, in most cases, it is difficult to know the exact number of spectral peaks and methods like AIC should be used to obtain the AR model order. For example, using the ECG signal shown in Figure 1.2 and computing AIC (from order 1 to order 20) gives us the following plot: It can be seen that AIC values do not change significantly after model order 4, so order 4 can be chosen as the appropriate order. AR model can also be used to predict values of a signal. For example, assume that we have a signal x: the AR coefficients of order 3 are A = [-0.46 -0.41 -0.10]. In MATLAB, we will get AR coefficients in the form [1.0000 -0.4618 -0.4048 -0.1058]; which are the AR coefficients obtained if we were to rewrite (5.1): Computing x[10] using this 3rd order AR model by ignoring the error term for simplicity, we obtain25 As an example to illustrate the usage of AR coefficients as features, consider EEG signals obtained during two different mental activities26. In Figure 5.2 (a), two EEG plots for one subject are shown from two mental activities (mathematical activity and imagining an object being rotated). The abscissa is sampling points while the ordinate is amplitude (arbitrary units). Figure 5.2 (b) shows another two EEG plots (one from each mental activity taken at another time from the same subject). From these EEG plots, it is difficult to differentiate the maths and object rotation activities. But using the 6th order AR coefficients, the math and object rotation activities can be differentiated. This is though exact values are not produced by the AR model for the same mental activity, the values are sufficiently close within a mental activity and sufficiently different across activities (especially the first few AR coefficients).$$$﻿Features are computed values that should be representative of the signal and be reproducible at different times. Other criteria for the features will depend on the application, for example: 􀁸Smaller dimension than the signal; 􀁸High inter-class variance with low intra-class variance; 􀁸Robust/enhanced representation of the signal (i.e. invariant to changes caused by noise, scale factors etc). Examples of simple features are: 􀁸Mean (but not useful if we set it to zero as a pre-processing step for noise removal); 􀁸Standard deviation; 􀁸Energy (which can be computed by using variance after setting mean to zero) - very often, we measure the energy in different spectral bands and use them as features. For example, in the case of electroencephalogram (EEG), bands like delta, theta, alpha, beta and gamma are normally used. To do this, we can filter the signal in the specific band and then compute the energy. For example, using MATLAB code (with IIR Elliptic filter): Correlation of a test signal with a template signal can be used as a feature. It is a measure of similarity between two signals. In MATLAB, we can use R=corrcoef(X,Y) where X is the test signal and Y is the template signal. This is useful when we have a template of signals and need to test the class/category of the test signal. For example, if we have templates for electrocardiogram (ECG) signals from five different heart ailments, then we can use the correlation value from a test ECG signal with each of the templates: The highest correlation will tell us which activity the test ECG signal is likely to belong. This method is more suitable for ECG signals rather than EEG as EEG signals are more ‘random’ as compared to ECG signals which generally have more known patterns (such as sinus rhythm, atrial fibrillation etc). Autoregressive24 (AR) model is another popular linear feature extraction method for biological signals. A real valued, zero mean, stationary, non-deterministic, autoregressive process of order p is given by where p is the model order, x[n] is the data of the signal at sampled point n, ak are the real valued AR coefficients, and e[n] represents the white noise error term independent of past samples. AR modelling could be seen as a process to obtain an equation that fits the signal (like in curve fitting). AR modelling tries to model the signal assuming that a data point is closely related to the previous few data points. This is suitable for modelling biosignals. Many different techniques have been proposed to estimate ak such as Yule-Walker (YW) method. However, YW method is computationally complex and is erroneous for small data segments due to difficulties in properly estimating the autocorrelation function. Hence, recursive algorithms have been proposed to estimate ak with order p using ak of previous order p-1. Examples of such methods are Burg’s and Levinson– Durbin algorithms but the former is more accurate than the latter since it uses more data points simultaneously by minimising not only a forward error but also a backward error [1]. In MATLAB, we can compute the ak coefficients using arburg(x,p) function. A model order which is too high will overfit the data and represent too much noise but a model order which is too small will not sufficiently represent the signal. So a compromise has to be made for the model order. There are many methods to compute the model order like Akaike Information Criterion (AIC), Final Prediction Error, Criterion Autoregressive Transfer, Minimum Description Length etc [1, 2]. AIC is most common and its use will be explained here: where p is the model order, N is the length of the signal and 2 p 􀁖is the variance of the error sequence at order p. The first component of the AIC function represents the fitting ability (higher order, better fit) while the second component represents a penalty function with increasing order. In MATLAB, the code [A,E] = arburg(x,p) returns the ak coefficients of signal x in A and error variance in E (using order p). AIC is computed for order 1 to a certain maximum order (depending on the application, rule of thumb is p <N/3, though the selected order is typically lower than N/3) and then the order p that minimises the AIC function is chosen to be the optimal order. Also, in general, every additional peak in the spectral plot will require increase of two in the model order [2]. So, model order of six will be required when using AR method for a combination of three sinusoidal components, each with distinct frequency. However, in most cases, it is difficult to know the exact number of spectral peaks and methods like AIC should be used to obtain the AR model order. For example, using the ECG signal shown in Figure 1.2 and computing AIC (from order 1 to order 20) gives us the following plot: It can be seen that AIC values do not change significantly after model order 4, so order 4 can be chosen as the appropriate order. AR model can also be used to predict values of a signal. For example, assume that we have a signal x: the AR coefficients of order 3 are A = [-0.46 -0.41 -0.10]. In MATLAB, we will get AR coefficients in the form [1.0000 -0.4618 -0.4048 -0.1058]; which are the AR coefficients obtained if we were to rewrite (5.1): Computing x[10] using this 3rd order AR model by ignoring the error term for simplicity, we obtain25 As an example to illustrate the usage of AR coefficients as features, consider EEG signals obtained during two different mental activities26. In Figure 5.2 (a), two EEG plots for one subject are shown from two mental activities (mathematical activity and imagining an object being rotated). The abscissa is sampling points while the ordinate is amplitude (arbitrary units). Figure 5.2 (b) shows another two EEG plots (one from each mental activity taken at another time from the same subject). From these EEG plots, it is difficult to differentiate the maths and object rotation activities. But using the 6th order AR coefficients, the math and object rotation activities can be differentiated. This is though exact values are not produced by the AR model for the same mental activity, the values are sufficiently close within a mental activity and sufficiently different across activities (especially the first few AR coefficients).
EN04	1	﻿Computer simulation is used to reduce the risk associated with creating new systems or with making changes to existing ones. More than ever, modern organizations want assurance that investments will produce the expected results. For instance, an assembly line may be required to produce a particular number of autos during an eight hour shift. Complex, interacting factors influence operation and so powerful tools are needed to develop an accurate analysis. Over the past few decades, computer simulation software, together with statistical analysis techniques have evolved to give decision makers tools equal to the task. As the world grows more technical and the need for precision becomes more important, the margin for error will continue to shrink. Business, industry, and governments cannot afford to make educated guesses during systems development. For that reason, computer simulation is more important than ever. Simulation uses a model to develop conclusions providing insight on the behavior of real-world elements being studied. Computer simulation uses the same concept but requires the model be created through computer programming. While this field has grown and flourished with availability of powerful software and hardware, its origins in the desire to forecast future behaviors, run quite deep. Men and women have attempted to foretell the future since ancient times. Kings employed wizards and soothsayers. Various religions used prophets. Seers such as French apothecary Michel de Nostredame (better known as Nostradamus) became famous with their visions of the future. Others attempted to make predictions based on birth dates and the stars. Crystal balls, bones, and tarot cards were all used as tools to probe the future. Although this book does not advocate those methods, in the same way modern chemists bear a relationship to the ancient alchemist, the modern simulation practitioner has a relationship with the ancient prophet. Of course, methodologies used by modern simulation analysts bear virtually no similarity with the prediction methods used in ancient times. However, there are common elements. For instance, each sought to remove the risk of a future event or behavior and reduce uncertainty. The prophet tried to accomplish this with the magic available at the time. Today, the simulation analyst uses the modern magic of mathematical principles, experimentation, computer science and statistics. Computer simulation can be classified as a branch applied mathematics. The use of computer simulation increased due to availability of computing power and improvements in programming languages. Added to this are inherent difficulties or even impossibilities to accurately describe complex real world systems using analytical or purely mathematical models. For these reasons, a tool that can represent these complexities accurately is required. Computer simulation can be broadly defined as: “Using a computer to imitate the operations of a real world process or facility according to appropriately developed assumptions taking the form of logical, statistical, or mathematical relationships which are developed and shaped into a model.” The result can be manipulated by varying a set of input parameters to help an analyst understand the underlying system’s dynamics. The model typically is evaluated numerically over a simulated period of time and data is gathered to estimate real world system characteristics. Generally, the collected data is interpreted with statistics like any experiment. Computer simulation can be an expensive, time consuming, and complicated problem solving technique. Therefore, certain circumstances warrant its use. Situations well suited to its application include the following (Table 1.1): Real system does not yet exist and building a prototype is cost prohibitive, time-consuming or hazardous. Aircraft, Production System, Nuclear Reactor System is impossible to build. National Economy, Biological System Real system exists but experimentation is too expensive, hazardous or disruptive to conduct. Proposed Changes to a Materials Handling System, Military Unit, Transportation System, Airport Baggage Handling System Forecasting is required to analyze long time periods in a compressed format. Population Growth, Forest Fire Spread, Urbanization Studies, Pandemic Flu Spread Mathematical modeling has no practical analytical or numeric solution. Stochastic Problems, Nonlinear Differential Equations Using computer simulation for analysis has many advantages over other decision making techniques. Among these advantages are: 1. Allows Experimentation without Disruptions to Existing Systems - In systems that already exist, testing new ideas may be difficult, costly, or impossible. Simulation allows a model to be developed and compared to the system to ensure it accurately reflects current operation. Any desired modifications can be made to the model first, the impact on the system examined, and then a decision to implement the changes in the real world system can be made. Example: Changing the Line An automotive assembly line may run 24 hours a day, seven days a week. Shutting the line down, putting in a temporary modification (which may or may not speed up a process), and resuming production would be very expensive. Example: Adding Equipment Unless the machinery was already purchased, the process of adding it to the line and trying it firsthand would be impossible. 2. Concept can be Tested Prior to Installation - A computer simulation will allow concepts to be tested prior to the installation of new systems. This testing may reveal unforeseen design flaws and give designers a tool for improvement. If the same flaws were discovered after installation, changes to the system might end up being very costly or even impossible to implement. Example: Purchase of an Automatic Storage and Retrieval system (ASRS) Engineers in a medium sized company decide to replace an existing warehouse with an ASRS. After stretching their tight budget to its outer limits, the equipment was procured and installed. Several months of usage revealed the new system was unable to keep up with the demands for pallets being entering and removed from the racks. Their assembly line process began to bog down because material wasn't arriving from storage in a timely fashion. The budget was gone and upper management told manufacturing to live with their decision. The entire situation could have been avoided by simulating the ASRS system prior to purchase. 3. Detection of Unforeseen Problems or Bugs - When a system is simulated prior to installation and found to work in concept, the model is often refined to include finer details. The detailed simulation may reveal unforeseen problems or bugs that may exist in the system's design. By discovering these problems prior to installation, debug time and rework costs can be avoided. In addition, improvements to system operation may be discovered. ﻿Simulation languages are versatile, general purpose classes of simulation software that can be used to create a multitude of modeling applications. In a sense, these languages are comparable to FORTRAN, C#, Visual Basic.net or Java but also include specific features to facilitate the modeling process. Some examples of modern simulation languages are GPSS/H, GPSS/PC, SLX, and SIMSCRIPT III. Other simulation languages such as SIMAN have been integrated into broader development frameworks. In the case of SIMAN, this framework is ARENA. Simulation languages exist for discrete, continuous and agent-based modeling paradigms. The remainder of this book will focus on the discrete event family of languages. Specialized features usually differentiate simulation languages from general programming languages. These features are intended to free the analyst from recreating software tools and procedures used by virtually all modeling applications. Not only would the development of these features be time consuming and difficult, but without them, the consistency of a model could vary and additional debugging, validation and verification would be required. Most simulation languages provide the features shown in Table 2.1. 1) Simulation clock or a mechanism for advancing simulated time. 2) Methods to schedule the occurrence of events. 3) Tools to collect and analyze statistics concerning the usage of various resources and entities. 4) Methods for representing constrained resources and the entities using these resources. 5) Tools for reporting results. 6) Debugging and error detection facilities. 7) Random number generators and related sets of tools. 8) General frameworks for model creation. Although many models are written using simulation languages, some analysts still prefer to rely on traditional programming languages for model development. In other cases, extensions to a language are developed to add capabilities to a traditional language. For instance, Repast Simphony is a free and open source, agent-based modeling toolkit that adds features to Java in order to simplify model creation and use. This blended approach provides the advantages of both the traditional language and the simulation modeling extensions. The motivations behind using a general purpose language include: Programmer familiarity: Developers already know the general purpose programming language. They may not have the time or inclination to learn a simulation language. Flexibility: Programming languages inherently are flexible, giving the analyst freedom to create the model using his or her preferred methodology. Cost: Programming language software is usually more accessible and far less expensive than specific simulation software. This may not always be true since several leading simulation languages can be downloaded for no cost. However, other leading simulation language packages can be very expensive. Hardware Concern: General purpose software may be available on any hardware platform while some simulation languages may require special machines and memory configurations. Lack of Analyst Knowledge: The analyst may not understand simulation languages and may lack knowledge on the advantages of using a simulation language package. Training: Available classes in the use of traditional languages are more likely to be available than specialty simulation training. Although traditional languages do offer some advantages, most of these are outweighed by features standard to many simulation languages. In a typical modeling application, the programmer or analyst will find the initial investment in a simulation language more than pays off. A simulation language will provide a savings in coding, debugging, analysis of results, and in making changes. A variety of simulation languages exist and are used by businesses, researchers, manufacturing and service companies, and consultants. The next sections briefly discuss two common simulation languages: GPSS and SIMSCRIPT. GPSS: General Purpose Simulation System (GPSS) was originally developed by Geoffrey Gordon of IBM and released in October of 1961. Following IBM’s release of GPSS to the public domain, it became a multivendor simulation language and has been in continuous use since. In general, GPSS enjoys widespread popularity due to its sensible world view and overall power. Its basic functions can be easily learned while powerful features make it ideal for modeling complex systems. In general, GPSS is used to simulate queuing systems that consist of customer entities interacting and completing in a system of constrained resources. The resources are structured as networks of blocks that entities (also called transactions) enter and use to perform various tasks in certain amounts of simulated time. As entities move through these networks, which have been organized to represent a real world system, statistics are collected and used to determine if the system contains bottlenecks, is over or under utilization, or exhibits other characteristics. Output data is made available for analysis at the end of a production run. Presently, several vendors offer versions of GPSS. Included are: Wolverine Software which produces GPSS/H, a powerful, state-of-the-art version of GPSS engineered to allow creation of large, complex models (http://www.wolverinesoftware.com). Minuteman Software which produces a user friendly GPSS simulation environment called GPSS World that features special model development tools (http://minutemansoftware.com). ngolf Ståhl and Beliber AB which produce WebGPSS, a stream-lined version of GPSS, with a focus simulation and modeling concept education (http://www.webgpss.com). SIMSCRIPT III: This language is a direct descendant of the original SIMSCRIPT language produced at Rand Corporation in the 1960's. SIMSCRIPT III has constructs that allow a modeler to approach a problem from either a process or an event oriented world view. SIMSCRIPT III offers unique features which add to its appeal. Among these are: • Object-Oriented Programming • Modularity • SIMSCRIPT III Development Studio (SimStudio) • Object-Oriented Simscript III graphics • Data Base Connectivity SDBC In general, SIMSCRIPT III is a free form language with English-like syntax. This syntax allows the code in the system to become self-documenting. Model components can be programmed clearly enough to provide an excellent representation of the organization and logic of the system being simulated. SIMSCRIPT III is maintained and distributed at http:// www.simscript.com by CACI Products Company. Most discrete event simulation languages model a system by updating the simulation clock to the time that the next event is scheduled to occur. Events and their scheduled times of occurrence are maintained automatically on one of two ordered lists: the current events chain or the future events chain. The current events chain keeps a list of all events that will (or may) occur at the present clock time. The future events chain is a record of all events that can occur at some point in the future. A simulation clock moves to the next event on the future events chain and changes the system state of the model based on that event’s characteristics. ﻿There has been a long running debate that concentrates on trying to decide whether simulation should be defined as an art or a science. Those who believe it to be a science feel that statistics, mathematics, and computer science comprise its foundation and are the basis for this classification. Others feel that the skill of the modeling team, the creativity involved in developing the model, and the interpretation of the results all add up to an individualized art. In this author's opinion, there is no real way to scientifically structure a simulation to guarantee that its results are valid. Instead, after the model has been coded and run, the outputs can be studied and compared with corresponding known values to determine suitability. If no known values exist then the modeler must rely on his instincts and the judgment of experts to make this determination. The creativity and instincts used are akin to an art. Much of the methodology involved in model creation and analysis are based on computer science and mathematical principles. Therefore, elements of art and science exist in modeling. Simulation best may be defined as a “soft-science” containing both. Figure 3.1 depicts the spectrum extending from art to science and the simulation's place. In recent years simulation has been moving along the spectrum more toward the science end. Improvements in simulation languages and the advent of simulators have removed some of the need to be as creative and innovative. Much of the uncertainty in model creation has also been eliminated through the development of application specific languages. Numerous explanations exist for reasons behind the phenomenal growth computer simulation has experienced both in terms of application areas and in the number of available software products. Among these reasons are: 1. Improvements in computers: The first simulations were done on large, room-sized mainframe computers. These computers relied on card decks and operated most often in the batch mode. Not many people had access to these mammoth devices. In the last thirty years, computers have been reduced in size and cost considerably. Equivalents of the mainframes that used to occupy large rooms are now carried around in briefcase sized packages. Computers have moved from research laboratories and can now be found on practically every desk in all industries. The computing power of a single chip is fast becoming all that is necessary to run the most sophisticated commercial simulation software. This widespread availability and reduction in cost of computers has enabled simulation to prosper. 2. Improvements in simulation products: Thirty years ago, most simulation work was done using assembly language, FORTRAN, or other high level languages. Development time was much greater, as was debugging, statistic tabulation, and the reliability of results. This situation began to change with the advent of GPSS in 1961. Since that time, a multitude of simulation languages, analysis programs, animators, and pre-programmed simulators have become available. Much of the development time required to create a model has been eliminated through standard features found in these products. In addition to performing the necessary simulation functions, varying degrees of user friendliness are available. Simulation languages such as Simscript and GPSS/H appeal to practitioners with programming skills while non-programmers can enjoy the mouse driven menus found in many application specific simulators. 3. New opportunities for simulation education: Three decades ago, very few universities offered discrete event simulation classes. Today many universities offer simulation classes in engineering and business curriculums. In addition, private seminars and training sessions are available. These sessions are sponsored by simulation software vendors, consultants, and corporations with an interest in simulation and modeling. Other sources of simulation education are trade magazines, academic publications, conferences, societies, and books. 4. Increasingly complex and technical work environments: During the previous few decades the average work environment has changed from simple manual assembly lines to complex automated systems. Many repetitive human tasks have been replaced with robots and factory automation equipment. Conveyors, forklift trucks, storage and retrieval racks, as well as many other factory floor items are now routinely controlled by programmable logic controllers or computers. This new complexity has made factory output rates very difficult to predict and manage. Thus, the need for better analysis tools arose. New simulation techniques evolved to satisfy this demand and were able to remove much of the guess work and uncertainty in the work place. 5. Computer literacy among analysts and engineers: Computer literacy and use is nearly ubiquitous among professionals. Everyone has the ability to receive computer training. 6. Competition and tightening budgets - Another factor adding to growth in simulation use is an emphasis on lowering overhead costs, reducing labor requirements, and streamlining operations. Much of this has come about as globalization has increased competition and businesses compete on an international basis. 7. Realization of the benefits offered by simulation: As more industries recognize the benefits of simulation, investing in capabilities to use these tools becomes more important. Apparent economic advantages have prompted companies to invest time and resources into this area. 8. Industrial peer pressure: Organizations without simulation capabilities have found themselves at a competitive disadvantage. This was most apparent in industries, such as materials handling, where a simulation study accompanying a quotation would lend credibility to a proposed system and often become a determining factor when the contract was awarded. A situation of industrial peer pressure was created. Purchasers would inquire why simulation was not being used by their vendors and demand that some type of through-put guarantee be given. Presently, most materials handling system request-for-quotations require modeling be performed prior to purchase. Similar requirements exist in other industries. These forces have been a key factor in popularizing simulation. 9. Warm fuzzy feeling: Many companies have developed internal simulation groups to model inhouse problems, proposed systems, and existing manufacturing processes. Presentation materials such as simulation animation packages have helped to sell internal proposals to management and create a corporate warm fuzzy feeling. “Seeing is believing” and many simulation packages available today place an emphasis on graphics and output. 10. Part of an effort to increase quality: Another force related to simulation adoption is a commitment to quality improvement processes. The success of these philosophies has been demonstrated in many industries with large productivity gains and improved profitability. A major precept of quality is prevention of problems. By creating a model of systems to be designed, purchased, or installed, costly mistakes can be avoided and installations can be done right the first time. ﻿A computer simulation project is more than purchasing modeling software, plugging in a few values and expecting an answer to a question to pop out. This chapter considers the broader picture that simulation encompasses by developing a methodology for starting the process correctly. However, before that is discussed, a failed project is examined. Simulations can go terribly wrong, particularly if the appropriate infrastructure is not carefully developed to support the modeling effort. Few simulation analysts can report that all of their models are flawless. The previous example analyzed a real simulation project that became disastrous and the steps taken to revive it. Although the model had been accurate, the simulation received criticism and blame for the problems. Several pitfalls can be identified by looking at the process in retrospect. 􀁸First: The simulation analyst was inexperienced. She lacked practical knowledge of the system's operation and this slowed initial model development. 􀁸Second: The simulation 'team' members didn't participate adequately. Although the model demonstrated problems did exist, time was not taken to solve these problems as a preventative measure. Rather, the system was installed and then problems were tackled at a much higher cost. 􀁸Third: Goals changed mid-simulation effort. The original simulation was meant to discover if the would work. When production fell short, a new goal of 'making it work' became the directive. The problems pointed out by this case study could have been avoided if the simulation effort had been taken more seriously. If preventative action had addressed system inadequacies early, costly corrections and time delays could have been avoided. A recent study of discrete event computer simulation projects revealed unsuccessful efforts are characterized by high costs, model size constraints, and slow software. In contrast, successful projects are characterized by teamwork, cooperation, mentoring, effective communication of outputs, high-quality vendor documentation, easily understood software syntax, higher levels of analyst experience, and structured approaches to model development. While these findings should not come as a surprise, the importance of having a structured methodology for approaching a simulation is apparent. The next sections of this textbook provide a detailed view of an approach to help enable simulation project success. The process used in this textbook is representative of many possible computer simulation project life cycles that have been used in practice and documented in simulation textbooks and journal articles. However, remember each simulation project is unique and specific requirements may be better served through customizing the steps discussed. In general, the simulation project life cycle described in this book consists of six major steps, each having multiple components. The major steps being investigated are: 1. Intelligence Phase. The precursor to any solution is full development and an understanding of the underlying dilemma or problem. 2. Managerial Phase. In general, this step requires interaction with management and other nontechnical staff of the organization to acquire necessary resources and support along with the formation of a simulation project team. 3. Developmental Phase. During this phase, the simulation model or models are created. System design, detail design, and coding all take place and rely on the interaction of the analysts and other members of the simulation team. 4. Quality Assurance Phase. While the model is being coded in complete or at least in prototype form, the analyst must ensure proper validation and verification. The ideas of testing and completion or integration are also important here. It may be necessary take testing one step further and begin development of face validity through interaction with end-users and management. Generally, quality assurance accompanies the entire modeling lifecycle. 5. Implementation Phase. Model use begins and decision support activities take place. 6. Operations, Maintenance, and Archival Phase. Development is essentially complete and only maintenance tasks remain. This phase may be substantial if the project results in a simulation maintained for repeated use A simulation project can be a complex organizational undertaking. In general, to believe a simulation project starts and ends with coding a model would be a mistake. Much more is involved. In many instances, modeling is one small component in a much larger simulation project life cycle. As stated previously, a simulation project is inclusive of a wide spectrum of activities ranging from system and problem definition to ensuring logic developed in the model can bridge the gap between model and real world and be used in actual system implementation. The following sections provide a closer look at the mechanics of conducting a professional simulation study. 4.1 Intelligence The intelligence phase of the simulation life cycle involves understanding the environment and determining problems to be solved. In general, this phase consists of the simulation analyst or potential simulation customer discovering situations that require modeling. The simulation analyst should emerge from this phase with knowledge that a problem exists and should have at least a preliminary understanding of the problem’s nature. Often, problem definition and feasibility are assessed at this point in time. 4.1.1 Problem Definition In order to ensure simulation project success, several events need to occur before any other work begins. First, the customer needs to define objectives for the study. The objectives for the simulation describe what questions the simulation needs to answer. In other words, the precursor to any solution is a full understanding of the underlying dilemma or problem. Parameters for the problem as well as broad scale constraints need to be defined. A simulation project may be driven by a broad statement such as ‘The Denver Chamber of Commerce has projected that incoming passenger flights will increase by 50% over the next 18 months.” Definition of the problem operationalizes the statement and looks at the system environment to clarify goals for analysis. For example the problem definition may become: Can the existing baggage handling system handle the project increase in passenger arrivals and departures? Of course, this question could rapidly be replaced with a related: What needs to be changed in order to accommodate the expected increase in passenger arrivals and departures? These sorts of clarifications need to be made early in the simulation life cycle. ﻿After the model has been conceptualized, coded, verified, and validated, it is ready to provide information through experimentation. Experimentation is the process of initializing key parameters in the model and setting up production runs to make inferences about the behavior of the system under study. The experimentation process consists of three steps: The first step in the experimentation process is experimental design. The term experimental design implies that an experiment is being designed or set up. This is done by identifying key parameters and initializing these parameters to values of interest. An example of an input parameter that could be the focus of an experiment is the number of doormen needed at the 21-Club (See section 6.3). The simulation could be initialized with either one or two doormen. Other issues that need to be resolved during the experimental design stage follow: A variable Z which can assume any value in the range (x,y) with equal probability is defined as being random (Figure 6.7). Random variables can be uniformly or non-uniformly distributed. They may be continuous or discrete. Random variables are selected by chance and are not influenced in any way by past values. Random numbers are widely used in experiments that are dependent upon chance. Examples of these are machine breakdowns, occurrences of rejected parts coming off an assembly line, and service times at an automatic teller machine. Random numbers can be generated in many different ways. The flip of a coin, drawing numbers from a hat, roulette wheels, and playing cards are examples of physical generation methods. The most common method of obtaining random numbers for use in the computer simulation field is through algebra. A number called a seed is selected and used to produce a sequence of numbers in the following manner (Figure 6.2): Although the resulting number stream from the preceding example is not truly random, if created properly, it will pass most tests for statistical randomness. Number streams generated through algebraic means are known as pseudo-random numbers. The use of pseudo-random number streams has several advantages over the use of true random numbers including: 1. The sequence of numbers is reproducible. This means different versions of a program can be tested using the same input data. 2. The streams can be generated quickly and efficiently for use in a simulation program. Although it is important to understand the concepts of random number generation and pseudo-random number streams, a simulation analyst will rarely have to create his or her own random number generator. Most simulation software products already have a built in means of creating pseudo-random numbers. As long as care in selecting a seed value (this is usually done for the analyst as well) is taken, very few problems should occur. However, this is not a topic to be taken too lightly. Without a reliable stream of random numbers, simulation results would be rendered invalid. Terminating simulations are models that represent a system that starts in a particular state and then terminates or ends after a fixed period of time or when a predetermined condition is reached. Terminating simulations are used to model particular manufacturing operations where the characteristics of the system startup are important to include or in situations where a specific period of time is being analyzed, like weekend ticket sales for a newly released movie. Terminating simulations study a system over a predefined time period like a shift, a day, or a week. An analyst using a terminating simulation makes no attempt to bring the model to a steady state or stable operating condition. Instead, model replications include all startup biases that might exist (as do most real world systems). It is important to remember that terminating simulations are still dependent on random samples from input distributions and each run must be treated as a single observation. This means replications using different random number seeds must be generated and used to compute a confidence interval or range likely to include the true value of the mean. Since terminating simulation runs produce output data exhibiting statistical independence, the analysis becomes easier. Steady state simulations are intended to examine a system from which startup and ending biases are removed. In other words, the analyst seeks to achieve a state in the model where conditions remain stable. While in many instances this might be less realistic than a terminating simulation, it can be useful in determining underlying characteristics of the system and filtering out the statistical noise. Two main challenges exist when developing a steady state simulation. The first is determining whether a steady state condition has been achieved. The second deals with statistical independence of the derived samples. Several techniques are used to deal with the first challenge. Almost every system experiences a period of time upon startup where resources are being acquired, customers are entering, or the initial stages of the system are in use while subsequent stages have not yet filled. These forces can result in a skewing of output statistics often called initial bias (See Figure 6.3). Several practical techniques are used to avoid the impact of startup bias. These include: 1. A warm-up period is determined after which model statistics collection is restarted. Most simulation software packages have built-in the capability of resetting statistics. In a sense, this would represent throwing out all data collected to the left of the dark line in Figure 6.3 and only considering data after that point in time. The analyst may have to review the simulation results and decide when the model appears to have entered steady state. 2. Initializing the model with a realistic, steady state condition of resource utilization and customers or other entities strategically placed within the model. This technique can work but must be verified as accurate and free of other forms of startup bias. Some researchers call this ‘preloading’ and others call it priming the model. 3. Data analysis can be used to remove the bias but this technique is rarely used. The second challenge is to ensure collected samples from the simulation are independent. In a steady state simulation, the current state of the model is dependent on the previous state. Therefore, independence cannot be assumed. In order to correct this problem, a series of simulation runs (often called replications) can be constructed in a way that collection of statistics is suspended between replications while the model runs in steady state. The resources and entities are not cleared, just the statistics.$$$﻿After the model has been conceptualized, coded, verified, and validated, it is ready to provide information through experimentation. Experimentation is the process of initializing key parameters in the model and setting up production runs to make inferences about the behavior of the system under study. The experimentation process consists of three steps: The first step in the experimentation process is experimental design. The term experimental design implies that an experiment is being designed or set up. This is done by identifying key parameters and initializing these parameters to values of interest. An example of an input parameter that could be the focus of an experiment is the number of doormen needed at the 21-Club (See section 6.3). The simulation could be initialized with either one or two doormen. Other issues that need to be resolved during the experimental design stage follow: A variable Z which can assume any value in the range (x,y) with equal probability is defined as being random (Figure 6.7). Random variables can be uniformly or non-uniformly distributed. They may be continuous or discrete. Random variables are selected by chance and are not influenced in any way by past values. Random numbers are widely used in experiments that are dependent upon chance. Examples of these are machine breakdowns, occurrences of rejected parts coming off an assembly line, and service times at an automatic teller machine. Random numbers can be generated in many different ways. The flip of a coin, drawing numbers from a hat, roulette wheels, and playing cards are examples of physical generation methods. The most common method of obtaining random numbers for use in the computer simulation field is through algebra. A number called a seed is selected and used to produce a sequence of numbers in the following manner (Figure 6.2): Although the resulting number stream from the preceding example is not truly random, if created properly, it will pass most tests for statistical randomness. Number streams generated through algebraic means are known as pseudo-random numbers. The use of pseudo-random number streams has several advantages over the use of true random numbers including: 1. The sequence of numbers is reproducible. This means different versions of a program can be tested using the same input data. 2. The streams can be generated quickly and efficiently for use in a simulation program. Although it is important to understand the concepts of random number generation and pseudo-random number streams, a simulation analyst will rarely have to create his or her own random number generator. Most simulation software products already have a built in means of creating pseudo-random numbers. As long as care in selecting a seed value (this is usually done for the analyst as well) is taken, very few problems should occur. However, this is not a topic to be taken too lightly. Without a reliable stream of random numbers, simulation results would be rendered invalid. Terminating simulations are models that represent a system that starts in a particular state and then terminates or ends after a fixed period of time or when a predetermined condition is reached. Terminating simulations are used to model particular manufacturing operations where the characteristics of the system startup are important to include or in situations where a specific period of time is being analyzed, like weekend ticket sales for a newly released movie. Terminating simulations study a system over a predefined time period like a shift, a day, or a week. An analyst using a terminating simulation makes no attempt to bring the model to a steady state or stable operating condition. Instead, model replications include all startup biases that might exist (as do most real world systems). It is important to remember that terminating simulations are still dependent on random samples from input distributions and each run must be treated as a single observation. This means replications using different random number seeds must be generated and used to compute a confidence interval or range likely to include the true value of the mean. Since terminating simulation runs produce output data exhibiting statistical independence, the analysis becomes easier. Steady state simulations are intended to examine a system from which startup and ending biases are removed. In other words, the analyst seeks to achieve a state in the model where conditions remain stable. While in many instances this might be less realistic than a terminating simulation, it can be useful in determining underlying characteristics of the system and filtering out the statistical noise. Two main challenges exist when developing a steady state simulation. The first is determining whether a steady state condition has been achieved. The second deals with statistical independence of the derived samples. Several techniques are used to deal with the first challenge. Almost every system experiences a period of time upon startup where resources are being acquired, customers are entering, or the initial stages of the system are in use while subsequent stages have not yet filled. These forces can result in a skewing of output statistics often called initial bias (See Figure 6.3). Several practical techniques are used to avoid the impact of startup bias. These include: 1. A warm-up period is determined after which model statistics collection is restarted. Most simulation software packages have built-in the capability of resetting statistics. In a sense, this would represent throwing out all data collected to the left of the dark line in Figure 6.3 and only considering data after that point in time. The analyst may have to review the simulation results and decide when the model appears to have entered steady state. 2. Initializing the model with a realistic, steady state condition of resource utilization and customers or other entities strategically placed within the model. This technique can work but must be verified as accurate and free of other forms of startup bias. Some researchers call this ‘preloading’ and others call it priming the model. 3. Data analysis can be used to remove the bias but this technique is rarely used. The second challenge is to ensure collected samples from the simulation are independent. In a steady state simulation, the current state of the model is dependent on the previous state. Therefore, independence cannot be assumed. In order to correct this problem, a series of simulation runs (often called replications) can be constructed in a way that collection of statistics is suspended between replications while the model runs in steady state. The resources and entities are not cleared, just the statistics.
EN07	0	﻿Computer simulation is used to reduce the risk associated with creating new systems or with making changes to existing ones. More than ever, modern organizations want assurance that investments will produce the expected results. For instance, an assembly line may be required to produce a particular number of autos during an eight hour shift. Complex, interacting factors influence operation and so powerful tools are needed to develop an accurate analysis. Over the past few decades, computer simulation software, together with statistical analysis techniques have evolved to give decision makers tools equal to the task. As the world grows more technical and the need for precision becomes more important, the margin for error will continue to shrink. Business, industry, and governments cannot afford to make educated guesses during systems development. For that reason, computer simulation is more important than ever. Simulation uses a model to develop conclusions providing insight on the behavior of real-world elements being studied. Computer simulation uses the same concept but requires the model be created through computer programming. While this field has grown and flourished with availability of powerful software and hardware, its origins in the desire to forecast future behaviors, run quite deep. Men and women have attempted to foretell the future since ancient times. Kings employed wizards and soothsayers. Various religions used prophets. Seers such as French apothecary Michel de Nostredame (better known as Nostradamus) became famous with their visions of the future. Others attempted to make predictions based on birth dates and the stars. Crystal balls, bones, and tarot cards were all used as tools to probe the future. Although this book does not advocate those methods, in the same way modern chemists bear a relationship to the ancient alchemist, the modern simulation practitioner has a relationship with the ancient prophet. Of course, methodologies used by modern simulation analysts bear virtually no similarity with the prediction methods used in ancient times. However, there are common elements. For instance, each sought to remove the risk of a future event or behavior and reduce uncertainty. The prophet tried to accomplish this with the magic available at the time. Today, the simulation analyst uses the modern magic of mathematical principles, experimentation, computer science and statistics. Computer simulation can be classified as a branch applied mathematics. The use of computer simulation increased due to availability of computing power and improvements in programming languages. Added to this are inherent difficulties or even impossibilities to accurately describe complex real world systems using analytical or purely mathematical models. For these reasons, a tool that can represent these complexities accurately is required. Computer simulation can be broadly defined as: “Using a computer to imitate the operations of a real world process or facility according to appropriately developed assumptions taking the form of logical, statistical, or mathematical relationships which are developed and shaped into a model.” The result can be manipulated by varying a set of input parameters to help an analyst understand the underlying system’s dynamics. The model typically is evaluated numerically over a simulated period of time and data is gathered to estimate real world system characteristics. Generally, the collected data is interpreted with statistics like any experiment. Computer simulation can be an expensive, time consuming, and complicated problem solving technique. Therefore, certain circumstances warrant its use. Situations well suited to its application include the following (Table 1.1): Real system does not yet exist and building a prototype is cost prohibitive, time-consuming or hazardous. Aircraft, Production System, Nuclear Reactor System is impossible to build. National Economy, Biological System Real system exists but experimentation is too expensive, hazardous or disruptive to conduct. Proposed Changes to a Materials Handling System, Military Unit, Transportation System, Airport Baggage Handling System Forecasting is required to analyze long time periods in a compressed format. Population Growth, Forest Fire Spread, Urbanization Studies, Pandemic Flu Spread Mathematical modeling has no practical analytical or numeric solution. Stochastic Problems, Nonlinear Differential Equations Using computer simulation for analysis has many advantages over other decision making techniques. Among these advantages are: 1. Allows Experimentation without Disruptions to Existing Systems - In systems that already exist, testing new ideas may be difficult, costly, or impossible. Simulation allows a model to be developed and compared to the system to ensure it accurately reflects current operation. Any desired modifications can be made to the model first, the impact on the system examined, and then a decision to implement the changes in the real world system can be made. Example: Changing the Line An automotive assembly line may run 24 hours a day, seven days a week. Shutting the line down, putting in a temporary modification (which may or may not speed up a process), and resuming production would be very expensive. Example: Adding Equipment Unless the machinery was already purchased, the process of adding it to the line and trying it firsthand would be impossible. 2. Concept can be Tested Prior to Installation - A computer simulation will allow concepts to be tested prior to the installation of new systems. This testing may reveal unforeseen design flaws and give designers a tool for improvement. If the same flaws were discovered after installation, changes to the system might end up being very costly or even impossible to implement. Example: Purchase of an Automatic Storage and Retrieval system (ASRS) Engineers in a medium sized company decide to replace an existing warehouse with an ASRS. After stretching their tight budget to its outer limits, the equipment was procured and installed. Several months of usage revealed the new system was unable to keep up with the demands for pallets being entering and removed from the racks. Their assembly line process began to bog down because material wasn't arriving from storage in a timely fashion. The budget was gone and upper management told manufacturing to live with their decision. The entire situation could have been avoided by simulating the ASRS system prior to purchase. 3. Detection of Unforeseen Problems or Bugs - When a system is simulated prior to installation and found to work in concept, the model is often refined to include finer details. The detailed simulation may reveal unforeseen problems or bugs that may exist in the system's design. By discovering these problems prior to installation, debug time and rework costs can be avoided. In addition, improvements to system operation may be discovered. ﻿Simulation languages are versatile, general purpose classes of simulation software that can be used to create a multitude of modeling applications. In a sense, these languages are comparable to FORTRAN, C#, Visual Basic.net or Java but also include specific features to facilitate the modeling process. Some examples of modern simulation languages are GPSS/H, GPSS/PC, SLX, and SIMSCRIPT III. Other simulation languages such as SIMAN have been integrated into broader development frameworks. In the case of SIMAN, this framework is ARENA. Simulation languages exist for discrete, continuous and agent-based modeling paradigms. The remainder of this book will focus on the discrete event family of languages. Specialized features usually differentiate simulation languages from general programming languages. These features are intended to free the analyst from recreating software tools and procedures used by virtually all modeling applications. Not only would the development of these features be time consuming and difficult, but without them, the consistency of a model could vary and additional debugging, validation and verification would be required. Most simulation languages provide the features shown in Table 2.1. 1) Simulation clock or a mechanism for advancing simulated time. 2) Methods to schedule the occurrence of events. 3) Tools to collect and analyze statistics concerning the usage of various resources and entities. 4) Methods for representing constrained resources and the entities using these resources. 5) Tools for reporting results. 6) Debugging and error detection facilities. 7) Random number generators and related sets of tools. 8) General frameworks for model creation. Although many models are written using simulation languages, some analysts still prefer to rely on traditional programming languages for model development. In other cases, extensions to a language are developed to add capabilities to a traditional language. For instance, Repast Simphony is a free and open source, agent-based modeling toolkit that adds features to Java in order to simplify model creation and use. This blended approach provides the advantages of both the traditional language and the simulation modeling extensions. The motivations behind using a general purpose language include: Programmer familiarity: Developers already know the general purpose programming language. They may not have the time or inclination to learn a simulation language. Flexibility: Programming languages inherently are flexible, giving the analyst freedom to create the model using his or her preferred methodology. Cost: Programming language software is usually more accessible and far less expensive than specific simulation software. This may not always be true since several leading simulation languages can be downloaded for no cost. However, other leading simulation language packages can be very expensive. Hardware Concern: General purpose software may be available on any hardware platform while some simulation languages may require special machines and memory configurations. Lack of Analyst Knowledge: The analyst may not understand simulation languages and may lack knowledge on the advantages of using a simulation language package. Training: Available classes in the use of traditional languages are more likely to be available than specialty simulation training. Although traditional languages do offer some advantages, most of these are outweighed by features standard to many simulation languages. In a typical modeling application, the programmer or analyst will find the initial investment in a simulation language more than pays off. A simulation language will provide a savings in coding, debugging, analysis of results, and in making changes. A variety of simulation languages exist and are used by businesses, researchers, manufacturing and service companies, and consultants. The next sections briefly discuss two common simulation languages: GPSS and SIMSCRIPT. GPSS: General Purpose Simulation System (GPSS) was originally developed by Geoffrey Gordon of IBM and released in October of 1961. Following IBM’s release of GPSS to the public domain, it became a multivendor simulation language and has been in continuous use since. In general, GPSS enjoys widespread popularity due to its sensible world view and overall power. Its basic functions can be easily learned while powerful features make it ideal for modeling complex systems. In general, GPSS is used to simulate queuing systems that consist of customer entities interacting and completing in a system of constrained resources. The resources are structured as networks of blocks that entities (also called transactions) enter and use to perform various tasks in certain amounts of simulated time. As entities move through these networks, which have been organized to represent a real world system, statistics are collected and used to determine if the system contains bottlenecks, is over or under utilization, or exhibits other characteristics. Output data is made available for analysis at the end of a production run. Presently, several vendors offer versions of GPSS. Included are: Wolverine Software which produces GPSS/H, a powerful, state-of-the-art version of GPSS engineered to allow creation of large, complex models (http://www.wolverinesoftware.com). Minuteman Software which produces a user friendly GPSS simulation environment called GPSS World that features special model development tools (http://minutemansoftware.com). ngolf Ståhl and Beliber AB which produce WebGPSS, a stream-lined version of GPSS, with a focus simulation and modeling concept education (http://www.webgpss.com). SIMSCRIPT III: This language is a direct descendant of the original SIMSCRIPT language produced at Rand Corporation in the 1960's. SIMSCRIPT III has constructs that allow a modeler to approach a problem from either a process or an event oriented world view. SIMSCRIPT III offers unique features which add to its appeal. Among these are: • Object-Oriented Programming • Modularity • SIMSCRIPT III Development Studio (SimStudio) • Object-Oriented Simscript III graphics • Data Base Connectivity SDBC In general, SIMSCRIPT III is a free form language with English-like syntax. This syntax allows the code in the system to become self-documenting. Model components can be programmed clearly enough to provide an excellent representation of the organization and logic of the system being simulated. SIMSCRIPT III is maintained and distributed at http:// www.simscript.com by CACI Products Company. Most discrete event simulation languages model a system by updating the simulation clock to the time that the next event is scheduled to occur. Events and their scheduled times of occurrence are maintained automatically on one of two ordered lists: the current events chain or the future events chain. The current events chain keeps a list of all events that will (or may) occur at the present clock time. The future events chain is a record of all events that can occur at some point in the future. A simulation clock moves to the next event on the future events chain and changes the system state of the model based on that event’s characteristics. ﻿There has been a long running debate that concentrates on trying to decide whether simulation should be defined as an art or a science. Those who believe it to be a science feel that statistics, mathematics, and computer science comprise its foundation and are the basis for this classification. Others feel that the skill of the modeling team, the creativity involved in developing the model, and the interpretation of the results all add up to an individualized art. In this author's opinion, there is no real way to scientifically structure a simulation to guarantee that its results are valid. Instead, after the model has been coded and run, the outputs can be studied and compared with corresponding known values to determine suitability. If no known values exist then the modeler must rely on his instincts and the judgment of experts to make this determination. The creativity and instincts used are akin to an art. Much of the methodology involved in model creation and analysis are based on computer science and mathematical principles. Therefore, elements of art and science exist in modeling. Simulation best may be defined as a “soft-science” containing both. Figure 3.1 depicts the spectrum extending from art to science and the simulation's place. In recent years simulation has been moving along the spectrum more toward the science end. Improvements in simulation languages and the advent of simulators have removed some of the need to be as creative and innovative. Much of the uncertainty in model creation has also been eliminated through the development of application specific languages. Numerous explanations exist for reasons behind the phenomenal growth computer simulation has experienced both in terms of application areas and in the number of available software products. Among these reasons are: 1. Improvements in computers: The first simulations were done on large, room-sized mainframe computers. These computers relied on card decks and operated most often in the batch mode. Not many people had access to these mammoth devices. In the last thirty years, computers have been reduced in size and cost considerably. Equivalents of the mainframes that used to occupy large rooms are now carried around in briefcase sized packages. Computers have moved from research laboratories and can now be found on practically every desk in all industries. The computing power of a single chip is fast becoming all that is necessary to run the most sophisticated commercial simulation software. This widespread availability and reduction in cost of computers has enabled simulation to prosper. 2. Improvements in simulation products: Thirty years ago, most simulation work was done using assembly language, FORTRAN, or other high level languages. Development time was much greater, as was debugging, statistic tabulation, and the reliability of results. This situation began to change with the advent of GPSS in 1961. Since that time, a multitude of simulation languages, analysis programs, animators, and pre-programmed simulators have become available. Much of the development time required to create a model has been eliminated through standard features found in these products. In addition to performing the necessary simulation functions, varying degrees of user friendliness are available. Simulation languages such as Simscript and GPSS/H appeal to practitioners with programming skills while non-programmers can enjoy the mouse driven menus found in many application specific simulators. 3. New opportunities for simulation education: Three decades ago, very few universities offered discrete event simulation classes. Today many universities offer simulation classes in engineering and business curriculums. In addition, private seminars and training sessions are available. These sessions are sponsored by simulation software vendors, consultants, and corporations with an interest in simulation and modeling. Other sources of simulation education are trade magazines, academic publications, conferences, societies, and books. 4. Increasingly complex and technical work environments: During the previous few decades the average work environment has changed from simple manual assembly lines to complex automated systems. Many repetitive human tasks have been replaced with robots and factory automation equipment. Conveyors, forklift trucks, storage and retrieval racks, as well as many other factory floor items are now routinely controlled by programmable logic controllers or computers. This new complexity has made factory output rates very difficult to predict and manage. Thus, the need for better analysis tools arose. New simulation techniques evolved to satisfy this demand and were able to remove much of the guess work and uncertainty in the work place. 5. Computer literacy among analysts and engineers: Computer literacy and use is nearly ubiquitous among professionals. Everyone has the ability to receive computer training. 6. Competition and tightening budgets - Another factor adding to growth in simulation use is an emphasis on lowering overhead costs, reducing labor requirements, and streamlining operations. Much of this has come about as globalization has increased competition and businesses compete on an international basis. 7. Realization of the benefits offered by simulation: As more industries recognize the benefits of simulation, investing in capabilities to use these tools becomes more important. Apparent economic advantages have prompted companies to invest time and resources into this area. 8. Industrial peer pressure: Organizations without simulation capabilities have found themselves at a competitive disadvantage. This was most apparent in industries, such as materials handling, where a simulation study accompanying a quotation would lend credibility to a proposed system and often become a determining factor when the contract was awarded. A situation of industrial peer pressure was created. Purchasers would inquire why simulation was not being used by their vendors and demand that some type of through-put guarantee be given. Presently, most materials handling system request-for-quotations require modeling be performed prior to purchase. Similar requirements exist in other industries. These forces have been a key factor in popularizing simulation. 9. Warm fuzzy feeling: Many companies have developed internal simulation groups to model inhouse problems, proposed systems, and existing manufacturing processes. Presentation materials such as simulation animation packages have helped to sell internal proposals to management and create a corporate warm fuzzy feeling. “Seeing is believing” and many simulation packages available today place an emphasis on graphics and output. 10. Part of an effort to increase quality: Another force related to simulation adoption is a commitment to quality improvement processes. The success of these philosophies has been demonstrated in many industries with large productivity gains and improved profitability. A major precept of quality is prevention of problems. By creating a model of systems to be designed, purchased, or installed, costly mistakes can be avoided and installations can be done right the first time. ﻿A computer simulation project is more than purchasing modeling software, plugging in a few values and expecting an answer to a question to pop out. This chapter considers the broader picture that simulation encompasses by developing a methodology for starting the process correctly. However, before that is discussed, a failed project is examined. Simulations can go terribly wrong, particularly if the appropriate infrastructure is not carefully developed to support the modeling effort. Few simulation analysts can report that all of their models are flawless. The previous example analyzed a real simulation project that became disastrous and the steps taken to revive it. Although the model had been accurate, the simulation received criticism and blame for the problems. Several pitfalls can be identified by looking at the process in retrospect. 􀁸First: The simulation analyst was inexperienced. She lacked practical knowledge of the system's operation and this slowed initial model development. 􀁸Second: The simulation 'team' members didn't participate adequately. Although the model demonstrated problems did exist, time was not taken to solve these problems as a preventative measure. Rather, the system was installed and then problems were tackled at a much higher cost. 􀁸Third: Goals changed mid-simulation effort. The original simulation was meant to discover if the would work. When production fell short, a new goal of 'making it work' became the directive. The problems pointed out by this case study could have been avoided if the simulation effort had been taken more seriously. If preventative action had addressed system inadequacies early, costly corrections and time delays could have been avoided. A recent study of discrete event computer simulation projects revealed unsuccessful efforts are characterized by high costs, model size constraints, and slow software. In contrast, successful projects are characterized by teamwork, cooperation, mentoring, effective communication of outputs, high-quality vendor documentation, easily understood software syntax, higher levels of analyst experience, and structured approaches to model development. While these findings should not come as a surprise, the importance of having a structured methodology for approaching a simulation is apparent. The next sections of this textbook provide a detailed view of an approach to help enable simulation project success. The process used in this textbook is representative of many possible computer simulation project life cycles that have been used in practice and documented in simulation textbooks and journal articles. However, remember each simulation project is unique and specific requirements may be better served through customizing the steps discussed. In general, the simulation project life cycle described in this book consists of six major steps, each having multiple components. The major steps being investigated are: 1. Intelligence Phase. The precursor to any solution is full development and an understanding of the underlying dilemma or problem. 2. Managerial Phase. In general, this step requires interaction with management and other nontechnical staff of the organization to acquire necessary resources and support along with the formation of a simulation project team. 3. Developmental Phase. During this phase, the simulation model or models are created. System design, detail design, and coding all take place and rely on the interaction of the analysts and other members of the simulation team. 4. Quality Assurance Phase. While the model is being coded in complete or at least in prototype form, the analyst must ensure proper validation and verification. The ideas of testing and completion or integration are also important here. It may be necessary take testing one step further and begin development of face validity through interaction with end-users and management. Generally, quality assurance accompanies the entire modeling lifecycle. 5. Implementation Phase. Model use begins and decision support activities take place. 6. Operations, Maintenance, and Archival Phase. Development is essentially complete and only maintenance tasks remain. This phase may be substantial if the project results in a simulation maintained for repeated use A simulation project can be a complex organizational undertaking. In general, to believe a simulation project starts and ends with coding a model would be a mistake. Much more is involved. In many instances, modeling is one small component in a much larger simulation project life cycle. As stated previously, a simulation project is inclusive of a wide spectrum of activities ranging from system and problem definition to ensuring logic developed in the model can bridge the gap between model and real world and be used in actual system implementation. The following sections provide a closer look at the mechanics of conducting a professional simulation study. 4.1 Intelligence The intelligence phase of the simulation life cycle involves understanding the environment and determining problems to be solved. In general, this phase consists of the simulation analyst or potential simulation customer discovering situations that require modeling. The simulation analyst should emerge from this phase with knowledge that a problem exists and should have at least a preliminary understanding of the problem’s nature. Often, problem definition and feasibility are assessed at this point in time. 4.1.1 Problem Definition In order to ensure simulation project success, several events need to occur before any other work begins. First, the customer needs to define objectives for the study. The objectives for the simulation describe what questions the simulation needs to answer. In other words, the precursor to any solution is a full understanding of the underlying dilemma or problem. Parameters for the problem as well as broad scale constraints need to be defined. A simulation project may be driven by a broad statement such as ‘The Denver Chamber of Commerce has projected that incoming passenger flights will increase by 50% over the next 18 months.” Definition of the problem operationalizes the statement and looks at the system environment to clarify goals for analysis. For example the problem definition may become: Can the existing baggage handling system handle the project increase in passenger arrivals and departures? Of course, this question could rapidly be replaced with a related: What needs to be changed in order to accommodate the expected increase in passenger arrivals and departures? These sorts of clarifications need to be made early in the simulation life cycle. ﻿Data model design belongs to the first development phase of a Laboratory Information Management System (LIMS). After model design, LIMS should be developed to enable a flexible integration of the heterogeneous data types mentioned above, data sources, and applications. Such systems should provide well defined user and data interfaces and fine grained user access levels. Consequently, following the specific aims must be considered for LIMS development: • Design and development of the LIMS including: o An integrated laboratory notebook to store the necessary information during biomaterial manipulation. o A laboratory information management system to keep track of the information that accrues during production in multiwell plates and the screening. o Well defined data interfaces for importing, exporting, and handling data. o An Plug-in Architecture (PA) to connect other bio applications and link to its data without amending the LIMS code. o A web-service interface to allow external applications such as data mining tools to query and read the stored data and to write back results. o The management of experimental data coming from various types of investigations. • Initiation, design and implementation of a user management system that provides libraries and interfaces which can be integrated in any application to facilitate user authentication and authorization. • Initiation of database and a web portal to browse and upload screening results and screening datasets in order to analyze the compound image analysis in the context of several biological assays. Currently, there are many LIMS available in life sciences (Table 3). The LIMS is a customizable software package and analysis platform designed to be installed in HCS laboratory and to serve many users simultaneously via the web or desktop client. LIMS should be able to import data into the database, group plate data together into experiments, and in a uniform and streamlined fashion, apply filters and transformations and run analyses. To facilitate online collaboration, users can share almost any object within the database with another user. Data can be exported in a multitude of formats for local analysis and publication. Compounds of a library stored in a library database can be interactively linked with the next module called HCS Results Database. The entry results data can begin with the definition of a project, screen, run and all experimental protocols presented in Figure 9, goes through definitions of biomaterials used, cell culture conditions, experimental treatments, experimental designs, definition of experimental variables, to definition of experimental and biological replicates and finally ends with the selection of the compound library for the screen. The user of the LIMS should easily simulate the project hierarchy via additional GUI interfaces which simulate cases that exist in a real screening process. The database should facilitate remote entry of all information concerning the screen, where users may create associations of labeled extracts and substances, scanned raw images from microscope and quantification matrices (files with results after image analysis). The user may wish to create associations of labeled extracts, scanned raw images, quantification matrices. As a single compound located in one well of a multiwell plate can be scanned in an automated screening microscope and/or under different settings. The researchers that use LIMS are in most cases organized in groups and each user belongs to one or more groups. The purpose of the groups is to define a set of users with common permissions over elements of the system, in other words, the subsets of plates that a group of users can view or use. The groups allow the assignment and management of permissions very easily, but also provide enough granularity to control access of the different users to the subsets and plates. A typical HCS unit and their users are composed by different groups and laboratories, each of them working in different projects. The manager is able to control privileges and is able to create at least one group for LIMS users or research group. A specific research group will work with a set of plates and the rest of laboratories should not have access to those plates. In many cases, there are three types of users or level access in LIMS systems: • Manager: This type of user is the responsible of introducing, maintaining and updating the data about plates and reporters in the database system. Additionally, the manager defines the screen, protocols, robots and external databases and assigns the adequate permissions to the rest of users for visualizing the subsets of plates. The manager has total access to the application and can do any modification within the database. • Researcher: The researcher represent the most general user of the LIMS. This access is limited to the visualization and searching of the data from plates. A researcher typically corresponds to a scientist of the institute or the laboratory. • Guest: This user access has the same privileges as the researcher, the difference is that it should be used by different people to access LIMS. The manager must carefully handle the permissions of subsets, and allow the visualization of these elements to the guest only if the data are to be published. HCS data is usually exported from LIMS to third party systems, for either further analysis “warehousing” purposes or archiving. Linkage at the data level via an export is a simple means to deliver HCS data into the enterprise as well as integrate HCS data into laboratory workflows. The informatics architecture therefore needs to support the necessary relational data structures to permit annotation, such as sample identifiers for compounds. In order to push data into the enterprise and link it in, format neutral export tools are required. Over the past years XML (eXtensible Markup Language9) has arisen as the format of choice for data export, as it is self-explaining format (i.e., not only does it contain the data to export but a description of the data in the same file). Any software can interpret XML and it can be translated into other formats, if necessary. Data-level integration has certain advantages: It is relatively straightforward to implement, almost any data can be integrated, and few changes, if any, are required by either the source or target applications. Disadvantages are that an additional copy of the data is made and there may not be$$$﻿Data model design belongs to the first development phase of a Laboratory Information Management System (LIMS). After model design, LIMS should be developed to enable a flexible integration of the heterogeneous data types mentioned above, data sources, and applications. Such systems should provide well defined user and data interfaces and fine grained user access levels. Consequently, following the specific aims must be considered for LIMS development: • Design and development of the LIMS including: o An integrated laboratory notebook to store the necessary information during biomaterial manipulation. o A laboratory information management system to keep track of the information that accrues during production in multiwell plates and the screening. o Well defined data interfaces for importing, exporting, and handling data. o An Plug-in Architecture (PA) to connect other bio applications and link to its data without amending the LIMS code. o A web-service interface to allow external applications such as data mining tools to query and read the stored data and to write back results. o The management of experimental data coming from various types of investigations. • Initiation, design and implementation of a user management system that provides libraries and interfaces which can be integrated in any application to facilitate user authentication and authorization. • Initiation of database and a web portal to browse and upload screening results and screening datasets in order to analyze the compound image analysis in the context of several biological assays. Currently, there are many LIMS available in life sciences (Table 3). The LIMS is a customizable software package and analysis platform designed to be installed in HCS laboratory and to serve many users simultaneously via the web or desktop client. LIMS should be able to import data into the database, group plate data together into experiments, and in a uniform and streamlined fashion, apply filters and transformations and run analyses. To facilitate online collaboration, users can share almost any object within the database with another user. Data can be exported in a multitude of formats for local analysis and publication. Compounds of a library stored in a library database can be interactively linked with the next module called HCS Results Database. The entry results data can begin with the definition of a project, screen, run and all experimental protocols presented in Figure 9, goes through definitions of biomaterials used, cell culture conditions, experimental treatments, experimental designs, definition of experimental variables, to definition of experimental and biological replicates and finally ends with the selection of the compound library for the screen. The user of the LIMS should easily simulate the project hierarchy via additional GUI interfaces which simulate cases that exist in a real screening process. The database should facilitate remote entry of all information concerning the screen, where users may create associations of labeled extracts and substances, scanned raw images from microscope and quantification matrices (files with results after image analysis). The user may wish to create associations of labeled extracts, scanned raw images, quantification matrices. As a single compound located in one well of a multiwell plate can be scanned in an automated screening microscope and/or under different settings. The researchers that use LIMS are in most cases organized in groups and each user belongs to one or more groups. The purpose of the groups is to define a set of users with common permissions over elements of the system, in other words, the subsets of plates that a group of users can view or use. The groups allow the assignment and management of permissions very easily, but also provide enough granularity to control access of the different users to the subsets and plates. A typical HCS unit and their users are composed by different groups and laboratories, each of them working in different projects. The manager is able to control privileges and is able to create at least one group for LIMS users or research group. A specific research group will work with a set of plates and the rest of laboratories should not have access to those plates. In many cases, there are three types of users or level access in LIMS systems: • Manager: This type of user is the responsible of introducing, maintaining and updating the data about plates and reporters in the database system. Additionally, the manager defines the screen, protocols, robots and external databases and assigns the adequate permissions to the rest of users for visualizing the subsets of plates. The manager has total access to the application and can do any modification within the database. • Researcher: The researcher represent the most general user of the LIMS. This access is limited to the visualization and searching of the data from plates. A researcher typically corresponds to a scientist of the institute or the laboratory. • Guest: This user access has the same privileges as the researcher, the difference is that it should be used by different people to access LIMS. The manager must carefully handle the permissions of subsets, and allow the visualization of these elements to the guest only if the data are to be published. HCS data is usually exported from LIMS to third party systems, for either further analysis “warehousing” purposes or archiving. Linkage at the data level via an export is a simple means to deliver HCS data into the enterprise as well as integrate HCS data into laboratory workflows. The informatics architecture therefore needs to support the necessary relational data structures to permit annotation, such as sample identifiers for compounds. In order to push data into the enterprise and link it in, format neutral export tools are required. Over the past years XML (eXtensible Markup Language9) has arisen as the format of choice for data export, as it is self-explaining format (i.e., not only does it contain the data to export but a description of the data in the same file). Any software can interpret XML and it can be translated into other formats, if necessary. Data-level integration has certain advantages: It is relatively straightforward to implement, almost any data can be integrated, and few changes, if any, are required by either the source or target applications. Disadvantages are that an additional copy of the data is made and there may not be
EN19	1	﻿Digital technology has become widespread and encompasses virtually all aspects of our everyday lives. We could see it being used in computers and related gadgets, entertainment, automation (robotics), medical etc. Though physical quantities measured in the real world are analogue, most of these are processed by digital means. In order to do this, we have to convert the measured analogue quantity into digital, process the digital quantity using digital circuitry and then reconvert to analogue. The contents of this book concentrate on the digital circuit design to enable the processing of the digital quantity. But before we look into the principles of such designs, we need to understand the basics of number systems. Decimal number system is the commonly used number system that has ten digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. It is also known as base (or radix) ten system since it has ten digits that can be used to represent any number. Figure 1.1 shows the positional values or weights of the decimal number system for an integer. The digit with least weight (i.e. the one on the foremost right) is known as the least significant digit (LSD) while the highest weight digit is known as the most significant digit (MSD). In the example shown in Figure 1.1, the MSD is digit 6 while the LSD is digit 3. Figure 1.2 shows the case for fractional decimal number. While decimal number system is the commonly used number system in everyday lives, digital devices uses only binary number system that consists of 0 and 1. The base is two for this system and Figure 1.3 show an example of binary number for decimal equivalent of 6.2510 Similarly, octal and hexadecimal (hex in short) number systems have number bases of 8 and 16. For octal number system, the eight digits are 0, 1, 2, 3, 4, 5, 6, and 7 while hexadecimal number system has 16 digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, and F. Figure 1.4 gives examples on these number systems. It is often necessary to convert a number from one base system to another. Converting a number to decimal is rather straightforward as we have seen in the previous examples. The weights or positional values (for the appropriate base) are multiplied with the digit and summed to give the decimal value. In this section, we will look at methods to convert numbers from decimal to binary, octal and hex. Other conversions such as octal to binary (and vice versa), binary to hex, hex to binary, octal to hex and hex to octal are also possible. There are two methods that can be used to achieve decimal to binary conversion. The first method is by presenting the decimal value in units, tens, hundreds etc. For example: The problem with this method is that certain positional values (such as 22 and 20 in the example above) can easily be forgotten. There is another method called repeated division that is more frequently employed. Figure 1.5 illustrates this method. It works by repeated division with a value of 2 (until the quotient is 0) and the remainder digits from each step represent the binary number (in reverse order). Similarly, we can convert a decimal number to octal and hex. Figures 1.6 and 1.7 illustrate the steps for these conversions. Do remember that the final answer is in the reverse order! Any binary number can be converted to octal simply by grouping them in groups of three digits. For example, 1001011108 can be converted to 4568 as shown in Figure 1.8 (a). The reverse procedure of converting an octal number to binary can be done by writing three binary digit equivalent for each octal digit. This is shown in Figure 1.8 (b). Similar to octal number, binary number can be converted to hex simply by grouping them in groups of four digits. For example, 100101112 can be converted to 9716 as shown in Figure 1.9 (a). A hex number can be converted to binary by writing four binary digit equivalent for each hex digit. This is shown in Figure 1.9 (b). In this section, several other commonly used codes will be discussed. ASCII stands for American Standard Code for Information Interchange. Characters such as ‘a’, ‘A’, ‘@’, ‘$’ each have a code that is recognised by the computer. Standard ASCII has 128 characters (represented by 7 binary digits; 27=128), though the first 32 is no longer used. Extended ASCII has another 128 characters, mostly to represent special characters and mathematical symbols such as ‘ÿ’, ‘ė’, ‘Σ’, and ‘σ’. Table 1.1 shows the standard ASCII code. BCD is actually a set of binary numbers where a group of four binary numbers represent a decimal digit. As there are 10 basic digits in the decimal number system, four binary digits (bits) are required1. Figure 1.10 shows an example, while Table 1.2 gives the BCD code. Gray code is another commonly encountered code system. The main feature of this code is that only one bit changes between two successive values. This system is less prone to errors and is considered very useful for practical applications such as mechanical switches and error correction in digital communication as compared to the standard binary system. Table 1.3 gives the BCD code with 4 bits (i.e. up to decimal value of 15). The basic building blocks for digital circuits are logic gates. Most logic gates are binary logic, i.e. have two states of 0 or 1. The input or output of these logic gates can only exist in one of these states, where a positive logic system treats 0 as FALSE value and 1 as TRUE value and conversely for the negative logic system. Figure 2.1 shows a logic waveform that is logic 1 between time t1 and t2 and is logic 0 at other times. Positive logic will be assumed throughout the book except where denoted otherwise. ﻿Basically AND gate is composed of two inputs and a single output as shown in Figure 2.5 with algebraic representation4 BAF⋅=or simply . The traditional symbol shown in Figure 2.5(a) is more commonly employed in text books. However, the IEEE/ANSI symbol as shown in Figure 2.5(b) is gaining popularity and has the advantage of containing qualifying symbols inside the logic-symbol that describes the operation of the gate. The truth table that gives the output F for inputs A and B is given in Table 2.1. It can be seen that the output is LOW (FALSE) when any one of the inputs is LOW (FALSE) and the output is only HIGH (TRUE) when all the inputs are HIGH (TRUE). AND gate inputs do not have to be limited to two; there can be any number of inputs greater than one as shown in Figure 2.6. Timing diagram is useful in describing the relationship between the inputs and output of a logic gate. The inputs of a digital logic gate can be shown diagrammatically as a waveform that represents the changing values over time. A waveform corresponding to the changing values of the inputs over time will be generated at the output of the logic gate. Figure 2.7 show examples of timing diagram waveform for equal and unequal mark-space cycles. The mark represents the time for logic level HIGH, while the space represents the time for logic level LOW. Equal mark-space requires periodic clock pulse5. All the discussion in this book will be using equal mark-space timing waveforms only. Figure 2.8 shows an example of a timing diagram for a two-input AND gate. At each time block, the inputs A and B affect the output F. For example, in time block t0 to t1, both inputs are LOW, so the output is also LOW. Similarly, the entire timing waveform for the output can be obtained using AND operation of inputs in each time block. OR gate as shown in Figure 2.9 has algebraic representation,BAF+=. The truth table that gives the output F for inputs A and B is given in Table 2.2. It can be seen that the output is HIGH when any one of the inputs is HIGH and the output is only LOW when all the inputs are LOW. Similar to AND gate, there can be any number of inputs greater than one as shown in Figure 2.10. Figure 2.11 shows an example of a timing diagram for a two-input OR gate. At each time block, the inputs A and B affect the output F. For example, in time block t5 to t6, one input is HIGH, so the output is HIGH. Similarly, the entire timing waveform for the output can be obtained using OR operation of inputs in each time block. NOT gate is also known as INVERTER as it inverts (complements) the input logic level. It is shown in Figure 2.12 and has only one input and one output with algebraic representation of AF= or 'AF=. The bubble in the symbol denotes inversion (without it, the symbol will represent a buffer gate that does not alter the logic level; in IEEE/ANSI standard, the bubble is replaced by a triangle). The truth table for NOT gate is given in Table 2.3. NOT gate can also be connected in cascade and a few examples are shown in Figure 2.13. It should be obvious that odd number of NOT gate connections give output logic level that is complement to the input logic level and an even number of NOT gates connections give output logic level that is the same as the input logic level. It is useful to know that AND gate logic can be easily implemented using OR gate and vice versa through a simple process using additional NOT gates. For example, an AND gate equivalent can be constructed with an OR gate with both the inputs and outputs inverted through NOT gates. Figure 2.14 shows an example with equivalent truth table in Table 2.4. This is actually DeMorgan’s first theorem, which will be discussed in detail in Chapter Three. It is mentioned here so that the reader is aware that it is possible to implement one gate logic with another gate(s). NAND and NOR gates that will be discussed in the following section are known as universal gates as combinations of these gates are sufficient to obtain equivalent operation of OR, AND or NOT gates. However, this is different to the implementation discussed in Section 2.4 as either NAND or NOR gates on their own will be sufficient to implement logic function of any of the other gates. NAND gate logic symbol is shown in Figure 2.15 (note the addition of a bubble when compared to AND gate) and its truth table is shown in Table 2.5. A NAND gate operation can also be obtained through cascade operation of AND and NOT gates as shown in Figure 2.16. Algebraically, the operation can be defined as. Figure 2.17 shows an example for implementing an AND gate using NAND gates only. The blue shaded tiny bubble represents branch-off of the signal and should not be confused with the empty bubble that is used to represent inversion operation. Similarly, other gates such as OR and NOT can be implemented using NAND gates and these are left as exercises for the reader. NOR gate is basically an OR gate with the output inverted. Figure 2.18 shows the logic symbol with truth table shown in Table 2.6. Algebraically, the operation can be defined asBAF+=. Similar to NAND gate, several NOR gates can be used to implement AND, OR or NOT gates. An example of this is shown in Figure 2.19 and the reader can easily verify through the use of truth tables that All the gates that we have discussed in this chapter are manufactured as integrated circuit (IC) with several gates in one IC. For example, 74LS00 is a transistor-transistor logic (TTL) technology based IC that has four (quad) two-input NAND gates. Complementary Metal-Oxide Semiconductor (CMOS) is another technology that is widely used for manufacturing IC but TTL devices are more commonly employed for laboratory experiments as they are more robust to electrostatic noise. Figure 2.20 shows the pin configuration of 74LS00 and Figure 2.21 shows an example of pin configurations to implement NOT operation. ﻿In the previous chapter, operation and truth tables of single gates were discussed. However, in practise, single gates are seldom useful and combinations of several gates are employed for a particular application. For example, see Figure 3.1 where different gates are used to obtain the output F. Very often, there is the need to simplify logic circuits (whenever possible). For example, the circuit shown in Figure 3.1 requires four gates but equivalent logic output can be obtained with just two gates by simplifying the expression as follows: BBA is zero due to the presence of BB as shown in the truth table given in Table 3.1. The simplified circuit is given in Figure 3.2. Table 3.2 gives the truth table and it can be seen that the outputs given by expressions )(CBBAF+=and CBAF= are the same. The above simplification may not be clear at this stage but that will be the purpose of the following sections to look into Boolean algebra that will be useful to simplify logic circuits. Not only will the simplification result in lower cost, smaller and simpler design (since fewer gates will be used), it will also reduce other complications such as overheating and propagation delay. Basic axioms of Boolean algebra are shown in Table 3.3, while Table 3.4 shows the Boolean theorems for operation of a single variable and a constant (either 0 or1). Boolean algebra satisfies commutative and associative laws. Therefore, the order of variables in a product or sum does not matter and the order of evaluating sub-expression in brackets does not matter. For example: Boolean algebra also satisfies the distributive law where the expression can be expanded by multiplying out the terms. For example: It should be evident by now that when an expression contains AND and OR, AND operator takes precedence over OR operator. DeMorgan’s theorem is very useful to simplify expressions when they contain a bar (inversion) over more than a single variable. It states that an inverted expression can be replaced by its individual inverted variables but with AND replaced by OR and vice versa. For example: The following examples show the usefulness of using DeMorgan’s theorem. Note that from now on, the use of óeò˜AND (⋅) sign in the expression will be dropped for simplicity sake unless noted otherwise, so CBAF⋅⋅=will be written as . In this section, several examples are given to illustrate simplification using Boolean algebra and DeMorgan’s theorem: As another example, consider the circuit diagram given in Figure 3.4 which can be simplified as The correctness of the simplified expression can be verified by constructing a truth table and comparing the output from both expressions. The simplified logic circuit diagram is shown in Figure 3.5 where only five gates are required as opposed to six gates in the original circuit. It can be seen that there is no input A as its logic value does not affect the output based on the simplified expression. While the expression for the logic circuit shown in Figure 3.5 is simplified to single literals, it is interesting to note that another equivalent logic circuit shown in Figure 3.6 only requires four gates asDCBDCB If complement inputs are available, then the simplified circuit shown in Figure 3.5 will only require two gates as shown in Figure 3.7. To conclude the chapter, it is useful to look at two more frequently used gates: Exclusive OR (XOR) and Exclusive NOR (XNOR). These gates would be useful when circuitry such as half adders and full adders are discussed in later chapters. XOR gate as shown in Figure 3.8 has algebraic representation,BABAF+=or more commonly written as BAF⊕=. The truth table that gives the output F for inputs A and B is given in Table 3.5. It can be seen that when both inputs have the same logic value, the output is LOW. The output is HIGH when the input logic values are dissimilar, i.e. one LOW and one HIGH. XNOR gate is simply XOR with an inversion. The gate is shown in Figure 3.9 and has algebraic representation, The truth table is given in Table 3.6. The output is HIGH when both inputs have the same logic value. The output is LOW when the input logic values are dissimilar, i.e. one LOW and one HIGH. Table 3.7 shows the Boolean algebra for XOR operation. XOR operation is also both commutative and associative: ABBA⊕=⊕ and CBACBACBA⊕⊕=⊕⊕=⊕⊕)()(. As mentioned earlier, XOR gates are useful when designing more advanced circuitry such as adders, but these are also used in parity checker devices. Parity checker is used to reduce errors from transmitting a binary code across a communication channel. For example, if the seven bit ASCII code for W, 1010111 (see Table 1.1) is to be transmitted, an eight parity bit is appended at the beginning of the code. This parity bit will either be 0 or 1 depending on whether even or odd parity is required. Assuming that it is even parity checker, then the total number of bits will be even. In this case, the parity bit will be 1 and code to be transmitted will be 11010111. XOR gates can be used as even parity checker. For example, with three inputs, the expression will be CBAF⊕⊕= and the output is HIGH if one of the inputs or all three inputs are HIGH. Similarly, for eight inputs, the output is HIGH when odd number of inputs is HIGH. Figure 3.10 shows the logic circuit using seven two-input XOR gates where the bits representing the code are A0, A1,…., A6 and the parity bit is P. The output F will be HIGH when odd number of inputs is HIGH. So if the code is not transmitted correctly (say resulting in odd number of 1s), then the LED will light-up to show that an error has occured. On the other hand, with correct transmission, the number of 1s will be even and the output will be low (i.e. LED will not light-up). It should be obvious that XNOR gates can be used as odd parity checker as the output will be HIGH only when even number of inputs is HIGH. ﻿Biological signal analysis1 encompasses several interdisciplinary topics that deal with analysing signals generated by various physiological processes in the human body. These signals could be electrical, chemical or acoustic in origin and an analysis of these signals are often useful in explaining and/or identifying pathological conditions of the human body. However, these signals in their rawest form do not provide much information and therefore, the motivation behind biological signal analysis is to extract (i.e. to reveal) the relevant information. This analysis has become even more important with modern healthcare striving to provide cost effective point-of care diagnosis and personalised treatment. Furthermore, fast computing power in recent years has made much of the more complex analysis methodologies possible. The purpose of this chapter is to provide an overview of biological signal origins and describe commonly encountered biological signals. Before we delve into the analysis of biological signals, it would be useful to understand that they are often represented as discrete in time. For example, Figure 1.1 shows an example of a sinus rhythm electrocardiogram (ECG), which represents the electrical activity obtained from normal heart. A single measurement of the signal x is a scalar and represents the electrical signals generated by the mechanisms in the heart (the details of this process follow later) at a particular instant of time t (denoted with index n) rather than at all points of time (the involved sampling process to obtain discrete time measurements from continuous signals will be discussed in the following chapter). There are two types of noise inherent in this signal: baseline and powerline interference. Figure 1.2 shows a cleaned version of Figure 1.1 obtained through band-pass filtering, which is a subject of discussion in Chapter Four. This figure also shows a zoomed version of the ECG representing one beat of heart. The sharp peaks in the signal denote the occurrence of what is known as the R wave and the time intervals between consecutive R-R peaks would be useful to measure the heart rate, i.e. the number of times the heart beats in a minute. Similarly, the segment from the Q wave to the S wave (more commonly known as QRS segment), is useful in indicating certain pathological2 variations in the heart’s electrical system. For example, Figure 1.3 shows an ECG waveform from a subject with an ectopic beat of Premature Ventricular Contraction (PVC) type. Ectopic beats are premature beats with a complex waveform that can occur occasionally in most people. However, the presence of frequent ectopic beats (more than six per minute) could indicate a serious fatal problem if left uncorrected. While it is visually obvious that the QRS segment for the PVC beat is different from Figure 1.1, it is impractical to sit and manually detect the occurrences from an ECG chart (or on-screen monitor) of a patient over many hours/days. Rather, a computer that is trained to automatically analyse and detect the occurrence of such beats using signal processing algorithms is employed. This is a typical biological signal analysis application. Figure 1.4 shows the typical signal analysis methodologies involved that would be necessary for such a computer based detection of ectopic beats; the details of which will be discussed generally in the forthcoming chapters and specifically in the final chapter. Commonly encountered biological signals that describe the electrical activity of the brain, heart, muscles etc will be introduced in this section. Some of these signals like ECG and electroencephalogram (EEG) are spontaneous activity of the human body while others such as evoked potentials are signals in response to external stimulus, for example the presentation of visual stimuli which results in visual evoked potential (VEP) signals. While some analysis procedures are common (like using filters to extract components in specific frequency range), the different properties of these biological signals do call for the application of widely varying analysis procedures. Hence, it is useful to know the fundamental details of how these signals are generated before studying analysis procedures for extraction of the required information. The ECG is a representation of the electrical activity of the heart and the cardiac rhythm is controlled by the pacemaker cells known as sinoatrial (SA) node. The PQRST waveform (as shown in Figure 1.5) represents one complete ECG cycle: P wave occurs when SA node fires and the impulse spreads across atria and triggers atrial contraction; PQ interval (isometric segment) is the propagation delay when the impulse travels from atria to ventricles (allowing blood flow to complete in similar direction); QRS complex occurs when the impulse spreads to ventricles and triggers ventricular contraction; ST segment is the period when the ventricles are depolarised and ventricular repolarisation (relaxation) begins and T wave represents the return to resting state by the ventricles [2]. These electrical impulses are normally recorded through electrodes placed on particular areas of the body either using 12-channel ECG in a hospital or 3-channel ECG in the field. Figure 1.6 shows the hypothetical Einthoven’s triangle that is commonly used for the electrode setup. The setup requires four limb electrodes placed on the right and left arms and legs; the fourth limb electrode that is not shown in the diagram is placed on the right leg and serves as reference channel. The top of the Einthoven’s triangle forms Lead I, while the left forms Lead II and right forms Lead III. Each lead represents a different aspect of the heart’s electrical system. The voltage differences between the limb electrodes: left arm (LA), right arm (RA), and left leg (LL) are used to obtain Leads I, II and III [3, 4]: It should be obvious that the three leads have the relationship: These three leads are normally sufficient to detect life threatening abnormal ECG rhythms (arrhythmias) and will often be used in this book. Einthovan’s triangle electrode setup does also give an additional three leads and these augmented limb leads (aVF, aVL, and aVR) can be computed by [3, 4]: The six precordial leads provide a more detailed view of the heart’s electrical activity compared to the six limb leads. These six precordial leads can be obtained by placing electrodes on the chest as shown in Figure 1.7.$$$﻿Biological signal analysis1 encompasses several interdisciplinary topics that deal with analysing signals generated by various physiological processes in the human body. These signals could be electrical, chemical or acoustic in origin and an analysis of these signals are often useful in explaining and/or identifying pathological conditions of the human body. However, these signals in their rawest form do not provide much information and therefore, the motivation behind biological signal analysis is to extract (i.e. to reveal) the relevant information. This analysis has become even more important with modern healthcare striving to provide cost effective point-of care diagnosis and personalised treatment. Furthermore, fast computing power in recent years has made much of the more complex analysis methodologies possible. The purpose of this chapter is to provide an overview of biological signal origins and describe commonly encountered biological signals. Before we delve into the analysis of biological signals, it would be useful to understand that they are often represented as discrete in time. For example, Figure 1.1 shows an example of a sinus rhythm electrocardiogram (ECG), which represents the electrical activity obtained from normal heart. A single measurement of the signal x is a scalar and represents the electrical signals generated by the mechanisms in the heart (the details of this process follow later) at a particular instant of time t (denoted with index n) rather than at all points of time (the involved sampling process to obtain discrete time measurements from continuous signals will be discussed in the following chapter). There are two types of noise inherent in this signal: baseline and powerline interference. Figure 1.2 shows a cleaned version of Figure 1.1 obtained through band-pass filtering, which is a subject of discussion in Chapter Four. This figure also shows a zoomed version of the ECG representing one beat of heart. The sharp peaks in the signal denote the occurrence of what is known as the R wave and the time intervals between consecutive R-R peaks would be useful to measure the heart rate, i.e. the number of times the heart beats in a minute. Similarly, the segment from the Q wave to the S wave (more commonly known as QRS segment), is useful in indicating certain pathological2 variations in the heart’s electrical system. For example, Figure 1.3 shows an ECG waveform from a subject with an ectopic beat of Premature Ventricular Contraction (PVC) type. Ectopic beats are premature beats with a complex waveform that can occur occasionally in most people. However, the presence of frequent ectopic beats (more than six per minute) could indicate a serious fatal problem if left uncorrected. While it is visually obvious that the QRS segment for the PVC beat is different from Figure 1.1, it is impractical to sit and manually detect the occurrences from an ECG chart (or on-screen monitor) of a patient over many hours/days. Rather, a computer that is trained to automatically analyse and detect the occurrence of such beats using signal processing algorithms is employed. This is a typical biological signal analysis application. Figure 1.4 shows the typical signal analysis methodologies involved that would be necessary for such a computer based detection of ectopic beats; the details of which will be discussed generally in the forthcoming chapters and specifically in the final chapter. Commonly encountered biological signals that describe the electrical activity of the brain, heart, muscles etc will be introduced in this section. Some of these signals like ECG and electroencephalogram (EEG) are spontaneous activity of the human body while others such as evoked potentials are signals in response to external stimulus, for example the presentation of visual stimuli which results in visual evoked potential (VEP) signals. While some analysis procedures are common (like using filters to extract components in specific frequency range), the different properties of these biological signals do call for the application of widely varying analysis procedures. Hence, it is useful to know the fundamental details of how these signals are generated before studying analysis procedures for extraction of the required information. The ECG is a representation of the electrical activity of the heart and the cardiac rhythm is controlled by the pacemaker cells known as sinoatrial (SA) node. The PQRST waveform (as shown in Figure 1.5) represents one complete ECG cycle: P wave occurs when SA node fires and the impulse spreads across atria and triggers atrial contraction; PQ interval (isometric segment) is the propagation delay when the impulse travels from atria to ventricles (allowing blood flow to complete in similar direction); QRS complex occurs when the impulse spreads to ventricles and triggers ventricular contraction; ST segment is the period when the ventricles are depolarised and ventricular repolarisation (relaxation) begins and T wave represents the return to resting state by the ventricles [2]. These electrical impulses are normally recorded through electrodes placed on particular areas of the body either using 12-channel ECG in a hospital or 3-channel ECG in the field. Figure 1.6 shows the hypothetical Einthoven’s triangle that is commonly used for the electrode setup. The setup requires four limb electrodes placed on the right and left arms and legs; the fourth limb electrode that is not shown in the diagram is placed on the right leg and serves as reference channel. The top of the Einthoven’s triangle forms Lead I, while the left forms Lead II and right forms Lead III. Each lead represents a different aspect of the heart’s electrical system. The voltage differences between the limb electrodes: left arm (LA), right arm (RA), and left leg (LL) are used to obtain Leads I, II and III [3, 4]: It should be obvious that the three leads have the relationship: These three leads are normally sufficient to detect life threatening abnormal ECG rhythms (arrhythmias) and will often be used in this book. Einthovan’s triangle electrode setup does also give an additional three leads and these augmented limb leads (aVF, aVL, and aVR) can be computed by [3, 4]: The six precordial leads provide a more detailed view of the heart’s electrical activity compared to the six limb leads. These six precordial leads can be obtained by placing electrodes on the chest as shown in Figure 1.7.
EN24	0	﻿Programming is not only coding. Primarily it implies structuring of the solution to a problem and then refine the solution step by step. When refined to a level deep enough, you have created an algorithm. Then it is time to translate each step of the algorithm to program code. Suppose you have a problem that needs to be solved. Then you begin with writing a sequence of operations at an overview level that need to be performed to solve the problem. Then you start from the beginning again and focus on one operation at a time and find out whether the operation needs to be refined to more detailed steps. Then you proceed to the next level and refine further. This refinement process goes on until you arrive at a level deep enough to start coding. Creating an algorithm to solve a problem is in general the most laboursome task of the programming work. Many people do the mistake of starting to code at once, which makes you focus on code details and forget the actual problem to be solved. That gives an unstructured and inefficient code hard to understand and maintain. That’s why we emphasize that you structure your logic train of thought and construct a good algorithm before starting to code. A JSP graph is a tool to create an algorithm. JSP is an abbreviation for Jackson Structured Programming and is a commonly used instrument for logic structuring. Let’s take an example. You are supposed to create a program that calculates the price of a product to be bought by a customer. The customer specifies the product id and the requested quantity. The program should then calculate the relevant discount, add tax and show the customer price. A JSP graph could look as follows: University West, Trollhättan Structured Programming with C++ Department of Economy and Informatics calculate the relevant discount, add tax and show the customer price. A JSP graph could look as follows The upper box is the name of the program. We have split the solution into four steps at an overview level. You read the steps from left to right. As you probably realize the algorithm is too rough to be able to write code. So we proceed by refining the solution to the next level: The box ”Enter information” has been split into two steps, “Show instructions to the user” and “Read product id & quantity”. In the same way we have refined the box “Deduct discount”. We could break down the box ”Calculate gross price” further: We could refine the algorithm further, but let us say that we are satisfied with the detail level. The shadowed boxes in the graph are the end points at the lowest level, the “leaves of the tree”. These are the boxes to be used for coding, from left to right. We will work a great deal with JSP graphs in the program examples of the course. Each program is logically built up from three basic logic principles: • Sequence – the program performs instructions in sequence, one after the other. • Selection – the program selects one of several operations depending on the prerequisites. The program thus makes a selection based on some condition. • Iteration – the program repeats a series of instructions a certain number of times. The logic principles can also be combined. For instance, a sequence of instructions can be repeated a number of times if a specific condition is satisfied, otherwise another sequence of instructions should be performed a specific number of times. All programming languages use these three logic principles. If you have built up your algorithm in a correct way, it is only a question of selecting a programming language when the coding is to take place. The price calculation algorithm above should consequently give the same result irrespective of whether the code is written in C++, Java or VisualBasic. In the JSP graph above the box “Calculate gross price” is refined in a sequence of three operations, from left to right: • Look for the product in product file • Pick the product’s price • Multiply by quantity The box ”Look for the product in product file” could suggest an iteration, e.g. “Read next product id until we find the product id stated by the user”. The discount calculation in the price program above could imply a more differentiated discount situation: • If the gross price is between 100:- and 500:- the customer will get 5% discount. • If the gross price is between 500:- and 1000:- the customer will get 8% discount. • If the gross price is above 1000:- the customer will get 10% discount. Here the program must do a selection. When you have refined your algorithm to a level detailed enough, it is time to write code. This written code is called source code. The code must of course follow the rules in effect for the programming language in question, it must follow the syntax. Each programming language has its own rules. You can in principle use any word processor or text editor you like, such as the program Notepad, Wordpad or Word. If you use word processors like Wordpad or Word, you must save the file as pure text file (.txt). It is however recommended to use the text editor present in the program development package you are using. The advantage is that you will get some support when coding. Microsoft Visual C++, which is the program development package used in this course, contains an editor which: • Shows key words in C++ in blue colour and comments in green colour, • Provides IntelliSense, i.e. proposes code alternatives in certain situations, • Supports context sensitive help, i.e. shows an explanation of a certain code item if you put the cursor on the item and press F1, • Provides extensive support at debugging by allowing you to execute the code up to a certain breakpoint, where you can examine variable values at this particular position. There are other development tools for C++ like Borland and Dev C++. The tools differ somewhat as concerns small details of the code. You can use any tool, the important thing is that you learn to “think” structured programming. In this course we have used Microsoft Visual C++ 2008, and all program examples are tested in this environment. ﻿In this chapter you will learn what a variable is, how to declare a variable, i.e. tell the computer that there is a variable in the program, and how to assign values to variables. You will also learn how to perform simple mathematic calculations, how to read values from the keyboard, how to display information on the screen and control where on the screen the information will be displayed. We will also present a number of programming examples with JSP graphs. A variable is used by the program to store a calculated or entered value. The program might need the value later, and must then be stored in the computer’s working memory. Example: Here we have selected the name ’dTaxpercent’ to hold the value 0.25. You can in principle use any variable name, but it is recommended to use a name that corresponds to the use of the variable. When the variable name appears in a calculation the value will automatically be used, for example: means that 1500 will be multiplied by 0.25. The purpose of declaring a variable is to tell the program to allocate space in the working memory for the variable. The declaration: tells that we have a variable with the name iNo and that is of integer type (int). You must always specify the data type to allocate the correct memory space. An integer might for instance require 4 bytes while a decimal value might require 16 bytes. How many bytes to allocate depends on the operating system. Different operating systems use different amounts of bytes for the different data types. The variable name should tell something about the usage of the variable. Also, a standard used by many people is to allocate 1-3 leading characters to denote the data type (i for integer). Note that each program statement is ended by a semicolon. Below we declare a variable of decimal type: double means decimal number with double precision. Compare to float which has single precision. Since double requires twice as many bytes, a double variable can of course store many more decimals, which might be wise in technical calculations which require high precision. You can declare several variables of the same type in one single statement: The variables are separated by commas. Note that C++ is case sensitive, i.e. a ‘B’ and ‘b’ are considered different characters. Therefore the variables: are two different variables. Now we have explained how to declare variables, but how do the variables get their values? Look at the following code: Here the variable dTaxpercent gets the value 0.25, the variable iNo the value 5 and the variable dUnitprice the value 12. The equal character (=) is used as an assignment operator. Suppose that the next statement is: In this statement the variable iNo represents the value 5 and dUnitprice the value 12. The right part is first calculated as 5 * 12 = 60. This value is then assigned to the variable dTotal. Note that it is not the question of an equality in normal math like in the equation x = 60, where x has the value 60. In programming the equal sign means that something happens, namely that the right part is first calculated, and then the variable to the left is assigned that value. C++ performs the math operations in correct order. In the statement: the multiplication dTotal * dTaxpercent will first be performed, which makes 60 * 0.25 = 15. The value 15 will then be added to dTotal which makes 60 + 15 = 75. The value 75 will finally be assigned to the variable dToBePaid. If C++ would perform the operations in the stated order, we would get the erroneous value 60 + 60, which makes 120, multiplied by 0.25, which makes 30. If you need to perform an addition before a multiplication, you must use parentheses: Here the parenthesis is calculated first, which gives 1.25. This is then multiplied by 60, which gives the correct value 75. It is possible to initiate a variable, i.e. give it a start value, directly in the declaration: Here we declare the variable dTaxpercent and simultaneously specify it to get the value 0.25. You can mix initations and pure declarations in the same program statement: In addition to assigning the dTaxpercent a value, we have also declared the variables dTotal and dToBePaid, which not yet have any values. In the statement: we have initiated several variables and declared the variable iSum. Sometimes a programmer wants to ensure that a variable does not change its value in the program. A variable can of course not change its value if you don’t write code that changes its value. But when there are several programmers in the same project, or if a program is to be maintained by another programmer, it might be safe to declare a variable as a constant. Example: The key word const ensures that the constant dTaxpercent does not change its value. Therefore, a statement like this is forbidden: A constant must be initiated directly by the declaration, i.e. be given a value in the declaration statement. Consequently the following declaration is also forbidden: We have seen how a variable can be initiated in the declaration and how the variable can be assigned a value in other parts of the program. A variable can also get new values several times in the program. A variable can furthermore be changed by originating from the current value of the variable. The following example shows how the variable iNo is decreased by 2: As we have previously said the right part will first be calculated and then be assigned to the variable on the left side. Suppose that the variable iNo from the beginning has the value 5. The right part will then be 5-2 = 3. 3 is then assigned to the variable to the left, i.e. iNo. The effect of this statement is thus that iNo changes its value from 5 to 3, i.e. is decreased by 2. A more compact way of coding giving the same result is: ﻿In this chapter you will learn to incorporate intelligence into your programs, i.e. the program can do different things depending on different conditions (selections). You will also learn how to repeat certain tasks a specific number of times or until a specific condition is fulfilled (iteration, loop). We will introduce new symbols in our JSP graphs to illustrate selections and loops. A selection situation can be illustrated by the following figure: If the condition is fulfilled (yes option) the program will do one thing, else (no option) another thing. The keyword if introduces the if statement. The condition is put within parentheses. If the condition is true statement1 will be performed, otherwise statement2. Here is a code example: The values of two variables are compared. If a is greater than b, the variable greatest will get a’s value. Otherwise, i.e. if b is greater than or equal to a, greatest will get b’s value. The result from this code section is that the variable greatest will contain the greatest of a and b. Sometimes you might want to perform more than one statement for an option. Then you must surround the statements with curly brackets: If the condition is true all statements in the first code section will be executed, otherwise all statements in the second code section will be executed. Example: If a is greater than b, the variable greatest will get a’s value and the text “a is greatest” will be printed. Otherwise the variable greatest will get b’s value and the text “b is greatest” will be printed. Sometimes you don’t want to do anything at all in the else case. Then the else section is simply omitted like in the following example: If the variable sum is greater than 1000 the variable dDiscPercent will get the value 20 and the text “You will get 20% discount” will be printed. Otherwise nothing will be executed and the program goes on with the statements after the last curly bracket. We will now create a program that calculates the total price of a product. The user is supposed to enter quantity and price per unit of the product. If the total exceeds 500:- you will get 10 % discount, otherwise 0 %. We start with a JSP graph: All boxes except “Calculate discount” are rather simple to code. “Calculate discount” requires a closer examination. It has a condition included which says that the discount is different depending on whether gross is less or greater than 500. We’ll break down that box: A conditional situation in JSP is identified by a ring in the upper right corner of the box. That implies that only one of the boxes will be executed. Here is the code: The declaration shows a constant dLimit, which later is used to check the gross value. The variable iNo is used to store the entered quantity and dUnitPrice is used for the entered unit price. It is common among programmers to use one or a few characters in the beginning of the variable name to signify the data type of the variable. The variable iNo has first character I (integer), and the variable dUnitPrice has d (double). After data entry the gross is calculated by multiplying the two entered values (quantity * unit price). That value is stored in the variable dGross. The if statement then checks the value of dGross. If greater than dLimit (i.e. 500) the variable dDisc will get the value 10, otherwise 0. dDisc contains the discount percent to be applied. The net is then calculated by subtracting the discount percent from 100, which then is multiplied by dGross and divided by 100 (to compensate for the percent value). Finally the total price is printed. In the if statements in previous example codes we have so far only used the comparison operator > (greater than). Here is a list of all comparison operators: In some situations you will need to check whether a number is evenly dividable by another number. Then the modulus operator % is used. Below are some code examples of how to check whether a number is odd or even, i.e. evenly dividable by 2. We will now study an example of a more complicated situation. Suppose the following conditions prevail: If a customer buys more than 100 pieces, he will get 20% discount. Otherwise if the quantity exceeds 50, i.e. lies in the interval 50-100, he will get 10%. Otherwise, i.e. if the quantity is below 50, no discount is given. The situation is shown by the following JSP graph: Here we use the keyword else if. You can use any number of else if-s to cover many conditional cases. The situation with different discount percentages for different quantity intervals can be solved in another way, namely by combining two conditions. In common English it can be expressed like this: If the quantity is less than 100 and the quantity is greater than 50, the customer will get 10% discount. Here we combine two conditions: - If the quantity is less than 100 and - and the quantity is greater than 50 The combination of the conditions means that the quantity lies in the interval 50-100. Both conditions must be fulfilled in order to get 10%. The conditions are combined with “and” which is a logical operator. It is written && in C++. The code will then be: Suppose the situation is this: If the quantity is greater than 100 or the total order sum is greater than 1000, the customer will get 20% discount. Here we combine the conditions: - If the quantity is greater than 100 eller - or the total order sum is greater than 1000 In both cases the customer has bought so much that he will get 20% discount. One of the conditions is sufficient to get that discount. The conditions are combined with the logic operator “or”, which is written || in C++. The code for this situation will be: In many situations you cannot predict what a user is going to enter. It might happen that the user enters characters when the program expects integers, or that he does not enter anything at all but just press Enter. Then you can use conditional input: ﻿In this chapter you will learn what an array is, namely a method of storing many values under a single variable name, instead of using a specific variable for each value. We will begin by declaring an array and assign values to it. In connection with arrays you will have great use for loops, by means of which you can efficiently search for a value in the array and sort the values. Arrays is a fundamental concept within programming which will frequently be used in the future. An array is, as already mentioned, a method of storing many values of the same data type and usage under a single variable name. Suppose you want to store temperatures measured per day during a month: If you didn’t know about arrays, you would need 30 different variable names, for instance: This is a bad option, especially if you want to calculate the average temperature or anything else. Then you would need to write a huge program statement for the sum of the 30 variables. Instead, we use an array, i.e. one single variable name followed by an index within square brackets that defines which of the temperatures in the array that is meant: The name of the array is temp. The different values in the array are called elements. In this way we can use a loop, where the loop variable represents the index, and do a repeated calculation on each of the temperatures: The loop variable i goes from 1 to 30. In the first turn of the loop i has the value 1, which means that temp[i] represents temp[1], i.e. the first temperature. In the second turn of the loop i has the value 2 and temp[i] represents the second temperature. By using a loop the amount of code required will not increase with the number of temperatures to handle. The only thing to be modified is the number of turns that the for loop must execute. In the code below we calculate the average of all the temperatures: The variable iSum is set to 0 since it later on will be increased by one temperature at a time. The loop goes from 1 to 30, i.e. equal to the number of elements in the array. In the loop body the variable iSum is increased by one temperature at a time. When the loop has completed, all temperatures have been accumulated in iSum. Finally we divide by 30 to get the average, which is printed. Like for all variables, an array must be declared. Below we declare the array temp: The number within square brackets indicates how many items the array can hold, 31 in our example. 31 positions will be created in the primary memory each of which can store a double value. The indeces will automatically be counted from 0. This means that the last index is 30. If you need temperatures for the month April, which has 30 days, you have two options: 1. Declare temp[30], which means that the indeces goes from 0 to 29. 1st of April will correspond to index 0, 2nd of April to index 1 etc. 30th of April will correspond to index 29. The index lies consequently one step after the actual date. 2. Declare temp[31]. Then 1st of April can correspond to index 1, 2nd of April to index 2 etc. 30th of April will correspond to index 30. The date and index are here equal all the time. This means that the item temp[0] is created ”in vain” and will never be used. It is no big deal which of the methods you use, but you will have to be conscious about the method selected, because it affects the code you write. We will show examples of both methods. Note that, in the declaration: all elements are of the same data type, namely double. For arrays all elements all items always have the same data type. You can assign values to an array already at the declaration, e.g.: Here the array iNo will hold 5 items, where the first item with index 0 gets the value 23, the second item with index 1 the value 12 etc. The enumeration of the values must be within curly brackets, separated by commas. As a matter of fact it is redundant information to specify the number of items to 5 in the declaration above, since the number of enumerated values is 5. Therefore you could as well write: An enumeration within curly brackets can only be written in the declaration of an array. For instance, the following is erroneous: In the following code section we declare an array of integers and assign values to the array in the loop: The array iSquare is declared to hold 11 items of integer type. The loop then goes from 0 to 10. In the first turn of the loop i is =0 and the item iSquare[0] gets the value 0*0, i.e. 0. In the second turn of the loop i is =1 and iSquare[1] gets the value 1*1, i.e. 1. In the third turn of the loop the item iSquare[2] gets the value 2*2, i.e. 4. Each item will contain a value equal to the square of the index. As a C++ programmer you must yourself keep track of the valid index interval. The result could be disastrous if you wrote: This means that we store the value 23.5 in the primary memory at an adress that does not belong to the array, but might belong to a memory area used for other data or program code. If you run such a code the system might in worst case break down and you will have to restart the computer. Suppose we want to copy the temperatures from the array with April’s values to an array with June’s values. You cannot copy an entire array in this way: You will have to copy the values item by item by means of a loop: Here the loop goes from 1 to 30 and we copy item by item for each turn of the loop. ﻿A string is a text, i.e. a sequence of characters (letters, digits and other special characters). String handling is a little tricky in C++, so we have dedicated an separate chapter to this subject. Actually, a string is an array that consists of a number of items, where each item is a character in the string. There are a number of string handling functions. We will in this chapter get acquainted with the most common and usable string functions, like calculating the length of a string, copying a string, concatenating strings and picking out parts of a string. In programming, string handling is important, since a user often enters information in text form which is taken care of by the program. We will go through several programming examples, where we will have use of our string knowledge. We will first get to know the most simple of all strings, namely the one containing just one character. To store one character in a variable we use the data type char. Below we declare a string variable of the char type: The variable cLetter can now contain any one character. We can assign a value to the variable: Note that we use single quotes for the character. We can also, like for all kinds of variables, assign a value directly in the declaration: Our first program shows how to handle entry of characters by the user to select a menu option. The program will first display a menu: Here the user can enter one of the letters A, B, C or D to choose an item. We will not build a full-featured order/invoice/ warehouse/ finance system, but we will only let the program print a text about the selected choice. We start with a JSP graph: The first action is that the menu is displayed. Then the user is prompted for a choice by means of the letters A-D. Finally the requested option is executed, i.e. a simple text message will be printed. If the user enters another character than A-D, an error message is printed. First a char variable is declared named cSel, and then the menu is printed with a number of cout statements. The subsequent cin statement reads a character from the user to the variable cSel, which then is checked in the switch statement. The switch statement contains one case section for each option. Note that each case line has the character A-D within single quotes, which is necessary since it is a char variable that is checked. The default section takes care of all characters other than A, B, C or D. We will now improve our menu program so that the user repeatedly can enter different options without terminating the program. We then put the entire menu printing and switch statement in a loop. The JSP graph will then be: We complete the menu with still another option, X – Exit. As long as the user does not enter X, the loop proceeds. Furthermore we also clean the screen before the menu is displayed, which is the first operation of the loop. Here is the code: To be able to clear the screen we need the header file stdlib.h. W use a do loop, where the condition is checked after the loop to ensure that the loop is run at least once. The first action in the loop is to clear the screen with system(“cls”). Then the menu is printed and the user is prompted for an option, i.e. a character to be stored in the variable cSel. That variable is checked in the switch statement, where a text is printed corresponding the selected option. If the user enters ‘X’, the text ‘You selected to exit’ will be printed, and the loop is terminated since the while condition specifies that cSel must not equal ‘X’. The method of letting the user enter an extra character to the variable temp at the end of the loop is a relatively unconvenient solution, but it has the advantage of avoiding to struggle with special C++ features regarding input, which we don’t go into here. We will now create a logically rather complex program that uses char variables to print a number of ’X’ on the screen with the shape of a Christmas tree: As you can see the tree has eleven branches and two ’X’:s to the trunk. We therefore need an outer loop that runs eleven times, where each turn prints a branch. Each branch consists of a number of ‘X’:s, different depending on which branch being printed. Furthermore we will have to print a suitable number of blanks before the ‘X’:s, so that the branches are centered symmetrically around the middle trunk. We therefore need two inner loops, one that prints the leading blanks and one that prints the ‘X’:s for each branch. After completion of the branches we need a loop that runs two turns and that prints the ‘X’:s for the trunk. We start with a JSP graph: The JSP graph tells us that there is one outer loop for the branches, where each turn of the loop prints a branch. We then have one inner loop that prints the correct number of blanks and one inner loop that prints the ‘X’:s. The same is true for the trunk where we have an outer loop where each turn of the loop prints one line of the trunk, and one inner loop that prints the blanks before one single ‘X’ is printed. We declare a variable named x that correspond to the ’X’, and a variable named blank corresponding to the blank character. The first outer for-loop has an i as loop counter and goes from 10 to 0 (11 turns). We have selected to let the values run backwards to make the subsequent math easier. i is consequently a line counter for the branches of the tree. The first inner loop has a j as loop counter and goes from 0 to i. It prints the correct number of blanks for each branch. Since i is counted reversed, the number of blanks will decrease for each branch. Each turn of the loop prints one blank. After completion of the blanks for a bransch, first an ’X’ is printed. Since the number of ’X’:s at each bransch is odd, the remaining number of ’X’:s to be printed is even. ﻿Chapter 7 focuses on taking a simulation project from start to finish. Although the system might seem simplistic and could probably be adequately modeled using a spreadsheet, the intent is to demonstrate the thought process used in defining, analyzing, and applying modeling principles. The system to be analyzed provides a good framework for discussion and will be interesting to a wide variety of simulation (or potential simulation) analysts. The project will follow the simulation life cycle illustrated in Chapter 4 (See Figure 7.1). The intelligence phase of the simulation life cycle involves understanding the environment and determining problems to be solved. This often starts with a general understanding of the system to be modeled together with a problem definition and feasibility considerations. DePorres Tours wants to create a simulation to help determine bus capacity for their tour operation and then be able to utilize the model in the future to pre-test any changes to their operation. Their current problem statement becomes: “What size bus will best accommodate expected customer traffic?” Specifically, they are looking at buying either a 24 or a 48 passenger bus. Before beginning the simulation project, DePorres assessed general feasibility using the TELOS approach (See Figure 7.2): DePorres Tours recently sent Telly O’Sullivan to a simulation training course and has allocated 4 hours a day for him to gather information and conduct the simulation project. Since it is his first project, he has been given a month to complete the model. DePorres Tours owner, the indomitable Kafy DePorres has informed all drivers, tour guides, and office personnel about the project and strongly encouraged their full cooperation. Additionally, a small team of one driver, one tour guide, and Kafy herself will meet with Telly weekly to assess his progress and help with any problems. Kafy has set aside adequate funds for the project and plans to acquire a simulation software package for model development. Telly, anxious to apply what he learned in class, developed a working view of the system to be modeled. He used the following definition to help break the tour bus system into its major components. System: A set of components or elements that are related to each other in such a manner as to create a connected whole. Although the table contains some repetitive information, it becomes a working document to help understand the system and its basic elements. Based on the data shown in Table 7.2, the system environment can be defined as the road system in Chicago, existing traffic patterns, and tourism patterns. All these things lie outside the system and have an influence on its behavior. The systems boundary separates the tour bus system, together with tourists, driver, route, and stops from the environment. Telly decided the scope and scale of his model would include the following: Scope: 1) Customer arrivals and departures will be modeled at each bus stop. 2) Bus driver and bus will be modeled. 3) Bus travel times along the route will be modeled. Scale: 1) Two customer types will be considered: first time riders and those using the bus as a taxi. 2) Bus driver and bus will be modeled as a single resource. Bus driver behavior will be averaged into bus stop times. In the future this could be redeveloped using bus driver persims. 3) Bus travel times will be modeled using collected distributions covering an entire day. Telly decided to use a process orientation view of the system. In other words, he will view the bus system as a time ordered sequence of interrelated events separated by passages of time. This will influence his choice of simulation software but is consistent with the recent training he received. Telly created a couple of concept models to help understand the system better. First he created a spreadsheet that calculated hourly capacity and average route time (Figure 7.2). Next he used an Excel spreadsheet with Paul Jensen’s ORMM Queuing Add-ins to create a rough model of the bus route if viewed as a queuing system (See Figure 7.3). Both models created quick approximations of the system. However, neither sufficiently captured the dynamics of the system adequately. This left Telly with no option other than to move ahead with a full simulation. Both concept models demonstrated a 24 passenger bus could move around 80-84 people per hour but this was in contrast to what had been observed in the real world. Telly suspected the variance in the system had not been included in spreadsheets realistically. Telly’s next task was to begin data collection in earnest. Although he had calculated quick averages he used in the concept models, he wanted to develop more accurate information. He did this by visiting each bus stop and collecting customer arrival times as they waited for the bus. Table 7.3 provides a look at the data he collected at Bus Stop #1. He loaded this into Stat::Fit 2 and determined the tourists were arriving according to a Poisson distribution with a mean of 3.58 minutes (See Figure 7.4). Telly repeated his data collection for the other 4 stops and determined the following interarrival rates (see Table 7.4). Telly continued to collect input data throughout the week and was able to determine the following quantitative information would need to be included in the model: Average Time Bus Spends at a Stop: Normally distributed 2 minutes with a standard deviation of .3 minutes. Time between Stops (see Table 7.5). • Passenger Route Decision Percentages (see Table 7.7). • Qualitative Data. The following assumptions were also gathered: 1) The bus always goes in sequence 1-2-3-4-5 then back to 1. 2) The bus stops even if no passengers are visible at the stop or desire to be let off. 3) Bus drivers change shifts without disrupting the schedule. As part of the Quality Phase of his simulation project, Telly validated his input data in the following ways: Observation: He observed the current system and double checked his input data against what he observed. Expert's Opinions: He showed the current drivers and ticket sellers his assumptions and input data for evaluation. Intuition and Experience: He also used his own experience to double check the data. Telly continued to check for validation throughout his model development process.$$$﻿Chapter 7 focuses on taking a simulation project from start to finish. Although the system might seem simplistic and could probably be adequately modeled using a spreadsheet, the intent is to demonstrate the thought process used in defining, analyzing, and applying modeling principles. The system to be analyzed provides a good framework for discussion and will be interesting to a wide variety of simulation (or potential simulation) analysts. The project will follow the simulation life cycle illustrated in Chapter 4 (See Figure 7.1). The intelligence phase of the simulation life cycle involves understanding the environment and determining problems to be solved. This often starts with a general understanding of the system to be modeled together with a problem definition and feasibility considerations. DePorres Tours wants to create a simulation to help determine bus capacity for their tour operation and then be able to utilize the model in the future to pre-test any changes to their operation. Their current problem statement becomes: “What size bus will best accommodate expected customer traffic?” Specifically, they are looking at buying either a 24 or a 48 passenger bus. Before beginning the simulation project, DePorres assessed general feasibility using the TELOS approach (See Figure 7.2): DePorres Tours recently sent Telly O’Sullivan to a simulation training course and has allocated 4 hours a day for him to gather information and conduct the simulation project. Since it is his first project, he has been given a month to complete the model. DePorres Tours owner, the indomitable Kafy DePorres has informed all drivers, tour guides, and office personnel about the project and strongly encouraged their full cooperation. Additionally, a small team of one driver, one tour guide, and Kafy herself will meet with Telly weekly to assess his progress and help with any problems. Kafy has set aside adequate funds for the project and plans to acquire a simulation software package for model development. Telly, anxious to apply what he learned in class, developed a working view of the system to be modeled. He used the following definition to help break the tour bus system into its major components. System: A set of components or elements that are related to each other in such a manner as to create a connected whole. Although the table contains some repetitive information, it becomes a working document to help understand the system and its basic elements. Based on the data shown in Table 7.2, the system environment can be defined as the road system in Chicago, existing traffic patterns, and tourism patterns. All these things lie outside the system and have an influence on its behavior. The systems boundary separates the tour bus system, together with tourists, driver, route, and stops from the environment. Telly decided the scope and scale of his model would include the following: Scope: 1) Customer arrivals and departures will be modeled at each bus stop. 2) Bus driver and bus will be modeled. 3) Bus travel times along the route will be modeled. Scale: 1) Two customer types will be considered: first time riders and those using the bus as a taxi. 2) Bus driver and bus will be modeled as a single resource. Bus driver behavior will be averaged into bus stop times. In the future this could be redeveloped using bus driver persims. 3) Bus travel times will be modeled using collected distributions covering an entire day. Telly decided to use a process orientation view of the system. In other words, he will view the bus system as a time ordered sequence of interrelated events separated by passages of time. This will influence his choice of simulation software but is consistent with the recent training he received. Telly created a couple of concept models to help understand the system better. First he created a spreadsheet that calculated hourly capacity and average route time (Figure 7.2). Next he used an Excel spreadsheet with Paul Jensen’s ORMM Queuing Add-ins to create a rough model of the bus route if viewed as a queuing system (See Figure 7.3). Both models created quick approximations of the system. However, neither sufficiently captured the dynamics of the system adequately. This left Telly with no option other than to move ahead with a full simulation. Both concept models demonstrated a 24 passenger bus could move around 80-84 people per hour but this was in contrast to what had been observed in the real world. Telly suspected the variance in the system had not been included in spreadsheets realistically. Telly’s next task was to begin data collection in earnest. Although he had calculated quick averages he used in the concept models, he wanted to develop more accurate information. He did this by visiting each bus stop and collecting customer arrival times as they waited for the bus. Table 7.3 provides a look at the data he collected at Bus Stop #1. He loaded this into Stat::Fit 2 and determined the tourists were arriving according to a Poisson distribution with a mean of 3.58 minutes (See Figure 7.4). Telly repeated his data collection for the other 4 stops and determined the following interarrival rates (see Table 7.4). Telly continued to collect input data throughout the week and was able to determine the following quantitative information would need to be included in the model: Average Time Bus Spends at a Stop: Normally distributed 2 minutes with a standard deviation of .3 minutes. Time between Stops (see Table 7.5). • Passenger Route Decision Percentages (see Table 7.7). • Qualitative Data. The following assumptions were also gathered: 1) The bus always goes in sequence 1-2-3-4-5 then back to 1. 2) The bus stops even if no passengers are visible at the stop or desire to be let off. 3) Bus drivers change shifts without disrupting the schedule. As part of the Quality Phase of his simulation project, Telly validated his input data in the following ways: Observation: He observed the current system and double checked his input data against what he observed. Expert's Opinions: He showed the current drivers and ticket sellers his assumptions and input data for evaluation. Intuition and Experience: He also used his own experience to double check the data. Telly continued to check for validation throughout his model development process.
EN30	0	﻿Digital signal processing (DSP) has become a common tool for many disciplines. The topic includes the methods of dealing with digital signals and digital systems. The techniques are useful for all the branches of natural and social sciences which involve data acquisition, analysis and management, such as engineering, physics, chemistry, meteorology, information systems, financial and social services. Before the digital era, signal processing devices were dominated by analogue type. The major reason for DSP advancement and shift from analogue is the extraordinary growth and popularization of digital microelectronics and computing technology. The reason that digital becomes a trend to replace analogue systems, apart from it is a format that microprocessors can be easily used to carry out functions, high quality data storage, transmission and sophisticated data management are the other advantages. In addition, only 0s and 1s are used to represent a digital signal, noise can easily be suppressed or removed. The quality of reproduction is high and independent of the medium used or the number of reproduction. Digital images are two dimensional digital signals, which represent another wide application of digital signals. Digital machine vision, photographing and videoing are already widely used in various areas. In the field of signal processing, a signal is defined as a quantity which carries information. An analogue signal is a signal represented by a continuous varying quantity. A digital signal is a signal represented by a sequence of discrete values of a quantity. The digital signal is the only form for which the modern microprocessor can take and exercise its powerful functions. Examples of digital signals which are in common use include digital sound and imaging, digital television, digital communications, audio and video devices. To process a signal is to make numerical manipulation for signal samples. The objective of processing a signal can be to detect the trend, to extract a wanted signal from a mixture of various signal components including unwanted noise, to look at the patterns present in a signal for understanding underlying physical processes in the real world. To analyse a digital system is to find out the relationship between input and output, or to design a processor with pre-defined functions, such as filtering and amplifying under applied certain frequency range requirements. A digital signal or a digital system can be analysed in time domain, frequency domain or complex domain, etc. Representation of digital signals can be specific or generic. A digital signal is refereed to a series of numerical numbers, such as: where 2, 4, 6 are samples and the whole set of samples is called a signal. In a generic form, a digital signal can be represented as time-equally spaced data where -1, 0, 1, 2 etc are the sample numbers, x[0], x[1], x[2], etc are samples. The square brackets represent the digital form. The signal can be represented as a compact form In the signal, x[-1], x[1], x[100], etc, are the samples, n is the sample number. The values of a digital signal are only being defined at the sample number variable n , which indicates the occurrence order of samples and may be given a specific unit of time, such as second, hour, year or even century, in specific applications. We can have many digital signal examples: -- Midday temperature at Brighton city, measured on successive days, -- Daily share price, -- Monthly cost in telephone bills, -- Student number enrolled on a course, -- Numbers of vehicles passing a bridge, etc. Examples of digital signal processing can be given in the following: Example 1.1 To obtain a past 7 day’s average temperature sequence. The averaged temperature sequence for past 7 days is For example, if n=0 represents today, the past 7 days average is where x[0], x[−1], x[−2], ... represent the temperatures of today, yesterday, the day before yesterday, …; y[0] represents the average of past 7 days temperature from today and including today. On the other hand, represents the average of past 7 days temperature observed from tomorrow and including tomorrow, and so on. In a shorter form, the new sequence of averaged temperature can be written as where x[n] is the temperature sequence signal and y[n] is the new averaged temperature sequence. The purpose of average can be used to indicate the trend. The averaging acts as a low-pass filter, in which fast fluctuations have been removed as a result. Therefore, the sequence y[n] will be smoother than x[n]. Example 1.2. To obtain the past M day simple moving averages of share prices, let x[n] denotes the close price, y [n] M the averaged close price over past M days. For example, M=20 day simple moving average is used to indicate 20 day trend of a share price. M=5, 120, 250 (trading days) are usually used for indicating 1 week, half year and one year trends, respectively. Figure 1.1 shows a share’s prices with moving averages of different trading days. Although some signals are originally digital, such as population data, number of vehicles and share prices, many practical signals start off in analogue form. They are continuous signals, such as human’s blood pressure, temperature and heart pulses. A continuous signal can be first converted to a proportional voltage waveform by a suitable transducer, i.e. the analogue signal is generated. Then, for adapting digital processor, the signal has to be converted into digital form by taking samples. Those samples are usually equally spaced in time for easy processing and interpretation. Figure 1.2 shows a analogue signal and its digital signal by sampling with equal time intervals. The upper is the analogue signal x(t) and the lower is the digital signal sampled at time t = nT, where n is the sample number and T is the sampling interval. Therefore, For ease of storage or digital processing, an analogue signal must be sampled into a digital signal. The continuous signal is being taken sample at equal time interval and represented by a set of members. First of all, a major question about it is how often should an analogue signal be sampled, or how frequent the sampling can be enough to represent the details of the original signal. ﻿A digital system is also refereed as a digital processor, which is capable of carrying out a DSP function or operation. The digital system takes variety of forms, such as a microprocessor, a programmed general-purpose computer, a part of digital device or a piece of computing software. Among digital systems, linear time-invariant (LTI) systems are basic and common. For those reasons, it will be restricted to address about only the LTI systems in this whole book. The linearity is an important and realistic assumption in dealing with a large number of digital systems, which satisfies the following relationships between input and output described by Figure 3.1. i.e. a single input [ ] 1 x n produces a single output [ ] 1 y n , Applying sum of inputs [ ] [ ] 1 2 x n + x n produces [ ] [ ] 1 2 y n + y n , and applying input [ ] [ ] 1 2 ax n bx n generates [ ] [ ] 1 2 ay n by n . The linearity can be described as the combination of a scaling rule and a superposition rule. The time-invariance requires the function of the system does not vary with the time. e.g. a cash register at a supermarket adds all costs of purchased items x[n], x[n −1],… at check-out during the period of interest, and the total cost y[n] is given by where y[n] is the total cost, and if x[0] is an item registered at this moment, x[−1] then is the item at the last moment, etc. The calculation method as a simple sum of all those item’s costs is assumed to remain invariant at the supermarket, at least, for the period of interest. Like a differential equation is used to describe the relationship between its input and output of a continuous system, a difference equation can be used to characterise the relationship between the input and output of a digital system. Many systems in real life can be described by a continuous form of differential equations. When a differential equation takes a discrete form, it generates a difference equation. For example, a first order differential equation is commonly a mathematical model for describing a heater’s rising temperature, water level drop of a leaking tank, etc: where x[n] is the input and y[n] is the output. For digital case, the derivative can be described as i.e. the ratio of the difference between the current sample and one backward sample to the time interval of the two samples. Therefore, the differential equation can be approximately represented by a difference equation: yielding a standard form difference equation: For input’s derivative, we have similar digital form as Further, the second order derivative in a differential equation contains can be discretised as When the output can be expressed only by the input and shifted input, the difference equation is called non-recursive equation, such as On the other hand, if the output is expressed by the shifted output, the difference equation is a recursive equation, such as where the output y[n] is expressed by it shifted signals y[n −1] , y[n − 2], etc. In general, an LTI processor can be represented as or a short form A difference equation is not necessarily from the digitization of differential equation. It can originally take digital form, such as the difference equation in Eq.(3.1). Alternatively, equivalent to the difference equation, an LTI system can also be represented by a block diagram, which also characterises the input and output relationship for the system. For example, to draw a block diagram for the digital system described by the difference equation: The output can be rewrite as The block diagram for the system is shown in Figure 3.2. In the bock diagram, T is the sampling interval, which acts as a delay or right-shift by one sample in time. For general cases, instead of Eq.(3.9), Eq. (3.8) is used for drawing a block diagram. It can easily begin with the input, output flows and the summation operator, then add input and output branches. Both the difference equation and block diagram can be used to describe a digital system. Furthermore, the impulse response can also be used to represent the relationship between input and output of a digital system. As the terms suggest, impulse response is the response to the simplest input – unit impulse. Figure 3.2 illustrates a digital LTI system, in which the input is the unit impulse and the output is the impulse response. Once the impulse response of a system is known, it can be expected that the response to other types of input can be derived. An LTI system can be classified as causal or non-causal. A causal system is refereeing to those in which the response is no earlier than input, or h[n] =0 before n=0. This is the case for most of practical systems or the systems in the natural world. However, non-causal system can exist if the response is arranged, such as programmed, to be earlier than the excitation. The impulse response of a system can be evaluated from its difference equation. Following are the examples of finding the values of impulse responses from difference equations Example 3.1 Evaluating the impulse response for the following systems We know that when the input is the simplest unit impulse d[n], the output response will be the impulse response. Therefore, replacing input x[n] by d[n] and response y[n] by h[n], the equation is still holding and has become special: It is easy to evaluate the impulse response by letting n=-1, 0,1,2,3,… Generally for the difference equation: The impulse response can evaluated by the special equation with the simple unit impulse input: The step response is also commonly used to characterize the relationship between the input and output of a system. To find the step response using the impulse response, we know that the unit step can be expressed by unit impulses as The linear system satisfies the superposition rule. Therefore, the step response is a sum of a series of impulse responses excited by a series of shifted unit impulses. i.e., the step response is a sum of impulse responses ﻿The previous chapters on controller design have mainly concentrated on introducing the compensator in the forward path, but use of a simple compensator in the feedback path has been discussed. Also feedback compensation has been mentioned with respect to the PI-PD controller and velocity feedback in a position control system. Both these two cases can be regarded as feedback of two states, namely, the output to form the error and the derivative of the output. It is therefore appropriate to look in general at how the performance of a control system can be changed by the feedback of state variables. If this is to be done in practice then the state variables have to be available either as measured values or estimates. Obtaining measurements can be costly because of the requirement for additional sensors so in many cases the variables are estimated using estimation methods. This is a topic outside the scope of this book but it will suffice to say that estimation methods have become relatively easy to implement with the use of modern technologies employing microprocessors with significant software included to do the required computations. In the next section results are derived for full state variable feedback and this is followed by a discussion of the linear quadratic regulator problem. The problem of direct closed loop transfer function synthesis, or standard forms, is looked at again in terms of using state variable feedback to achieve such a design. Finally as an example of the benefits of using a state variable feedback design the problem of controlling a plant having a transfer function with lightly damped complex poles, considered initially in section 9.5 is reconsidered. Consider a SISO system, G, with a state space representation (A,B,C,0). Assume state feedback is used so that the system input u K (v k T x) c 􀀠􀀐, as shown in Figure 11.1. Here the row vector kT, is given by 􀀋. . . . .􀀌1 2 3 n k T 􀀠k k k k , which means that the signal fed back and subtracted from v is n n k x k x ......k x 1 1 2 2 􀀎. The thick line is used to show that it represents more than one signal, in this case the state x which has n components. The new system, with input v, is which can be written where the matrices Now suppose the original system was in controllable canonical form so that as the matrix T c c BK k is all zeros apart from the last row. The gain vector has been subscripted by c to denote that it has state inputs from the controllable canonical form. Thus the characteristic equation of the system with state feedback is and in principle the poles can be placed anywhere by choice of the components of T c k . Larger values of the components of T c k will speed up the system response but in practice this will not be possible due to physical limitations on the magnitudes of signals for linear operation. The gain Kc is basically redundant, however, it is useful to include it as the structure might, as is clear from Figure 11.1, be a resultant closed loop system with Kc the controller gain. In this case the controller input will include the error and for this to be the case when the state x1 is the output, k1 will be equal to one. If the system is not in controllable canonical form then the coefficient terms in the characteristic equation will not each involve a single feedback gain. This means that simultaneous equations need to be solved to find the required feedback gains to give a specific characteristic equation. One way to avoid this is to transform the original system to controllable canonical form, determine the required feedback gains for this representation and then transform these gains back to the required feedback values from the original states. The system must be controllable to do this transformation and it can be shown that this is a required condition to be able to place the poles in desired locations. Thus, if the calculated state feedback gain vector is kc T from the controllable form states xc and the transformation from the original states x is x = Txc then the required vector kT for the original states, x, is obtained from the relationship kT = kc TT-1. Several algorithms are available in MATLAB which calculate the required feedback gain vector kT for a given system (A,B) to give specified pole locations The feedback signal kTx can be written in transfer function terms as k T􀀩(s)BU and the output Y(s) 􀀠C􀀩(s)BU so that in terms of the classical block diagram of Figure 5.1 the state feedback is equivalent to a feedback transfer function of ( ) ( ) ( ) C s H s k s T 􀀩 􀀩 􀀠. It can be shown [11.1] for a state space representation with matrix A and column vector B that if a performance index is to be minimised then the required control signal, u(t), is given u(t) 􀀠􀀐k T x(t) , a linear function of the state variables. Further the value of the feedback gain vector is given by k T 􀀠R􀀐1BT P where P is the unique positive definite symmetrical matrix solution of the algebraic Riccati equation (ARE) Obviously the solution for kT depends upon the choice of the positive scalar, R, and the matrix Q which must be at least semi-positive definite. Although this is a very elegant theoretical result, the solution depends on the choice of the weighting matrix Q and scalar R. No simple method is available for choosing their values so that the closed loop performance meets a given specification. A further point is that whatever values are chosen then the open loop frequency response must avoid the circle of unit radius centred at (-1,0) on the Nyquist plot [11.2]. This means a phase margin of at least 90°, which makes the design very conservative. The command [x,l,g] = care( A,B,cT*c,R) in MATLAB will return the solution P for the ARE in x, where the vector c defines the matrix Q by Q 􀀠cT * c .$$$﻿The previous chapters on controller design have mainly concentrated on introducing the compensator in the forward path, but use of a simple compensator in the feedback path has been discussed. Also feedback compensation has been mentioned with respect to the PI-PD controller and velocity feedback in a position control system. Both these two cases can be regarded as feedback of two states, namely, the output to form the error and the derivative of the output. It is therefore appropriate to look in general at how the performance of a control system can be changed by the feedback of state variables. If this is to be done in practice then the state variables have to be available either as measured values or estimates. Obtaining measurements can be costly because of the requirement for additional sensors so in many cases the variables are estimated using estimation methods. This is a topic outside the scope of this book but it will suffice to say that estimation methods have become relatively easy to implement with the use of modern technologies employing microprocessors with significant software included to do the required computations. In the next section results are derived for full state variable feedback and this is followed by a discussion of the linear quadratic regulator problem. The problem of direct closed loop transfer function synthesis, or standard forms, is looked at again in terms of using state variable feedback to achieve such a design. Finally as an example of the benefits of using a state variable feedback design the problem of controlling a plant having a transfer function with lightly damped complex poles, considered initially in section 9.5 is reconsidered. Consider a SISO system, G, with a state space representation (A,B,C,0). Assume state feedback is used so that the system input u K (v k T x) c 􀀠􀀐, as shown in Figure 11.1. Here the row vector kT, is given by 􀀋. . . . .􀀌1 2 3 n k T 􀀠k k k k , which means that the signal fed back and subtracted from v is n n k x k x ......k x 1 1 2 2 􀀎. The thick line is used to show that it represents more than one signal, in this case the state x which has n components. The new system, with input v, is which can be written where the matrices Now suppose the original system was in controllable canonical form so that as the matrix T c c BK k is all zeros apart from the last row. The gain vector has been subscripted by c to denote that it has state inputs from the controllable canonical form. Thus the characteristic equation of the system with state feedback is and in principle the poles can be placed anywhere by choice of the components of T c k . Larger values of the components of T c k will speed up the system response but in practice this will not be possible due to physical limitations on the magnitudes of signals for linear operation. The gain Kc is basically redundant, however, it is useful to include it as the structure might, as is clear from Figure 11.1, be a resultant closed loop system with Kc the controller gain. In this case the controller input will include the error and for this to be the case when the state x1 is the output, k1 will be equal to one. If the system is not in controllable canonical form then the coefficient terms in the characteristic equation will not each involve a single feedback gain. This means that simultaneous equations need to be solved to find the required feedback gains to give a specific characteristic equation. One way to avoid this is to transform the original system to controllable canonical form, determine the required feedback gains for this representation and then transform these gains back to the required feedback values from the original states. The system must be controllable to do this transformation and it can be shown that this is a required condition to be able to place the poles in desired locations. Thus, if the calculated state feedback gain vector is kc T from the controllable form states xc and the transformation from the original states x is x = Txc then the required vector kT for the original states, x, is obtained from the relationship kT = kc TT-1. Several algorithms are available in MATLAB which calculate the required feedback gain vector kT for a given system (A,B) to give specified pole locations The feedback signal kTx can be written in transfer function terms as k T􀀩(s)BU and the output Y(s) 􀀠C􀀩(s)BU so that in terms of the classical block diagram of Figure 5.1 the state feedback is equivalent to a feedback transfer function of ( ) ( ) ( ) C s H s k s T 􀀩 􀀩 􀀠. It can be shown [11.1] for a state space representation with matrix A and column vector B that if a performance index is to be minimised then the required control signal, u(t), is given u(t) 􀀠􀀐k T x(t) , a linear function of the state variables. Further the value of the feedback gain vector is given by k T 􀀠R􀀐1BT P where P is the unique positive definite symmetrical matrix solution of the algebraic Riccati equation (ARE) Obviously the solution for kT depends upon the choice of the positive scalar, R, and the matrix Q which must be at least semi-positive definite. Although this is a very elegant theoretical result, the solution depends on the choice of the weighting matrix Q and scalar R. No simple method is available for choosing their values so that the closed loop performance meets a given specification. A further point is that whatever values are chosen then the open loop frequency response must avoid the circle of unit radius centred at (-1,0) on the Nyquist plot [11.2]. This means a phase margin of at least 90°, which makes the design very conservative. The command [x,l,g] = care( A,B,cT*c,R) in MATLAB will return the solution P for the ARE in x, where the vector c defines the matrix Q by Q 􀀠cT * c .
EN18	1	﻿Programming is not only coding. Primarily it implies structuring of the solution to a problem and then refine the solution step by step. When refined to a level deep enough, you have created an algorithm. Then it is time to translate each step of the algorithm to program code. Suppose you have a problem that needs to be solved. Then you begin with writing a sequence of operations at an overview level that need to be performed to solve the problem. Then you start from the beginning again and focus on one operation at a time and find out whether the operation needs to be refined to more detailed steps. Then you proceed to the next level and refine further. This refinement process goes on until you arrive at a level deep enough to start coding. Creating an algorithm to solve a problem is in general the most laboursome task of the programming work. Many people do the mistake of starting to code at once, which makes you focus on code details and forget the actual problem to be solved. That gives an unstructured and inefficient code hard to understand and maintain. That’s why we emphasize that you structure your logic train of thought and construct a good algorithm before starting to code. A JSP graph is a tool to create an algorithm. JSP is an abbreviation for Jackson Structured Programming and is a commonly used instrument for logic structuring. Let’s take an example. You are supposed to create a program that calculates the price of a product to be bought by a customer. The customer specifies the product id and the requested quantity. The program should then calculate the relevant discount, add tax and show the customer price. A JSP graph could look as follows: University West, Trollhättan Structured Programming with C++ Department of Economy and Informatics calculate the relevant discount, add tax and show the customer price. A JSP graph could look as follows The upper box is the name of the program. We have split the solution into four steps at an overview level. You read the steps from left to right. As you probably realize the algorithm is too rough to be able to write code. So we proceed by refining the solution to the next level: The box ”Enter information” has been split into two steps, “Show instructions to the user” and “Read product id & quantity”. In the same way we have refined the box “Deduct discount”. We could break down the box ”Calculate gross price” further: We could refine the algorithm further, but let us say that we are satisfied with the detail level. The shadowed boxes in the graph are the end points at the lowest level, the “leaves of the tree”. These are the boxes to be used for coding, from left to right. We will work a great deal with JSP graphs in the program examples of the course. Each program is logically built up from three basic logic principles: • Sequence – the program performs instructions in sequence, one after the other. • Selection – the program selects one of several operations depending on the prerequisites. The program thus makes a selection based on some condition. • Iteration – the program repeats a series of instructions a certain number of times. The logic principles can also be combined. For instance, a sequence of instructions can be repeated a number of times if a specific condition is satisfied, otherwise another sequence of instructions should be performed a specific number of times. All programming languages use these three logic principles. If you have built up your algorithm in a correct way, it is only a question of selecting a programming language when the coding is to take place. The price calculation algorithm above should consequently give the same result irrespective of whether the code is written in C++, Java or VisualBasic. In the JSP graph above the box “Calculate gross price” is refined in a sequence of three operations, from left to right: • Look for the product in product file • Pick the product’s price • Multiply by quantity The box ”Look for the product in product file” could suggest an iteration, e.g. “Read next product id until we find the product id stated by the user”. The discount calculation in the price program above could imply a more differentiated discount situation: • If the gross price is between 100:- and 500:- the customer will get 5% discount. • If the gross price is between 500:- and 1000:- the customer will get 8% discount. • If the gross price is above 1000:- the customer will get 10% discount. Here the program must do a selection. When you have refined your algorithm to a level detailed enough, it is time to write code. This written code is called source code. The code must of course follow the rules in effect for the programming language in question, it must follow the syntax. Each programming language has its own rules. You can in principle use any word processor or text editor you like, such as the program Notepad, Wordpad or Word. If you use word processors like Wordpad or Word, you must save the file as pure text file (.txt). It is however recommended to use the text editor present in the program development package you are using. The advantage is that you will get some support when coding. Microsoft Visual C++, which is the program development package used in this course, contains an editor which: • Shows key words in C++ in blue colour and comments in green colour, • Provides IntelliSense, i.e. proposes code alternatives in certain situations, • Supports context sensitive help, i.e. shows an explanation of a certain code item if you put the cursor on the item and press F1, • Provides extensive support at debugging by allowing you to execute the code up to a certain breakpoint, where you can examine variable values at this particular position. There are other development tools for C++ like Borland and Dev C++. The tools differ somewhat as concerns small details of the code. You can use any tool, the important thing is that you learn to “think” structured programming. In this course we have used Microsoft Visual C++ 2008, and all program examples are tested in this environment. ﻿In this chapter you will learn what a variable is, how to declare a variable, i.e. tell the computer that there is a variable in the program, and how to assign values to variables. You will also learn how to perform simple mathematic calculations, how to read values from the keyboard, how to display information on the screen and control where on the screen the information will be displayed. We will also present a number of programming examples with JSP graphs. A variable is used by the program to store a calculated or entered value. The program might need the value later, and must then be stored in the computer’s working memory. Example: Here we have selected the name ’dTaxpercent’ to hold the value 0.25. You can in principle use any variable name, but it is recommended to use a name that corresponds to the use of the variable. When the variable name appears in a calculation the value will automatically be used, for example: means that 1500 will be multiplied by 0.25. The purpose of declaring a variable is to tell the program to allocate space in the working memory for the variable. The declaration: tells that we have a variable with the name iNo and that is of integer type (int). You must always specify the data type to allocate the correct memory space. An integer might for instance require 4 bytes while a decimal value might require 16 bytes. How many bytes to allocate depends on the operating system. Different operating systems use different amounts of bytes for the different data types. The variable name should tell something about the usage of the variable. Also, a standard used by many people is to allocate 1-3 leading characters to denote the data type (i for integer). Note that each program statement is ended by a semicolon. Below we declare a variable of decimal type: double means decimal number with double precision. Compare to float which has single precision. Since double requires twice as many bytes, a double variable can of course store many more decimals, which might be wise in technical calculations which require high precision. You can declare several variables of the same type in one single statement: The variables are separated by commas. Note that C++ is case sensitive, i.e. a ‘B’ and ‘b’ are considered different characters. Therefore the variables: are two different variables. Now we have explained how to declare variables, but how do the variables get their values? Look at the following code: Here the variable dTaxpercent gets the value 0.25, the variable iNo the value 5 and the variable dUnitprice the value 12. The equal character (=) is used as an assignment operator. Suppose that the next statement is: In this statement the variable iNo represents the value 5 and dUnitprice the value 12. The right part is first calculated as 5 * 12 = 60. This value is then assigned to the variable dTotal. Note that it is not the question of an equality in normal math like in the equation x = 60, where x has the value 60. In programming the equal sign means that something happens, namely that the right part is first calculated, and then the variable to the left is assigned that value. C++ performs the math operations in correct order. In the statement: the multiplication dTotal * dTaxpercent will first be performed, which makes 60 * 0.25 = 15. The value 15 will then be added to dTotal which makes 60 + 15 = 75. The value 75 will finally be assigned to the variable dToBePaid. If C++ would perform the operations in the stated order, we would get the erroneous value 60 + 60, which makes 120, multiplied by 0.25, which makes 30. If you need to perform an addition before a multiplication, you must use parentheses: Here the parenthesis is calculated first, which gives 1.25. This is then multiplied by 60, which gives the correct value 75. It is possible to initiate a variable, i.e. give it a start value, directly in the declaration: Here we declare the variable dTaxpercent and simultaneously specify it to get the value 0.25. You can mix initations and pure declarations in the same program statement: In addition to assigning the dTaxpercent a value, we have also declared the variables dTotal and dToBePaid, which not yet have any values. In the statement: we have initiated several variables and declared the variable iSum. Sometimes a programmer wants to ensure that a variable does not change its value in the program. A variable can of course not change its value if you don’t write code that changes its value. But when there are several programmers in the same project, or if a program is to be maintained by another programmer, it might be safe to declare a variable as a constant. Example: The key word const ensures that the constant dTaxpercent does not change its value. Therefore, a statement like this is forbidden: A constant must be initiated directly by the declaration, i.e. be given a value in the declaration statement. Consequently the following declaration is also forbidden: We have seen how a variable can be initiated in the declaration and how the variable can be assigned a value in other parts of the program. A variable can also get new values several times in the program. A variable can furthermore be changed by originating from the current value of the variable. The following example shows how the variable iNo is decreased by 2: As we have previously said the right part will first be calculated and then be assigned to the variable on the left side. Suppose that the variable iNo from the beginning has the value 5. The right part will then be 5-2 = 3. 3 is then assigned to the variable to the left, i.e. iNo. The effect of this statement is thus that iNo changes its value from 5 to 3, i.e. is decreased by 2. A more compact way of coding giving the same result is: ﻿In this chapter you will learn to incorporate intelligence into your programs, i.e. the program can do different things depending on different conditions (selections). You will also learn how to repeat certain tasks a specific number of times or until a specific condition is fulfilled (iteration, loop). We will introduce new symbols in our JSP graphs to illustrate selections and loops. A selection situation can be illustrated by the following figure: If the condition is fulfilled (yes option) the program will do one thing, else (no option) another thing. The keyword if introduces the if statement. The condition is put within parentheses. If the condition is true statement1 will be performed, otherwise statement2. Here is a code example: The values of two variables are compared. If a is greater than b, the variable greatest will get a’s value. Otherwise, i.e. if b is greater than or equal to a, greatest will get b’s value. The result from this code section is that the variable greatest will contain the greatest of a and b. Sometimes you might want to perform more than one statement for an option. Then you must surround the statements with curly brackets: If the condition is true all statements in the first code section will be executed, otherwise all statements in the second code section will be executed. Example: If a is greater than b, the variable greatest will get a’s value and the text “a is greatest” will be printed. Otherwise the variable greatest will get b’s value and the text “b is greatest” will be printed. Sometimes you don’t want to do anything at all in the else case. Then the else section is simply omitted like in the following example: If the variable sum is greater than 1000 the variable dDiscPercent will get the value 20 and the text “You will get 20% discount” will be printed. Otherwise nothing will be executed and the program goes on with the statements after the last curly bracket. We will now create a program that calculates the total price of a product. The user is supposed to enter quantity and price per unit of the product. If the total exceeds 500:- you will get 10 % discount, otherwise 0 %. We start with a JSP graph: All boxes except “Calculate discount” are rather simple to code. “Calculate discount” requires a closer examination. It has a condition included which says that the discount is different depending on whether gross is less or greater than 500. We’ll break down that box: A conditional situation in JSP is identified by a ring in the upper right corner of the box. That implies that only one of the boxes will be executed. Here is the code: The declaration shows a constant dLimit, which later is used to check the gross value. The variable iNo is used to store the entered quantity and dUnitPrice is used for the entered unit price. It is common among programmers to use one or a few characters in the beginning of the variable name to signify the data type of the variable. The variable iNo has first character I (integer), and the variable dUnitPrice has d (double). After data entry the gross is calculated by multiplying the two entered values (quantity * unit price). That value is stored in the variable dGross. The if statement then checks the value of dGross. If greater than dLimit (i.e. 500) the variable dDisc will get the value 10, otherwise 0. dDisc contains the discount percent to be applied. The net is then calculated by subtracting the discount percent from 100, which then is multiplied by dGross and divided by 100 (to compensate for the percent value). Finally the total price is printed. In the if statements in previous example codes we have so far only used the comparison operator > (greater than). Here is a list of all comparison operators: In some situations you will need to check whether a number is evenly dividable by another number. Then the modulus operator % is used. Below are some code examples of how to check whether a number is odd or even, i.e. evenly dividable by 2. We will now study an example of a more complicated situation. Suppose the following conditions prevail: If a customer buys more than 100 pieces, he will get 20% discount. Otherwise if the quantity exceeds 50, i.e. lies in the interval 50-100, he will get 10%. Otherwise, i.e. if the quantity is below 50, no discount is given. The situation is shown by the following JSP graph: Here we use the keyword else if. You can use any number of else if-s to cover many conditional cases. The situation with different discount percentages for different quantity intervals can be solved in another way, namely by combining two conditions. In common English it can be expressed like this: If the quantity is less than 100 and the quantity is greater than 50, the customer will get 10% discount. Here we combine two conditions: - If the quantity is less than 100 and - and the quantity is greater than 50 The combination of the conditions means that the quantity lies in the interval 50-100. Both conditions must be fulfilled in order to get 10%. The conditions are combined with “and” which is a logical operator. It is written && in C++. The code will then be: Suppose the situation is this: If the quantity is greater than 100 or the total order sum is greater than 1000, the customer will get 20% discount. Here we combine the conditions: - If the quantity is greater than 100 eller - or the total order sum is greater than 1000 In both cases the customer has bought so much that he will get 20% discount. One of the conditions is sufficient to get that discount. The conditions are combined with the logic operator “or”, which is written || in C++. The code for this situation will be: In many situations you cannot predict what a user is going to enter. It might happen that the user enters characters when the program expects integers, or that he does not enter anything at all but just press Enter. Then you can use conditional input: ﻿In this chapter you will learn what an array is, namely a method of storing many values under a single variable name, instead of using a specific variable for each value. We will begin by declaring an array and assign values to it. In connection with arrays you will have great use for loops, by means of which you can efficiently search for a value in the array and sort the values. Arrays is a fundamental concept within programming which will frequently be used in the future. An array is, as already mentioned, a method of storing many values of the same data type and usage under a single variable name. Suppose you want to store temperatures measured per day during a month: If you didn’t know about arrays, you would need 30 different variable names, for instance: This is a bad option, especially if you want to calculate the average temperature or anything else. Then you would need to write a huge program statement for the sum of the 30 variables. Instead, we use an array, i.e. one single variable name followed by an index within square brackets that defines which of the temperatures in the array that is meant: The name of the array is temp. The different values in the array are called elements. In this way we can use a loop, where the loop variable represents the index, and do a repeated calculation on each of the temperatures: The loop variable i goes from 1 to 30. In the first turn of the loop i has the value 1, which means that temp[i] represents temp[1], i.e. the first temperature. In the second turn of the loop i has the value 2 and temp[i] represents the second temperature. By using a loop the amount of code required will not increase with the number of temperatures to handle. The only thing to be modified is the number of turns that the for loop must execute. In the code below we calculate the average of all the temperatures: The variable iSum is set to 0 since it later on will be increased by one temperature at a time. The loop goes from 1 to 30, i.e. equal to the number of elements in the array. In the loop body the variable iSum is increased by one temperature at a time. When the loop has completed, all temperatures have been accumulated in iSum. Finally we divide by 30 to get the average, which is printed. Like for all variables, an array must be declared. Below we declare the array temp: The number within square brackets indicates how many items the array can hold, 31 in our example. 31 positions will be created in the primary memory each of which can store a double value. The indeces will automatically be counted from 0. This means that the last index is 30. If you need temperatures for the month April, which has 30 days, you have two options: 1. Declare temp[30], which means that the indeces goes from 0 to 29. 1st of April will correspond to index 0, 2nd of April to index 1 etc. 30th of April will correspond to index 29. The index lies consequently one step after the actual date. 2. Declare temp[31]. Then 1st of April can correspond to index 1, 2nd of April to index 2 etc. 30th of April will correspond to index 30. The date and index are here equal all the time. This means that the item temp[0] is created ”in vain” and will never be used. It is no big deal which of the methods you use, but you will have to be conscious about the method selected, because it affects the code you write. We will show examples of both methods. Note that, in the declaration: all elements are of the same data type, namely double. For arrays all elements all items always have the same data type. You can assign values to an array already at the declaration, e.g.: Here the array iNo will hold 5 items, where the first item with index 0 gets the value 23, the second item with index 1 the value 12 etc. The enumeration of the values must be within curly brackets, separated by commas. As a matter of fact it is redundant information to specify the number of items to 5 in the declaration above, since the number of enumerated values is 5. Therefore you could as well write: An enumeration within curly brackets can only be written in the declaration of an array. For instance, the following is erroneous: In the following code section we declare an array of integers and assign values to the array in the loop: The array iSquare is declared to hold 11 items of integer type. The loop then goes from 0 to 10. In the first turn of the loop i is =0 and the item iSquare[0] gets the value 0*0, i.e. 0. In the second turn of the loop i is =1 and iSquare[1] gets the value 1*1, i.e. 1. In the third turn of the loop the item iSquare[2] gets the value 2*2, i.e. 4. Each item will contain a value equal to the square of the index. As a C++ programmer you must yourself keep track of the valid index interval. The result could be disastrous if you wrote: This means that we store the value 23.5 in the primary memory at an adress that does not belong to the array, but might belong to a memory area used for other data or program code. If you run such a code the system might in worst case break down and you will have to restart the computer. Suppose we want to copy the temperatures from the array with April’s values to an array with June’s values. You cannot copy an entire array in this way: You will have to copy the values item by item by means of a loop: Here the loop goes from 1 to 30 and we copy item by item for each turn of the loop. ﻿A string is a text, i.e. a sequence of characters (letters, digits and other special characters). String handling is a little tricky in C++, so we have dedicated an separate chapter to this subject. Actually, a string is an array that consists of a number of items, where each item is a character in the string. There are a number of string handling functions. We will in this chapter get acquainted with the most common and usable string functions, like calculating the length of a string, copying a string, concatenating strings and picking out parts of a string. In programming, string handling is important, since a user often enters information in text form which is taken care of by the program. We will go through several programming examples, where we will have use of our string knowledge. We will first get to know the most simple of all strings, namely the one containing just one character. To store one character in a variable we use the data type char. Below we declare a string variable of the char type: The variable cLetter can now contain any one character. We can assign a value to the variable: Note that we use single quotes for the character. We can also, like for all kinds of variables, assign a value directly in the declaration: Our first program shows how to handle entry of characters by the user to select a menu option. The program will first display a menu: Here the user can enter one of the letters A, B, C or D to choose an item. We will not build a full-featured order/invoice/ warehouse/ finance system, but we will only let the program print a text about the selected choice. We start with a JSP graph: The first action is that the menu is displayed. Then the user is prompted for a choice by means of the letters A-D. Finally the requested option is executed, i.e. a simple text message will be printed. If the user enters another character than A-D, an error message is printed. First a char variable is declared named cSel, and then the menu is printed with a number of cout statements. The subsequent cin statement reads a character from the user to the variable cSel, which then is checked in the switch statement. The switch statement contains one case section for each option. Note that each case line has the character A-D within single quotes, which is necessary since it is a char variable that is checked. The default section takes care of all characters other than A, B, C or D. We will now improve our menu program so that the user repeatedly can enter different options without terminating the program. We then put the entire menu printing and switch statement in a loop. The JSP graph will then be: We complete the menu with still another option, X – Exit. As long as the user does not enter X, the loop proceeds. Furthermore we also clean the screen before the menu is displayed, which is the first operation of the loop. Here is the code: To be able to clear the screen we need the header file stdlib.h. W use a do loop, where the condition is checked after the loop to ensure that the loop is run at least once. The first action in the loop is to clear the screen with system(“cls”). Then the menu is printed and the user is prompted for an option, i.e. a character to be stored in the variable cSel. That variable is checked in the switch statement, where a text is printed corresponding the selected option. If the user enters ‘X’, the text ‘You selected to exit’ will be printed, and the loop is terminated since the while condition specifies that cSel must not equal ‘X’. The method of letting the user enter an extra character to the variable temp at the end of the loop is a relatively unconvenient solution, but it has the advantage of avoiding to struggle with special C++ features regarding input, which we don’t go into here. We will now create a logically rather complex program that uses char variables to print a number of ’X’ on the screen with the shape of a Christmas tree: As you can see the tree has eleven branches and two ’X’:s to the trunk. We therefore need an outer loop that runs eleven times, where each turn prints a branch. Each branch consists of a number of ‘X’:s, different depending on which branch being printed. Furthermore we will have to print a suitable number of blanks before the ‘X’:s, so that the branches are centered symmetrically around the middle trunk. We therefore need two inner loops, one that prints the leading blanks and one that prints the ‘X’:s for each branch. After completion of the branches we need a loop that runs two turns and that prints the ‘X’:s for the trunk. We start with a JSP graph: The JSP graph tells us that there is one outer loop for the branches, where each turn of the loop prints a branch. We then have one inner loop that prints the correct number of blanks and one inner loop that prints the ‘X’:s. The same is true for the trunk where we have an outer loop where each turn of the loop prints one line of the trunk, and one inner loop that prints the blanks before one single ‘X’ is printed. We declare a variable named x that correspond to the ’X’, and a variable named blank corresponding to the blank character. The first outer for-loop has an i as loop counter and goes from 10 to 0 (11 turns). We have selected to let the values run backwards to make the subsequent math easier. i is consequently a line counter for the branches of the tree. The first inner loop has a j as loop counter and goes from 0 to i. It prints the correct number of blanks for each branch. Since i is counted reversed, the number of blanks will decrease for each branch. Each turn of the loop prints one blank. After completion of the blanks for a bransch, first an ’X’ is printed. Since the number of ’X’:s at each bransch is odd, the remaining number of ’X’:s to be printed is even. ﻿When working with data from files and databases it is often convenient to process big portions of data in one lump, for instance an entire customer record in a customer file. A good tool for this is the structure concept. A structure is a set of data that in some way has an intermediary relation. In connection with structures we will be using pointers and pointer arithmetics that we learnt in the previous chapter. Structures are a pre-state to classes within object oriented programming. Therefore, this chapter is a bridge to the next step of your programmer education, object oriented programming. In this chapter we will learn how to define structures, handle information stored in structures, work with arrays of structures and files in connection with structures. We will also learn how to use pointers to structures, how to sent structures to a function and store structures in the dynamic memory. Think of a customer record in a customer file that contains name, address, telephone, email, discount profile, terms of delivery, terms of payment and so forth. All this information is stored for each customer in the customer file. When reading, processing and saving this information to a file or database it is convenient to be able to handle all data for a customer in a uniform way. It is then gathered into a structure, which provides better organization of the program code. A structure is like a template for all information per customer. A structure behaves in the code like a data type such as int, double or char. You declare a variable of the structure type defined. In the structure variable you can then store all information for a particular customer. You can also create an array of structure items, where each item of the array is a structure with all information per customer. The array will thus contain all information for all customers. First we will learn to define a structure template, i.e. specify the shape of the structure, the structure members and the data type of each member of the structure. Suppose we want to work with a product file with: • Product name • Product id • Price • Quantity in stock • Supplier This means that each product in the file will contain these five members. Here is the code for definition of the structure: First there is the keyword struct, and then the name of the structure or data type (Prod). Within curly brackets you then enumerate the members of the structure, where each member is declared in the usual way of declaring variables. Each member is ended with a semicolon. After the last right curly bracket there must also be a semicolon. The structure above shows that the different members can be of different data types (char, int, double) and also arrays like cName. You can also have other structures as members of the structure, if applicable. The names of the structure and members are of course arbitrarily selected, but they should in some way correspond to their usage. To declare a structure variable, i.e. a variable of the data type Prod, you write: Here we declare a variable prodOne which is of the Prod type. You can also initiate it with values already in the declaration: Within curly brackets we enumerate values for the structure members in the correct sequence, separated by commas. The data types of the values must correspond to the definition of the members. When updating, copying or in other ways processing the value of a structure member, you use the following way of coding: You write the name of the structure variable, followed by a period and the name of the member in question. Here the quantity in stock will be set to 251 for the ‘Oliv Oil’ product. Or: This requires that cString is a string array whose content is copied to the cSupp member. We will now create an entire program using structures. We will create a product structure according to the previous example and two structure variables with product information. One of them should be initiated directly in the declaration and the other is supposed to be supplied with information from the user. Finally the program should print a table of the products. We start with a JSP graph: The logic is simple. The most difficult task is to handle the structure in the correct way. Here is the code: The definition of the structure is before main(), which makes it valid for the entire program, also inside functions. You can also define the structure inside main(), but then it is only valid in main() and not in other functions. The first structure variable prodOne is initiated with values directly in the declaration. Then there are a number of heading texts and entry of values to the structure members of the second structure variable. Note that we use a period between the structure variable and member. The output is done by means of tabs \t. You might need to accommodate the length of the entered texts to make the information be printed in the correct column. We could have done that more flexible by means of the text formatting functions from chapter 1, but we used a rough method for simplicity’s sake. A disadvantage with the previous program is that we needed a separate structure variable (prodOne, prodTwo) for each product. A more convenient solution is to use an array with structure variables allowing the use of a loop to process the structure variables in a uniform way. Below we declare a structure array sProds of the type Prod with three items: We have allocated memory space for three products, but we have not yet assigned values to the structure members. That could be made directly at the declaration: Note that the values for each structure variable are surrounded by curly brackets, and that the values are enumerated in the usual way within each pair of curly brackets. All three pair of brackets are surrounded by an extra pair of curly brackets delimiting the initiation list of values. After the last bracket there must be a semicolon. If you want to let the user enter values, this is preferably done in a loop:$$$﻿When working with data from files and databases it is often convenient to process big portions of data in one lump, for instance an entire customer record in a customer file. A good tool for this is the structure concept. A structure is a set of data that in some way has an intermediary relation. In connection with structures we will be using pointers and pointer arithmetics that we learnt in the previous chapter. Structures are a pre-state to classes within object oriented programming. Therefore, this chapter is a bridge to the next step of your programmer education, object oriented programming. In this chapter we will learn how to define structures, handle information stored in structures, work with arrays of structures and files in connection with structures. We will also learn how to use pointers to structures, how to sent structures to a function and store structures in the dynamic memory. Think of a customer record in a customer file that contains name, address, telephone, email, discount profile, terms of delivery, terms of payment and so forth. All this information is stored for each customer in the customer file. When reading, processing and saving this information to a file or database it is convenient to be able to handle all data for a customer in a uniform way. It is then gathered into a structure, which provides better organization of the program code. A structure is like a template for all information per customer. A structure behaves in the code like a data type such as int, double or char. You declare a variable of the structure type defined. In the structure variable you can then store all information for a particular customer. You can also create an array of structure items, where each item of the array is a structure with all information per customer. The array will thus contain all information for all customers. First we will learn to define a structure template, i.e. specify the shape of the structure, the structure members and the data type of each member of the structure. Suppose we want to work with a product file with: • Product name • Product id • Price • Quantity in stock • Supplier This means that each product in the file will contain these five members. Here is the code for definition of the structure: First there is the keyword struct, and then the name of the structure or data type (Prod). Within curly brackets you then enumerate the members of the structure, where each member is declared in the usual way of declaring variables. Each member is ended with a semicolon. After the last right curly bracket there must also be a semicolon. The structure above shows that the different members can be of different data types (char, int, double) and also arrays like cName. You can also have other structures as members of the structure, if applicable. The names of the structure and members are of course arbitrarily selected, but they should in some way correspond to their usage. To declare a structure variable, i.e. a variable of the data type Prod, you write: Here we declare a variable prodOne which is of the Prod type. You can also initiate it with values already in the declaration: Within curly brackets we enumerate values for the structure members in the correct sequence, separated by commas. The data types of the values must correspond to the definition of the members. When updating, copying or in other ways processing the value of a structure member, you use the following way of coding: You write the name of the structure variable, followed by a period and the name of the member in question. Here the quantity in stock will be set to 251 for the ‘Oliv Oil’ product. Or: This requires that cString is a string array whose content is copied to the cSupp member. We will now create an entire program using structures. We will create a product structure according to the previous example and two structure variables with product information. One of them should be initiated directly in the declaration and the other is supposed to be supplied with information from the user. Finally the program should print a table of the products. We start with a JSP graph: The logic is simple. The most difficult task is to handle the structure in the correct way. Here is the code: The definition of the structure is before main(), which makes it valid for the entire program, also inside functions. You can also define the structure inside main(), but then it is only valid in main() and not in other functions. The first structure variable prodOne is initiated with values directly in the declaration. Then there are a number of heading texts and entry of values to the structure members of the second structure variable. Note that we use a period between the structure variable and member. The output is done by means of tabs \t. You might need to accommodate the length of the entered texts to make the information be printed in the correct column. We could have done that more flexible by means of the text formatting functions from chapter 1, but we used a rough method for simplicity’s sake. A disadvantage with the previous program is that we needed a separate structure variable (prodOne, prodTwo) for each product. A more convenient solution is to use an array with structure variables allowing the use of a loop to process the structure variables in a uniform way. Below we declare a structure array sProds of the type Prod with three items: We have allocated memory space for three products, but we have not yet assigned values to the structure members. That could be made directly at the declaration: Note that the values for each structure variable are surrounded by curly brackets, and that the values are enumerated in the usual way within each pair of curly brackets. All three pair of brackets are surrounded by an extra pair of curly brackets delimiting the initiation list of values. After the last bracket there must be a semicolon. If you want to let the user enter values, this is preferably done in a loop:
