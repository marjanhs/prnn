id	sentiment	review
EN29	0	﻿Computing is a constantly changing our world and our environment. In the 1960s large machines called mainframes were created to manage large volumes of data (numbers) efficiently. Bank account and payroll programs changed the way organisations worked and made parts of these organisations much more efficient. In the 1980s personal computers became common and changed the way many individuals worked. People started to own their own computers and many used word processors and spreadsheets applications (to write letters and to manage home accounts). In the 1990s email became common and the world wide web was born. These technologies revolutionised communications allowing individuals to publish information that could easily be accessed on a global scale. The ramifications of these new technologies are still not fully understood as society is adapting to opportunities of internet commerce, new social networking technologies (twitter, facebook, myspace, online gaming etc) and the challenges of internet related crime. Just as new computing technologies are changing our world so too are new techniques and ideas changing the way we develop computer systems. In the 1950s the use machine code (unsophisticated, complex and machine specific) languages were common. In the 1960s high level languages, which made programming simpler, became common. However these led to the development of large complex programs that were difficult to manage and maintain. In the 1970s the structured programming paradigm became the accepted standard for large complex computer programs. The structured programming paradigm proposed methods to logically structure the programs developed into separate smaller, more manageable components. Furthermore methods for analysing data were proposed that allowed large databases to be created that were efficient, preventing needless duplication of data and protected us against the risks associated with data becoming out of sync. However significant problems still persisted in a) understanding the systems we need to create and b) changing existing software as users requirements changed. In the 1980s ‘modular’ languages, such as Modula-2 and ADA were developed that became the precursor to modern Object Oriented languages. In the 1990s the Object Oriented paradigm and component-based software development ideas were developed and Object Oriented languages became the norm from 2000 onwards. The object oriented paradigm is based on many of the ideas developed over the previous 30 years of abstraction, encapsulation, generalisation and polymorphism and led to the development of software components where the operation of the software and the data it operates on are modelled together. Proponents of the Object Oriented software development paradigm argue that this leads to the development of software components that can be re-used in different applications thus saving significant development time and cost savings but more importantly allow better software models to be produced that make systems more maintainable and easier to understand. It should perhaps be noted that software development ideas are still evolving and new agile methods of working are being proposed and tested. Where these will lead us in 2020 and beyond remains to be seen. The structured programming paradigm proposed that programs could be developed in sensible blocks that make the program more understandable and easier to maintain. Activity 1 Assume you undertake the following activities on a daily basis. Arrange this list into a sensible order then split this list into three blocks of related activities and give each block a heading to summarise the activities carried out in that block. Get out of bed Eat breakfast Park the car Get dressed Get the car out of the garage Drive to work Find out what your boss wants you to do today Feedback to the boss on today’s results. Do what the boss wants you to do Feedback 1 You should have been able to organise these into groups of related activities and give each group a title that summarises those activities. Get up :- Get out of bed Get dressed Eat breakfast Go to Work :- Get the car out of the garage Drive to work Park the car Do your job :- Find out what your boss wants you to do today Do what the boss wants you to do Feedback to the boss on today’s results. By structuring our list of instructions and considering the overall structure of the day (Get up, go to work, do your job) we can change and improve one section of the instructions without changing the other parts. For example we could improve the instructions for going to work…. Listen to the local traffic and weather report Decide whether to go by bus or by car If going by car, get the car and drive to work. Else walk to the bus station and catch the bus without worrying about any potential impact this may have on ‘getting up’ or ‘doing your job’. In the same way structuring computer programs can make each part more understandable and make large programs easier to maintain. Feedback 2 With an address book we would want to be able to perform the following actions :- find out details of a friend i.e. their telephone number, add an address to the address book and, of course, delete an address. We can create a simple software component to store the data in the address book (i.e. list of names etc) and the operations, things we can do with the address book (i.e add address, find telephone number etc). By creating a simple software component to store and manage addresses of friends we can reuse this in another software system i.e. it could be used by a business manager to store and find details of customers. It could also become part of a library system to be used by a librarian to store and retrieve details of the users of the library. Thus in object oriented programming we can create re-usable software components (in this case an address book). While we can focus our attention on the actual program code we are writing, whatever development methodology is adopted, it is not the creation of the code that is generally the source of most problems. Most problems arise from :- • poor maintainability: the system is hard to understand and revise when, as is inevitable, requests for change arise. • Statistics show 70% of the cost of software is not incurred during its initial development phase but is incurred during subsequent years as the software is amended to meet the ever changing needs of the organisation for which it was developed. For this reason it is essential that software engineers do everything possible to ensure that software is easy to maintain during the years after its initial creation. ﻿The Unified Modelling Language, UML, is sometimes described as though it was a methodology. It is not! A methodology is a system of processes in order to achieve a particular outcome e.g. an organised sequence of activities in order to gather user requirements. UML does not describe the procedures a programmer should follow – hence it is not a methodology. It is, on the other hand, a precise diagramming notation that will allow program designs to be represented and discussed. As it is graphical in nature it becomes easy to visualise, understand and discuss the information presented in the diagram. However, as the diagrams represent technical information they must be precise and clear – in order for them to work - therefore there is a precise notation that must be followed. As UML is not a methodology it is left to the user to follow whatever processes they deem appropriate in order to generate the designs described by the diagrams. UML does not constrain this – it merely allows those designs to be expressed in an easy to use, but precise, graphical notation. A process will be explained in chapter 6 that will help you to generate good UML designs. Developing good designs is a skill that takes practise to this end the process is repeated in the case study (chapter 11). For now we will just concentrate on the UML notation not these processes. Classes are the basic components of any object oriented software system and UML class diagrams provide an easy way to represent these. As well as showing individual classes, in detail, class diagrams show multiple classes and how they are related to each other. Thus a class diagram shows the architecture of a system. A class consists of :- • a unique name (conventionally starting with an uppercase letter) • a list of attributes (int, double, boolean, String etc) • a list of methods This is shown in a simple box structure… For attributes and methods visibility modifiers are shown (+ for public access, – for private access). Attributes are normally kept private and methods are normally made public. Accessor methods are created to provide access to private attributes when required. Thus a public method SetTitle() can be created to change the value of a private attribute ‘title’. Thus a class Book, with String attributes of title and author, and the following methods SetTitle(), GetTitle(), SetAuthor(), GetAuthor() and ToString() would be shown as …. Note: String shown above is not a primitive data type but is itself a class. Hence it starts with a capital letter. Some programmers use words beginning in capitals to denote class names and words beginning in lowercase to represent attributes or methods (thus ToString() would be shown as toString()). This is a common convention when designing and writing programs in Java (another common OO language). However it is not a convention followed by C# programmers – where method names usually start in Uppercase. Method names can be distinguished from class names by the use of (). This in the example above. ‘Book’ is a class ‘title’ is an attribute and ‘SetTitle()’ is a method. UML diagrams are not language specific thus a software design, communicated via UML diagrams, can be implemented in a range of OO languages. Furthermore traditional accessor methods, getters and setters, are not required in C# programs as they are replaced by ‘properties’. Properties are in effect hidden accessor methods thus the getter and setter methods shown above, GetTitle(), SetTitle() etc are not required in a C# program. In C# an attribute would be defined called ‘title’ and a property would be defined as ‘Title’. This would allow us to set the ‘title’ directly by using the associated property ‘Title =…..;’. The UML diagrams shown in this book will use the naming convention common among C# programmers … for the simple reason that we will be writing sample code in C# to demonstrate the OO principles discussed here. Though initially we will show conventional assessor methods these will be replaced with properties when coding. UML allows us to suppress any information we do not wish to highlight in our diagrams – this allows us to suppress irrelevant detail and bring to the readers attention just the information we wish to focus on. Therefore the following are all valid class diagrams… Firstly with the access modifiers not shown…. Secondly with the access modifiers and the data types not shown….. And finally with the attributes and methods not shown….. i.e. there is a class called ‘BankAccount’ but the details of this are not being shown. Of course virtually all C# programs will be made up of many classes and classes will relate to each other – some classes will make use of other classes. These relationships are shown by arrows. Different type of arrow indicate different relationships (including inheritance and aggregation relationships). In addition to this class diagrams can make use of keywords, notes and comments. As we will see in examples that follow, a class diagram can show the following information :- • Classes --attributes --operations --visibility • Relationships --navigability --multiplicity --dependency --aggregation --composition • Generalization / specialization --inheritance --interfaces • Keywords • Notes and Comments As UML diagrams convey precise information there is a precise syntax that should be followed. Attributes should be shown as: visibility name : type multiplicity Where visibility is one of :- --‘+’ public --‘-’ private --‘#’ protected --‘~’ package and Multiplicity is one of :- --‘n’ exactly n --‘*’ zero or more --‘m..‘n’ between m and n The following are examples of attributes correctly specified using UML :- - custRef : int [1] a private attribute custRef is a single int value this would often be shown as - custRef : int However with no multiplicity shown we cannot safely assume a multiplicity of one was intended by the author. # itemCodes : String [1..*] a protected attribute itemCodes is one or more String values validCard : boolean an attribute validCard, of unspecified visibility, has unspecified multiplicity ﻿Many kinds of things in the world fall into related groups of ‘families’. ‘Inheritance’ is the idea ‘passing down’ characteristics from parent to child, and plays an important part in Object Oriented design and programming. While you are probably already familiar with constructors, and access control (public/private), there are particular issues in relating these to inheritance we need to consider. Additionally we need to consider the use of Abstract classes and method overriding as these are important concepts in the context of inheritance. Finally we will look at the ‘Object’ class which has a special role in relation to all other classes in C#. Classes are a generalized form from which objects with differing details can be created. Objects are thus ‘instances’ of their class. For example Student 051234567 is an instance of class Student. More concisely, 051234567 is a Student. Constructors are special methods that create an object from the class definition. Classes themselves can often be organised by a similar kind of relationship. One hierarchy, that we all have some familiarity with, is that which describes the animal kingdom :- • Kingdom (e.g. animals) • Phylum (e.g. vertebrates) • Class (e.g. mammal) • Order (e.g. carnivore) • Family (e.g. cat) • Genus (e.g. felix) • Species (e.g. felix leo) We can represent this hierarchy graphically …. Of course to draw the complete diagram would take more time and space than we have available. Here we can see one specific animal shown here :-‘Fred’. Fred is not a class of animal but an actual animal. Fred is a felix leo is a felix is a cat is a carnivore Carnivores eat meat so Fred has the characteristic ‘eats meat’. Fred is a felix leo is a felix is a cat is a carnivore is a mammal is a vertebrate Vertebrates have a backbone so Fred has the characteristic ‘has a backbone’. The ‘is a’ relationship links an individual to a hierarchy of characteristics. This sort of relationship applies to many real world entities, e.g. BonusSuperSaver is a SavingsAccount is a BankAccount. We specify the general characteristics high up in the hierarchy and more specific characteristics lower down. An important principle in OO – we call this generalization and specialization. All the characteristics from classes above in a class/object in the hierarchy are automatically featured in it – we call this inheritance. Consider books and magazines - both are specific types of publication. We can show classes to represent these on a UML class diagram. In doing so we can see some of the instance variables and methods these classes may have. Attributes ‘title’, ‘author’ and ‘price’ are obvious. Less obvious is ‘copies’ this is how many are currently in stock. For books, OrderCopies() takes a parameter specifying how many extra copies are added to stock. For magazines, orderQty is the number of copies received of each new issue and currIssue is the date/period of the current issue (e.g. “January 2011”, “Fri 6 Jan”, “Spring 2011” etc.) When a new issue is received the old issues are discarded and orderQty copies are placed in stock. Therefore RecNewIssue() sets currIssue to the date of new issue and restores copies to orderQty. AdjustQty() modifies orderQty to alter how many copies of subsequent issues will be stocked. We can separate out (‘factor out’) these common members of the classes into a superclass called Publication. In C# a superclass is often called a base class. The differences will need to be specified as additional members for the ‘subclasses’ Book and Magazine. In this is a UML Class Diagram the hollow-centred arrow denotes inheritance. Note the subclass has the generalized superclass (or base class) characteristics + additional specialized characteristics. Thus the Book class has four instance variables (title, price, copies and author) it also has two methods (SellCopy() and OrderCopies()). The inherited characteristics are not listed in subclasses. The arrow shows they are acquired from the superclass. No special features are required to create a superclass. Thus any class can be a superclass unless specifically prevented. A subclass specifies it is inheriting features from a superclass using the : symbol. For example…. Constructors are methods that create objects from a class. Each class (whether sub or super) should encapsulate its own initialization in a constructor, usually relating to setting the initial state of its instance variables. Constructors are methods given the same name as the class. A constructor for a superclass, or base class, should deal with general initialization. Each subclass can have its own constructor for specialised initialization but it must often invoke the behaviour of the base constructor. It does this using the keyword base. Usually some of the parameters passed to MySubClass will be initializer values for superclass instance variables, and these will simply be passed on to the superclass constructor as parameters. In other words super-parameters will be some (or all) of sub-parameters. Shown below are two constructors, one for the Publication class and one for Book. The book constructor requires four parameters three of which are immediately passed on to the base constructor to initialize its instance variables. Thus in creating a book object we first create a publication object. The constructor for Book does this by calling the constructor for Publication. Rules exist that govern the invocation of a superconstructor. If the superclass has a parameterless (or default) constructor this will be called automatically if no explicit call to base is made in the subclass constructor though an explicit call is still better style for reasons of clarity. However if the superclass has no parameterless constructor but does have a parameterized one, this must be called explicitly using : base. To illustrate this…. On the left above:- it is legal, though bad practice, to have a subclass with no constructor because superclass has a parameterless constructor. In the centre:- if subclass constructor doesn’t call the base constructor then the parameterless superclass constructor will be called. On the right:- because superclass has no paramterless constructor, subclass must have a constructor, it must call the super constructor using the keyword base and it must pass on the required paramter. This is simply because a (super) class with only a parameterized constructor can only be initialized by providing the required parameter(s). ﻿Within hierarchical classification of animals Pinky is a pig (species sus scrofa) Pinky is (also, more generally) a mammal Pinky is (also, even more generally) an animal We can specify the type of thing an organism is at different levels of detail: higher level = less specific lower level = more specific If you were asked to give someone a pig you could give them Pinky or any other pig. If you were asked to give someone a mammal you could give them Pinky, any other pig or any other mammal (e.g. any lion, or any mouse, or any cat). If you were asked to give someone an animal you could give them Pinky, any other pig, any other mammal, or any other animal (bird, fish, insect etc). The idea here is that an object in a classification hierarchy has an ‘is a’ relationship with every class from which it is descended and each classification represents a type of animal. This is true in object oriented programs as well. Every time we define a class we create a new ‘type’. Types determine compatibility between variables, parameters etc. A subclass type is a subtype of the superclass type and we can substitute a subtype wherever a ‘supertype’ is expected. Following this we can substitute objects of a subtype whenever objects of a supertype are required (as in the example above). The class diagram below shows a hierarchical relationship of types of object – or classes. In other words we can ‘substitute’ an object of any subclass where an object of a superclass is required. This is NOT true in reverse! When designing class/type hierarchies, the type mechanism allows us to place a subclass object where a superclass is specified. However this has implications for the design of subclasses – we need to make sure they are genuinely substitutable for the superclass. If a subclass object is substitutable then clearly it must implement all of the methods of the superclass – this is easy to guarantee as all of the methods defined in the superclass are inherited by the subclass. Thus while a subclass may have additional methods it must at least have all of the methods defined in the superclass and should therefore be substitutable. However what happens if a method is overridden in the subclass? When overriding methods we must ensure that they are still substitutable for the method being replaced. Therefore when overriding methods, while it is perfectly acceptable to tailor the method to the needs of the subclass a method should not be overridden with functionality which performs an inherently different operation. For example, RecNewIssue() in DiscMag overrides RecNewIssue() from Magazine but does the same basic job (“fulfils the contract”) as the inherited version with respect to updating the number of copies and the current issue. While it extends that functionality in a way specifically relevant to DiscMags by displaying a reminder to check the cover discs, essentially these two methods perform the same operation. What do we know about a ‘Publication’? Answer: It’s an object which supports (at least) the operations: void SellCopy() String ToString() and it has properties that allow us to set the price, get the number of copies set the number of copies. Inheritance guarantees that objects of any subclass of Publications provides at least these. Note that a subclass can never remove an operation inherited from its superclass(es) – this would break the guarantee. Because subclasses extend the capabilities of their superclasses, the superclass functionality can be assumed. It is quite likely that we would choose to override the ToString() method (initially defined within ‘Object’) within Publication and override it again within Magazine so that the String returned provides a better description of Publications and Magazines. However we should not override the ToString() method in order to return the price – this would be changing the functionality of the method so that the method performs an inherently different function. Doing this would break the substitutability principle. Because an instance of a subclass is an instance of its superclass we can handle subclass objects as if they were superclass objects. Furthermore because a superclass guarantees certain operations in its subclasses we can invoke those operations without caring which subclass the actual object is an instance of. This characteristic is termed ‘polymorphism’, originally meaning ‘having multiple shapes’. Thus a Publication comes in various shapes … it could be a Book, Magazine or DiscMag. We can invoke the SellCopy() method on any of these Publications irrespective of their specific details. Polymorphism is a fancy name for a common idea. Someone who knows how to drive can get into and drive most cars because they have a set of shared key characteristics – steering wheel, gear stick, pedals for clutch, brake and accelerator etc – which the driver knows how to use. There will be lots of differences between any two cars, but you can think of them as subclasses of a superclass which defines these crucial shared ‘operations’. If ‘p’ ‘is a’ Publication, it might be a Book or a Magazine or a DiscMag. Whichever it is we know that it has a SellCopy() method. So we can invoke p.SellCopy() without worrying about what exactly ‘p’ is. This can make life a lot simpler when we are manipulating objects within an inheritance hierarchy. We can create new types of Publication e.g. a Newspaper and invoke p,SellCopy() on a Newspaper without have to create any functionality within the new class – all the functionality required is already defined in Publication. Polymorphism makes it very easy to extend the functionality of our programs as we will see now and we will see this again in the case study (in Chapter 11). Huge sums of money are spent annually creating new computer programs but over the years even more is spent changing and adapting those programs to meet the changing needs of an organisation. Thus as professional software engineers we have a duty to facilitate this and help to make those programs easier to maintain and adapt. Of course the application of good programming standards, commenting and layout etc, have a part to play here but also polymorphism can help as it allows programs to be made that are easily extended. ﻿Historically in computer programs method names were required to be unique. Thus the compiler could identify which method was being invoked just by looking at its name. However several methods were often required to perform very similar functionality for example a method could add two integer numbers together and another method may be required to add two floating point numbers. If you have to give these two methods unique names which one would you call ‘Add()’? In order to give each method a unique name the names would need to be longer and more specific. We could therefore call one method AddInt() and the other AddFloat() but this could lead to a proliferation of names each one describing different methods that are essentially performing the same operation i.e. adding two numbers. To overcome this problem in C# you are not required to give each method a unique name – thus both of the methods above could be called Add(). However if method names are not unique the C# must have some other way of deciding which method to invoke at run time. i.e. when a call is made to Add(number1, number2) the machine must decide which of the two methods to use. It does this by looking at the parameter list. While the two methods may have the same name they can still be distinguished by looking at the parameter list. :- Add(int number1, int number2) Add(float number1, float number2) This is resolved at run time by looking at the method call and the actual parameters being passed. If two integers are being passed then the first method is invoked. However if two floating point numbers are passed then the second method is used. Overloading refers to the fact that several methods may share the same name. As method names are no longer uniquely identify the method then the name is ‘overloaded’. Having several methods that essentially perform the same operation, but which take different parameter lists, can lead to enhanced flexibility and robustness in a system. Imagine a University student management system. A method would probably be required to enrol, or register, a new student. Such a method could have the following signature … EnrollStudent(String name, String address, String coursecode) However if a student had just arrived in the city and had not yet sorted out where they were living would the University want to refuse to enrol the student? They could do so but would it not be better to allow such a student to enrol (and set the address to ‘unkown’)? To allow this the method EnrollStudent() could be overloaded and an alternative method provided as… EnrollStudent(String name, String coursecode) At run time the method invoked will depend upon the parameter list provided. Thus given a call to EnrollStudent(“Fred”, “123 Abbey Gardens”, “G700”) the first method would be used. Overloading methods don’t just provide more flexibility for the user they also provide more flexibility for programmers who may have the job of extending the system in the future and thus overloading methods can make the system more future proof and robust to changing requirements. Constructors can be overloaded as well as ordinary methods. We can make our programs more adaptable by overloading constructors and other methods. Even if we don’t initially use all of the different constructors, or methods, by providing them we are making our programs more flexible and adaptable to meet changing requirements. Method overloading is the name given to the concept that several methods may exist that essentially perform the same operation and thus have the same name. The CLR engine distinguishes these by looking at the parameter list. If two or more methods have the same name then their parameter list must be different. At run time each method call, which may be ambiguous, is resolved by the CLR engine by looking at the parameters passed and matching the data types with the method signatures defined in the class. By overloading constructors and ordinary methods we are providing extra flexibility to the programmers who may use our classes in the future. Even if these are not all used initially, providing these can help make the program more flexible to meet changing user requirements. The development of any computer program starts by identifying a need :- • An engineer who specialises in designing bridges may need some software to create three dimensional models of the designs so people can visualise the finished bridge long before it is actually built. • A manager may need a piece of software to keep track of personnel, what projects they are assigned to, what skills they have and what skills need to be developed etc. But how do we get from a ‘need’ for some software to an object oriented software design that will meet this need? Some software engineers specialise in the task of Requirement Analysis which is the task of clarifying exactly what is required of the software. Often this is done by iteratively performing the following tasks :- 1) interviewing clients and potential users of the system to find out what they say about the system needed 2) documenting the results of these conversations, 3) identifying the essential features of the required system 4) producing preliminary designs (and possibly prototypes of the system) 5) evaluating these initial plans with the client and potential users 6) repeating the steps above until a finished design has evolved. Performing requirements analysis is a specialised skill that is outside the scope of this text but here we will focus on steps three and four above ie. given a description of a system how do we convert this into a potential OO design. While we can hope to develop preliminary design skills experience is a significant factor in this task. Producing simple and elegant designs is important if we want the software to work well and be easy to develop however identifying good designs from weaker designs is not simple and experience is a key factor. A novice chess player may know all the rules but it takes experience to learn how to choose good moves from bad moves and experience is essential to becoming a skilled player. Similarly experience is essential to becoming skilled at performing user requirements analysis and in producing good designs. ﻿We can can use the following C constructs to control program execution. When we can count our way through a sequence or series: When we are waiting for some condition to change: or if we want to do something at least once then test: When we have a single option to test: When we have more options to test: When we have more options to test based on an integer or single character value: This part is all about if, and then, and else and true and false – the nuts and bolts of how we express and control the execution of a program. This can be very dry and dusty material so to make it more understandable we are going to solve a problem you are going to need to solve to do any interactive web work of any complexity. We will build something we can use in order to provide something like the functionality that can be obtained from typical getParameter(“ITEM1”) method in Java servlets or $_REQUEST[''ITEM1”] function in PHP. In Chapter 1 we saw that environment variables can be accessed by the implicit argument to the main function. We can also use the library function getenv() to request the value of any named environment variable. Here we display the QUERY_STRING which is what the program gets as the entire contents of an HTML form which contains NAME=VALUE pairs for all the named form elements. An HTML form by default uses the GET method which transmits all form data back to the program or page that contains the form unless otherwise specified in an action attribute. This data is contained in the QUERY_STRING as a series of variable = value pairs separated by the & character. Note that in HTML values of things are enclosed in quotation marks, so to embed these inside a C string we have to “escape” the character with a special sign \ like this “\”ITEM1\” “. Also we are using “\n” or explicit new line characters at the end of each piece of HTML output, so that when we select “view source” in the browser we get some reasonably formatted text to view rather than the whole page appearing as one long single line. Calling this program in a browser we see a form and can enter some data in the boxes: And after submitting the form we see: To make much sense of the QUERY_STRING and find a particular value in it, we are going to have to parse it, to chop it up into its constituent pieces and for this we will need some conditional logic (if, else etc) and some loop to count through the characters in the variable. A basic function to do this would ideally be created as this is a task you might need to do do again and again so it makes sense to have a chunk of code that can be called over again. In the next example we add this function and the noticeable difference in the output is that we can insert the extracted values into the HTML boxes after we have parsed them. We seem to have successfully created something like a java getParameter() function – or have we? Have a good long look at chapter4_2.c and try it out with characters other than A-Z a-z or numerals and you will see something is not quite right. There is some kind of encoding going on here! If I were tp type DAVID !!! into the first field: I get this result: A space character has become a + and ! has become %21. This encoding occurs because certain characters are explicitly used in the transmission protocol itself. The & for example is used to separate portions of the QUERY_STRING and the space cannot be sent at all as it is. Any program wishing to use information from the HTML form must be able to decode all this stuff which will now attempt to do. The program chapter4_2.c accomplishes what we see so far. It has a main function and a decode_value function all in the same file. The decode_value function takes three arguments: the name of the value we are looking for “ITEM1=” or “ITEM2=”. the address of the variable into which we are going to put the value if found the maximum number of characters to copy The function looks for the start and end positions in the QUERY_STRING of the value and then copies the characters found one by one to the value variable, adding a NULL charcter to terminate the string. It looks like we are going to have to do some serious work on this decode_value package so as this is work we can expect to do over and over again it makes sense to write a function that can be reused. First off we can put this function into a separate file called decode_value.c and create a file for all the functions we may write called c_in_linux.h and compile all this into a library. In the Make file we can add: This looks horrible and complex but all it means is this: typing “make all” will: compile all the *.c files listed in the list OBJ_SRC and into object files *.o compile all the object files into a library archive called lib_c_in_linux.a compile 4-4 using this new archive. This is the model we will use to keep our files as small as possible and the share-ability of code at its maximum. We can now have a simpler “main” function file, and files for stuff we might want to write as call-able functions from anywhere really which we do not yet know about. All this is organised into a library file (*.a for archive) – these can also be compiled as dynamically loadable shared objects *.so whch are much like Windows DLLs. This exactly how all Linux software is written and delivered. For example the MySQL C Application Programmers Interface (API) comprises: all the header files in /usr/include/mysql the library file /usr/lib/mysql/libmysqlclient.a What we are doing really is how all of Linux is put together – we are simply adding to it in the same way. Our main file now looks like this:$$$﻿We can can use the following C constructs to control program execution. When we can count our way through a sequence or series: When we are waiting for some condition to change: or if we want to do something at least once then test: When we have a single option to test: When we have more options to test: When we have more options to test based on an integer or single character value: This part is all about if, and then, and else and true and false – the nuts and bolts of how we express and control the execution of a program. This can be very dry and dusty material so to make it more understandable we are going to solve a problem you are going to need to solve to do any interactive web work of any complexity. We will build something we can use in order to provide something like the functionality that can be obtained from typical getParameter(“ITEM1”) method in Java servlets or $_REQUEST[''ITEM1”] function in PHP. In Chapter 1 we saw that environment variables can be accessed by the implicit argument to the main function. We can also use the library function getenv() to request the value of any named environment variable. Here we display the QUERY_STRING which is what the program gets as the entire contents of an HTML form which contains NAME=VALUE pairs for all the named form elements. An HTML form by default uses the GET method which transmits all form data back to the program or page that contains the form unless otherwise specified in an action attribute. This data is contained in the QUERY_STRING as a series of variable = value pairs separated by the & character. Note that in HTML values of things are enclosed in quotation marks, so to embed these inside a C string we have to “escape” the character with a special sign \ like this “\”ITEM1\” “. Also we are using “\n” or explicit new line characters at the end of each piece of HTML output, so that when we select “view source” in the browser we get some reasonably formatted text to view rather than the whole page appearing as one long single line. Calling this program in a browser we see a form and can enter some data in the boxes: And after submitting the form we see: To make much sense of the QUERY_STRING and find a particular value in it, we are going to have to parse it, to chop it up into its constituent pieces and for this we will need some conditional logic (if, else etc) and some loop to count through the characters in the variable. A basic function to do this would ideally be created as this is a task you might need to do do again and again so it makes sense to have a chunk of code that can be called over again. In the next example we add this function and the noticeable difference in the output is that we can insert the extracted values into the HTML boxes after we have parsed them. We seem to have successfully created something like a java getParameter() function – or have we? Have a good long look at chapter4_2.c and try it out with characters other than A-Z a-z or numerals and you will see something is not quite right. There is some kind of encoding going on here! If I were tp type DAVID !!! into the first field: I get this result: A space character has become a + and ! has become %21. This encoding occurs because certain characters are explicitly used in the transmission protocol itself. The & for example is used to separate portions of the QUERY_STRING and the space cannot be sent at all as it is. Any program wishing to use information from the HTML form must be able to decode all this stuff which will now attempt to do. The program chapter4_2.c accomplishes what we see so far. It has a main function and a decode_value function all in the same file. The decode_value function takes three arguments: the name of the value we are looking for “ITEM1=” or “ITEM2=”. the address of the variable into which we are going to put the value if found the maximum number of characters to copy The function looks for the start and end positions in the QUERY_STRING of the value and then copies the characters found one by one to the value variable, adding a NULL charcter to terminate the string. It looks like we are going to have to do some serious work on this decode_value package so as this is work we can expect to do over and over again it makes sense to write a function that can be reused. First off we can put this function into a separate file called decode_value.c and create a file for all the functions we may write called c_in_linux.h and compile all this into a library. In the Make file we can add: This looks horrible and complex but all it means is this: typing “make all” will: compile all the *.c files listed in the list OBJ_SRC and into object files *.o compile all the object files into a library archive called lib_c_in_linux.a compile 4-4 using this new archive. This is the model we will use to keep our files as small as possible and the share-ability of code at its maximum. We can now have a simpler “main” function file, and files for stuff we might want to write as call-able functions from anywhere really which we do not yet know about. All this is organised into a library file (*.a for archive) – these can also be compiled as dynamically loadable shared objects *.so whch are much like Windows DLLs. This exactly how all Linux software is written and delivered. For example the MySQL C Application Programmers Interface (API) comprises: all the header files in /usr/include/mysql the library file /usr/lib/mysql/libmysqlclient.a What we are doing really is how all of Linux is put together – we are simply adding to it in the same way. Our main file now looks like this:
EN28	1	﻿As its name implies control engineering involves the design of an engineering product or system where a requirement is to accurately control some quantity, say the temperature in a room or the position or speed of an electric motor. To do this one needs to know the value of the quantity being controlled, so that being able to measure is fundamental to control. In principle one can control a quantity in a so called open loop manner where ‘knowledge’ has been built up on what input will produce the required output, say the voltage required to be input to an electric motor for it to run at a certain speed. This works well if the ‘knowledge’ is accurate but if the motor is driving a pump which has a load highly dependent on the temperature of the fluid being pumped then the ‘knowledge’ will not be accurate unless information is obtained for different fluid temperatures. But this may not be the only practical aspect that affects the load on the motor and therefore the speed at which it will run for a given input, so if accurate speed control is required an alternative approach is necessary. This alternative approach is the use of feedback whereby the quantity to be controlled, say C, is measured, compared with the desired value, R, and the error between the two, E = R - C used to adjust C. This gives the classical feedback loop structure of Figure 1.1. In the case of the control of motor speed, where the required speed, R, known as the reference is either fixed or moved between fixed values, the control is often known as a regulatory control, as the action of the loop allows accurate speed control of the motor for the aforementioned situation in spite of the changes in temperature of the pump fluid which affects the motor load. In other instances the output C may be required to follow a changing R, which for example, might be the required position movement of a robot arm. The system is then often known as a servomechanism and many early textbooks in the control engineering field used the word servomechanism in their title rather than control. The use of feedback to regulate a system has a long history [1.1, 1.2], one of the earliest concepts, used in Ancient Greece, was the float regulator to control water level, which is still used today in water tanks. The first automatic regulator for an industrial process is believed to have been the flyball governor developed in 1769 by James Watt. It was not, however, until the wartime period beginning in 1939, that control engineering really started to develop with the demand for servomechanisms for munitions fire control and guidance. With the major improvements in technology since that time the applications of control have grown rapidly and can be found in all walks of life. Control engineering has, in fact, been referred to as the ‘unseen technology’ as so often people are unaware of its existence until something goes wrong. Few people are, for instance, aware of its contribution to the development of storage media in digital computers where accurate head positioning is required. This started with the magnetic drum in the 50’s and is required today in disk drives where position accuracy is of the order of 1μm and movement between tracks must be done in a few ms. Feedback is, of course, not just a feature of industrial control but is found in biological, economic and many other forms of system, so that theories relating to feedback control can be applied to many walks of life. The book is concerned with theoretical methods for continuous linear feedback control system design, and is primarily restricted to single-input single-output systems. Continuous linear time invariant systems have linear differential equation mathematical models and are always an approximation to a real device or system. All real systems will change with time due to age and environmental changes and may only operate reasonably linearly over a restricted range of operation. There is, however, a rich theory for the analysis of linear systems which can provide excellent approximations for the analysis and design of real world situations when used within the correct context. Further simulation is now an excellent means to support linear theoretical studies as model errors, such as the affects of neglected nonlinearity, can easily be assessed. There are total of 11 chapters and some appendices, the major one being Appendix A on Laplace transforms. The next chapter provides a brief description of the forms of mathematical model representations used in control engineering analysis and design. It does not deal with mathematical modelling of engineering devices, which is a huge subject and is best dealt with in the discipline covering the subject, since the devices or components could be electrical, mechanical, hydraulic etc. Suffice to say that one hopes to obtain an approximate linear mathematical model for these components so that their effect in a system can be investigated using linear control theory. The mathematical models discussed are the linear differential equation, the transfer function and a state space representation, together with the notations used for them in MATLAB. Chapter 3 discusses transfer functions, their zeros and poles, and their responses to different inputs. The following chapter discusses in detail the various methods for plotting steady state frequency responses with Bode, Nyquist and Nichols plots being illustrated in MATLAB. Hopefully sufficient detail, which is brief when compared with many textbooks, is given so that the reader clearly understands the information these plots provide and more importantly understands the form of frequency response expected from a specific transfer function. The material of chapters 2-4 could be covered in other courses as it is basic systems theory, there having been no mention of control, which starts in chapter 5. The basic feedback loop structure shown in Figure 1.1 is commented on further, followed by a discussion of typical performance specifications which might have to be met in both the time and frequency domains. Steady state errors are considered both for input and disturbance signals and the importance and properties of an integrator are discussed from a physical as well as mathematical viewpoint. The chapter concludes with a discussion on stability and a presentation of several results including the Mikhailov criterion, which is rarely mentioned in English language texts. ﻿Control systems exist in many fields of engineering so that components of a control system may be electrical, mechanical, hydraulic etc. devices. If a system has to be designed to perform in a specific way then one needs to develop descriptions of how the outputs of the individual components, which make up the system, will react to changes in their inputs. This is known as mathematical modelling and can be done either from the basic laws of physics or from processing the input and output signals in which case it is known as identification. Examples of physical modelling include deriving differential equations for electrical circuits involving resistance, inductance and capacitance and for combinations of masses, springs and dampers in mechanical systems. It is not the intent here to derive models for various devices which may be used in control systems but to assume that a suitable approximation will be a linear differential equation. In practice an improved model might include nonlinear effects, for example Hooke’s Law for a spring in a mechanical system is only linear over a certain range; or account for time variations of components. Mathematical models of any device will always be approximate, even if nonlinear effects and time variations are also included by using more general nonlinear or time varying differential equations. Thus, it is always important in using mathematical models to have an appreciation of the conditions under which they are valid and to what accuracy. Starting therefore with the assumption that our model is a linear differential equation then in general it will have the form:- where D denotes the differential operator d/dt. A(D) and B(D) are polynomials in D with Di d i / dt i , the ith derivative, u(t) is the model input and y(t) its output. So that one can write where the a and b coefficients will be real numbers. The orders of the polynomials A and B are assumed to be n and m, respectively, with n m. Thus, for example, the differential equation with the dependence of y and u on t assumed can be written  In order to solve an nth order differential equation, that is determine the output y for a given input u, one must know the initial conditions of y and its first n-1 derivatives. For example if a projectile is falling under gravity, that is constant acceleration, so that D2y= constant, where y is the height, then in order to find the time taken to fall to a lower height, one must know not only the initial height, normally assumed to be at time zero, but the initial velocity, dy/dt, that is two initial conditions as the equation is second order (n = 2). Control engineers typically study solutions to differential equations using either Laplace transforms or a state space representation. A short introduction to the Laplace transformation is given in Appendix A for the reader who is not familiar with its use. It is an integral transformation and its major, but not sole use, is for differential equations where the independent time variable t is transformed to the complex variable s by the expression Since the exponential term has no units the units of s are seconds-1, that is using mks notation s has units of s-1. If denotes the Laplace transform then one may write [f(t)] = F(s) and -1[F(s)] = f(t). The relationship is unique in that for every f(t), [F(s)], there is a unique F(s), [f(t)]. It is shown in Appendix A that when the n-1 initial conditions, Dn-1y(0) are zero the Laplace transform of Dny(t) is snY(s). Thus the Laplace transform of the differential equation (2.1) with zero initial conditions can be written with the assumed notation that signals as functions of time are denoted by lower case letters and as functions of s by the corresponding capital letter. If equation (2.8) is written then this is known as the transfer function, G(s), between the input and output of the ‘system’, that is whatever is modelled by equation (2.1). B(s), of order m, is referred to as the numerator polynomial and A(s), of order n, as the denominator polynomial and are from equations (2.2) and (2.3) Since the a and b coefficients of the polynomials are real numbers the roots of the polynomials are either real or complex pairs. The transfer function is zero for those values of s which are the roots of B(s), so these values of s are called the zeros of the transfer function. Similarly, the transfer function will be infinite at the roots of the denominator polynomial A(s), and these values are called the poles of the transfer function. The general transfer function (2.9) thus has m zeros and n poles and is said to have a relative degree of n-m, which can be shown from physical realisation considerations cannot be negative. Further for n > m it is referred to as a strictly proper transfer function and for n m as a proper transfer function. When the input u(t) to the differential equation of (2.1) is constant the output y(t) becomes constant when all the derivatives of the output are zero. Thus the steady state gain, or since the input is often thought of as a signal the term d.c. gain (although it is more often a voltage than a current!) is used, and is given by If the n roots of A(s) are i , i = 1….n and of B(s) are j, j = 1….m, then the transfer function may be written in the zero-pole form When the transfer function is known in the zero-pole form then the location of its zeros and poles can be shown on an s plane zero-pole plot, where the zeros are marked with a circle and the poles by a cross. The information on this plot then completely defines the transfer function apart from the gain K. In most instances engineers prefer to keep any complex roots in quadratic form, thus for example writing  ﻿As mentioned previously a major reason for wishing to obtain a mathematical model of a device is to be able to evaluate the output in response to a given input. Using the transfer function and Laplace transforms provides a particularly elegant way of doing this. This is because for a block with input U(s) and transfer function G(s) the output Y(s) = G(s)U(s). When the input, u(t), is a unit impulse which is conventionally denoted by (t), U(s) = 1 so that the output Y(s) = G(s). Thus in the time domain, y(t) = g(t), the inverse Laplace transform of G(s), which is called the impulse response or weighting function of the block. The evaluation of y(t) for any input u(t) can be done in the time domain using the convolution integral (see Appendix A, theorem (ix)) but it is normally much easier to use the transform relationship Y(s) = G(s)U(s). To do this one needs to find the Laplace transform of the input u(t), form the product G(s)U(s) and then find its inverse Laplace transform. G(s)U(s) will be a ratio of polynomials in s and to find the inverse Laplace transform, the roots of the denominator polynomial must be found to allow the expression to be put into partial fractions with each term involving one denominator root (pole). Assuming, for example, the input is a unit step so that U(s) = 1/s then putting G(s)U(s) into partial fractions will result in an expression for Y(s) of the form where in the transfer function G(s) = B(s)/A(s), the n poles of G(s) [zeros of A(s)] are i, i = 1…n and the coefficients C0 and Ci, i = 1…n, will depend on the numerator polynomial B(s), and are known as the residues at the poles. Taking the inverse Laplace transform yields The first term is a constant C0, sometimes written C0u0(t) because the Laplace transform is defined for t 0, where u0(t) denotes the unit step at time zero. Each of the other terms is an exponential, which provided the real part of i is negative will decay to zero as t becomes large. In this case the transfer function is said to be stable as a bounded input has produced a bounded output. Thus a transfer function is stable if all its poles lie in the left hand side (lhs) of the s plane zero-pole plot illustrated in Figure 2.1. The larger the negative value of i the more rapidly the contribution from the ith term decays to zero. Since any poles which are complex occur in complex pairs, say of the form 1, 2 = ± j , then the corresponding two residues C1 and C2 will be complex pairs and the two terms will combine to give a term of the form Ce t sin( t ) . This is a damped oscillatory exponential term where , which will be negative for a stable transfer function, determines the damping and the frequency [strictly angular frequency] of the oscillation. For a specific calculation most engineers, as mentioned earlier, will leave a complex pair of roots as a quadratic factor in the partial factorization process, as illustrated in the Laplace transform inversion example given in Appendix A. For any other input to G(s), as with the step input, the poles of the Laplace transform of the input will occur in a term of the partial fraction expansion (3.2), [as for the C0/s term above], and will therefore produce a bounded output for a bounded input. In control engineering the major deterministic input signals that one may wish to obtain responses to are a step, an impulse, a ramp and a constant frequency input. The purpose of this section is to discuss step responses of specific transfer functions, hopefully imparting an understanding of what can be expected from a knowledge of the zeros and poles of the transfer function without going into detailed mathematics. A transfer function with a single pole is s a G s K  ( ) 1 , which may also be written in the socalled time constant form sT G s K   1 ( ) , where K K / a 1 and T 1/ a The steady state gainG(0) K , that is the final value of the response, and T is called the time constant as it determines the speed of the response. K will have units relating the input quantity to the output quantity, for example °C/V, if the input is a voltage and the output temperature. T will have the same units of time as s-1, normally seconds. The output, Y(s), for a unit step input is given by Taking the inverse Laplace transform gives the result The larger the value of T (i.e. the smaller the value of a), the slower the exponential response. It can easily be shown that y(T) 0.632K , T dt dy(0) and y(5T) 0.993K or in words, the output reaches 63.2% of the final value after a time T, the initial slope of the response is T and the response has essentially reached the final value after a time 5T. The step response in MATLAB can be obtained by the command step(num,den). The figure below shows the step response for the transfer function with K = 1 on a normalised time scale. Here the transfer function G(s) is often assumed to be of the form It has a unit steady state gain, i.e G(0) = 1, and poles at   1 2 o o s j , which are complex when 1. For a unit step input the output Y(s), can be shown after some algebra, which has been done so that the inverse Laplace transforms of the second and third terms are damped cosinusoidal and sinusoidal expressions, to be given by Taking the inverse Laplace transform it yields, again after some algebra, where cos 1 . is known as the damping ratio. It can also be seen that the angle to the negative real axis from the origin to the pole with positive imaginary part is tan 1 (1 2 )1/ 2 / cos 1 .  ﻿The frequency response of a transfer function G(j ) was introduced in the last chapter. As G(j ) is a complex number with a magnitude and argument (phase) if one wishes to show its behaviour over a frequency range then one has 3 parameters to deal with the frequency, , the magnitude, M, and the phase . Engineers use three common ways to plot the information, which are known as Bode diagrams, Nyquist diagrams and Nichols diagrams in honour of the people who introduced them. All portray the same information and can be readily drawn in MATLAB for a system transfer function object G(s). One diagram may prove more convenient for a particular application, although engineers often have a preference. In the early days when computing facilities were not available Bode diagrams, for example, had some popularity because of the ease with which they could, in many instances, be rapidly approximated. All the plots will be discussed below, quoting many results without going into mathematical detail, in the hope that the reader will obtain enough knowledge to know whether MATLAB plots obtained are of the general shape expected. A Bode diagram consists of two separate plots the magnitude, M, as a function of frequency and the phase as a function of frequency. For both plots the frequency is plotted on a logarithmic (log) scale along the x axis. A log scale has the property that the midpoint between two frequencies 1 and 2 is the frequency 1 2    . A decade of frequency is from a value to ten times that value and an octave from a value to twice that value. The magnitude is plotted either on a log scale or in decibels (dB), where dB M 10 20log . The phase is plotted on a linear scale. Bode showed that for a transfer function with no right hand side (rhs) s-plane zeros the phase is related to the slope of the magnitude characteristic by the relationship It can be further shown from this expression that a relatively good approximation is that the phase at any frequency is 15° times the slope of the magnitude curve in dB/octave. This was a useful concept to avoid drawing both diagrams when no computer facilities were available. For two transfer functions G1 and G2 in series the resultant transfer function, G, is their product, this means for their frequency response which in terms of their magnitudes and phases can be written Thus since a log scale is used on the magnitude of a Bode diagram this means Bode magnitude plots for two transfer functions in series can be added, as also their phases on the phase diagram. Hence a transfer function in zero-pole form can be plotted on the magnitude and phase Bode diagrams simple by adding the individual contributions from each zero and pole. It is thus only necessary to know the Bode plots of single roots and quadratic factors to put together Bode plots for a complicated transfer function if it is known in zero-pole form. The single pole transfer function is normally considered in time constant form with unit steady state gain, that is It is easy to show that this transfer function can be approximated by two straight lines, one constant at 0 dB, as G(0) = 1, until the frequency, 1/T, known as the break point, and then from that point by a line with slope -6dB/octave. The actual curve and the approximation are shown in Figure 4.1 together with the phase curve. The differences between the exact magnitude curve and the approximation are symmetrical, that is a maximum at the breakpoint of 3dB, 1dB one octave each side of the breakpoint, 0.3 dB two octaves away etc. The phase changes between 0° and - 90° again with symmetry about the breakpoint phase of -45°. Note a steady slope of -6 dB/octave has a corresponding phase of -90° The Bode magnitude plot of a single zero time constant, that is is simply a reflection in the 0 dB axis of the pole plot. That is the approximate magnitude curve is flat at 0 dB until the break point frequency, 1/T, and then increases at 6 dB/octave. Theoretically as the frequency tends to infinity so does its gain so that it is not physically realisable. The phase curve goes from 0° to +90° The transfer function of an integrator, which is a pole at the origin in the zero-pole plot, is 1/s. It is sometimes taken with a gain K, i.e.K/s. Here K will be replaced by 1/T to give the transfer function On a Bode diagram the magnitude is a constant slope of -6 dB/octave passing through 0 dB at the frequency 1/T. Note that on a log scale for frequency, zero frequency where the integrator has infinite gain (the transfer function can only be produced electronically by an active device) is never reached. The phase is -90° at all frequencies. A differentiator has a transfer function of sT which gives a gain characteristic with a slope of 6 dB/octave passing through 0dB at a frequency of 1/T. Theoretically it produces infinite gain at infinite frequency so again it is not physically realisable. It has a phase of +90° at all frequencies. The quadratic factor form is again taken for two complex poles with < 1 as in equation (3.7), that is Again G(0) = 1 so the response starts at 0 dB and can be approximated by a straight line at 0 dB until o and by a line from o at -12 dB/octave. However, this is a very coarse approximation as the behaviour around o is highly dependent on . It can be shown that the magnitude reaches a maximum value of which is approximately 1/2 for small , at a frequency of This frequency is thus always less than o and only exists for < 0.707. The response with = 0.707 always has magnitude, M < 1. The phase curve goes from 0° to - 180° as expected from the original and final slopes of the magnitude curve, it has a phase shift of -90° at the frequency o independent of and changes more rapidly near o for smaller , as expected due to the more rapid change in the slope of the corresponding magnitude curve. ﻿State space modelling was briefly introduced in chapter 2. Here more coverage is provided of state space methods before some of their uses in control system design are covered in the next chapter. A state space model, or representation, as given in equation (2.26), is denoted by the two equations where equations (10.1) and (10.2) are respectively the state equation and output equation. The representation can be used for both single-input single-output systems (SISO) and multipleinput multiple-output systems (MIMO). For the MIMO representation A, B, C and D will all be matrices. If the state dimension is n and there are r inputs and m outputs then A, B , C and D will be matrices of order, n x n, n x r, m x n and m x r, respectively. For SISO systems B will be an n x 1 column vector, often denoted by b, C a 1 x n row vector, often denoted by cT, and D a scalar often denoted by d. Here the capital letter notation will be used, even though only SISO systems are considered, and B, C, and D will have the aforementioned dimensions. As mentioned in chapter 2 the choice of states is not unique and this will be considered further in section 10.3. First, however, obtaining a solution of the state equation is discussed in the next section. Obtaining the time domain solution to the state equation is analogous to the classical approach used to solve the simple first order equation The procedure in this case is to take u = 0, initially, and to assume a solution for x(t) of eatx(0) where x(0) is the initial value of x(t). Differentiating this expression gives so that the assumed solution is valid. Now if the input u is considered this is assumed to yield a solution of the form which on differentiating gives Thus the differential equation is satisfied if which has the solution where is a dummy variable. This solution can be written so that the complete solution for x(t) consists of the sum of the two solutions, known as the complimentary function (or initial condition response) and particular integral (or forced response), respectively and is For equation (10.1) x is an n vector and A an n x n matrix not a scalar a and to obtain the complimentary function one assumes x(t) e At x(0) . eAt is now a function of a matrix, which is defined by an infinite power series in exactly the same way as the scalar expression, so that where I is the n x n identity matrix. Term by term differentiation of equation (10.5) shows that the derivative of eAt is AeAt and that x(t) e At x(0) satisfies the differential equation with u = 0. eAt is often denoted by (t) and is known as the state transition matrix. Using the same approach as for the scalar case to get the forced response the total solution is found to be It is easily shown that the state transition matrix ( ) e A has the property that so that equation (10.6) can be written alternatively as This time domain solution of equation (10.1) is useful but most engineers prefer to make use of the Laplace transform approach. Taking the Laplace transform of equation (10.1) gives which on rearranging as X(s) is an n vector and A a n x n matrix gives Taking the inverse Laplace transform of this and comparing with equation (10.7) indicates that Also taking the Laplace transform of the output equation (10.2) and substituting for X(s) gives so that the transfer function, G(s), between the input u and output y is This will, of course, be the same independent of the choice of the states. Obviously there must be an algebraic relationship between different possible choices of state variables. Let this relationship be where x is the original choice in equations (10.1) and (10.2) and z is the new choice. Substituting this relationship in equation (10.2) givesTz ATz Bu which can be written Also substituting in the output equation (10.2) gives Thus under the state transformation of equation (10.13) a different state space representation (T 1AT,T 1B,CT,D) is obtained. If the new A matrix is denoted by A T AT z  1 then it is easy to show that A and Az have the following properties (i) The same eigenvalues (ii) The same determinant (iii) The same trace (Sum of elements on the main diagonal) There are some specific forms of the A matrix which are often commonly used in control engineering and not unsurprisingly these relate to how one might consider obtaining a state space representation for a transfer function, the topic of the next section. This topic was introduced in section 2.3 where the controllable canonical form for a differential equation was considered. Here this and some other forms will be considered by making use of block diagrams where every state will be an integrator output. To develop some representations consider the transfer function As seen from equation (2.20) the first n-1 state variables are integrals of the next state, that is   x x dx ( j 1) j , or as shown in the equation by j j x x ( 1) , for j = 2 to n. Thus the block diagram to represent this is n integrators in series. The input to the first integrator is n x and its value is given by x a x a x a x u n    .....  0 1 1 2 2 3 , the last row of the matrix representation of equation (2.20). The numerator terms are provided by feeding forward from the states to give the required output. Thus, for our simple example, this can be shown in the block diagram of Figure 10.1, done in SIMULINK, where since the transfer function is third order n = 3, there are three integrators, blocks with transfer functions 1/s, in series. Feedback from the states, where the integrator outputs from left to right are the states x3, x2, and x1, respectively, is by the coefficients -8, -14 and -7. (negative and in the reverse order of the transfer function denominator). The numerator coefficients provide feedforward from the states, with the s2 term from x3.$$$﻿State space modelling was briefly introduced in chapter 2. Here more coverage is provided of state space methods before some of their uses in control system design are covered in the next chapter. A state space model, or representation, as given in equation (2.26), is denoted by the two equations where equations (10.1) and (10.2) are respectively the state equation and output equation. The representation can be used for both single-input single-output systems (SISO) and multipleinput multiple-output systems (MIMO). For the MIMO representation A, B, C and D will all be matrices. If the state dimension is n and there are r inputs and m outputs then A, B , C and D will be matrices of order, n x n, n x r, m x n and m x r, respectively. For SISO systems B will be an n x 1 column vector, often denoted by b, C a 1 x n row vector, often denoted by cT, and D a scalar often denoted by d. Here the capital letter notation will be used, even though only SISO systems are considered, and B, C, and D will have the aforementioned dimensions. As mentioned in chapter 2 the choice of states is not unique and this will be considered further in section 10.3. First, however, obtaining a solution of the state equation is discussed in the next section. Obtaining the time domain solution to the state equation is analogous to the classical approach used to solve the simple first order equation The procedure in this case is to take u = 0, initially, and to assume a solution for x(t) of eatx(0) where x(0) is the initial value of x(t). Differentiating this expression gives so that the assumed solution is valid. Now if the input u is considered this is assumed to yield a solution of the form which on differentiating gives Thus the differential equation is satisfied if which has the solution where is a dummy variable. This solution can be written so that the complete solution for x(t) consists of the sum of the two solutions, known as the complimentary function (or initial condition response) and particular integral (or forced response), respectively and is For equation (10.1) x is an n vector and A an n x n matrix not a scalar a and to obtain the complimentary function one assumes x(t) e At x(0) . eAt is now a function of a matrix, which is defined by an infinite power series in exactly the same way as the scalar expression, so that where I is the n x n identity matrix. Term by term differentiation of equation (10.5) shows that the derivative of eAt is AeAt and that x(t) e At x(0) satisfies the differential equation with u = 0. eAt is often denoted by (t) and is known as the state transition matrix. Using the same approach as for the scalar case to get the forced response the total solution is found to be It is easily shown that the state transition matrix ( ) e A has the property that so that equation (10.6) can be written alternatively as This time domain solution of equation (10.1) is useful but most engineers prefer to make use of the Laplace transform approach. Taking the Laplace transform of equation (10.1) gives which on rearranging as X(s) is an n vector and A a n x n matrix gives Taking the inverse Laplace transform of this and comparing with equation (10.7) indicates that Also taking the Laplace transform of the output equation (10.2) and substituting for X(s) gives so that the transfer function, G(s), between the input u and output y is This will, of course, be the same independent of the choice of the states. Obviously there must be an algebraic relationship between different possible choices of state variables. Let this relationship be where x is the original choice in equations (10.1) and (10.2) and z is the new choice. Substituting this relationship in equation (10.2) givesTz ATz Bu which can be written Also substituting in the output equation (10.2) gives Thus under the state transformation of equation (10.13) a different state space representation (T 1AT,T 1B,CT,D) is obtained. If the new A matrix is denoted by A T AT z  1 then it is easy to show that A and Az have the following properties (i) The same eigenvalues (ii) The same determinant (iii) The same trace (Sum of elements on the main diagonal) There are some specific forms of the A matrix which are often commonly used in control engineering and not unsurprisingly these relate to how one might consider obtaining a state space representation for a transfer function, the topic of the next section. This topic was introduced in section 2.3 where the controllable canonical form for a differential equation was considered. Here this and some other forms will be considered by making use of block diagrams where every state will be an integrator output. To develop some representations consider the transfer function As seen from equation (2.20) the first n-1 state variables are integrals of the next state, that is   x x dx ( j 1) j , or as shown in the equation by j j x x ( 1) , for j = 2 to n. Thus the block diagram to represent this is n integrators in series. The input to the first integrator is n x and its value is given by x a x a x a x u n    .....  0 1 1 2 2 3 , the last row of the matrix representation of equation (2.20). The numerator terms are provided by feeding forward from the states to give the required output. Thus, for our simple example, this can be shown in the block diagram of Figure 10.1, done in SIMULINK, where since the transfer function is third order n = 3, there are three integrators, blocks with transfer functions 1/s, in series. Feedback from the states, where the integrator outputs from left to right are the states x3, x2, and x1, respectively, is by the coefficients -8, -14 and -7. (negative and in the reverse order of the transfer function denominator). The numerator coefficients provide feedforward from the states, with the s2 term from x3.
EN12	0	﻿This chapter gives a very broad overview of •what a database is •what a relational database is, in particular •what a database management system (DBMS) is •what a DBMS does •how a relational DBMS does what a DBMS does We start to familiarise ourselves with terminology and notation used in the remainder of the book, and we get a brief introduction to each topic that is covered in more detail in later sections.  You will find many definitions of this term if you look around the literature and the Web. At one time (in 2008), Wikipedia [1] offered this: “A structured collection of records or data.” I prefer to elaborate a little:  The organized, machine-readable collection of symbols is what you “see” if you “look at” a database at a particular point in time. It is to be interpreted as a true account of the enterprise at that point in time. Of course it might happen to be incorrect, incomplete or inaccurate, so perhaps it is better to say that the account is believed to be true. The alternative view of a database as a collection of variables reflects the fact that the account of the enterprise has to change from time to time, depending on the frequency of change in the details we choose to include in that account. The suitability of a particular kind of database (such as relational, or object-oriented) might depend to some extent on the requirements of its user(s). When E.F. Codd developed his theory of relational databases (first published in 1969), he sought an approach that would satisfy the widest possible ranges of users and uses. Thus, when designing a relational database we do so without trying to anticipate specific uses to which it might be put, without building in biases that would favour particular applications. That is perhaps the distinguishing feature of the relational approach, and you should bear it in mind as we explore some of its ramifications.  For example, the table in Figure 1.1 shows an organized collection of symbols.  Can you guess what this tabular arrangement of symbols might be trying to tell us? What might it mean, for symbols to appear in the same row? In the same column? In what way might the meaning of the symbols in the very first row (shown in blue) differ from the meaning of those below them? Do you intuitively guess that the symbols below the first row in the first column are all student identifiers, those in the second column names of students, and those in the third course identifiers? Do you guess that student S1’s name is Anne? And that Anne is enrolled on courses C1 and C2? And that Cindy is enrolled on neither of those two courses? If so, what features of the organization of the symbols led you to those guesses? Remember those features. In an informal way they form the foundation of relational theory. Each of them has a formal counterpart in relational theory, and those formal counterparts are the only constituents of the organized structure that is a relational database.  Perhaps those green symbols, organized as they are with respect to the blue ones, are to be understood to mean: “Student S1, named Anne, is enrolled on course C1.” An important thing to note here is that only certain symbols from the sentence in quotes appear in the table—S1, Anne, and C1. None of the other words appear in the table. The symbols in the top row of the table (presumably column headings, though we haven’t actually been told that) might help us to guess “student”, “named”, and “course”, but nothing in the table hints at “enrolled”. And even if those assumed column headings had been A, B and C, or X, Y and Z, the given interpretation might still be the intended one. Now, we can take the sentence “Student S1, named Anne, is enrolled on course C1” and replace each of S1, Anne, and C1 by the corresponding symbols taken from some other row in the table, such as S2, Boris, and C1. In so doing, we are applying exactly the same mode of interpretation to each row. If that is indeed how the table is meant to be interpreted, then we can conclude that the following sentences are all true: Student S1, named Anne, is enrolled on course C1. Student S1, named Anne, is enrolled on course C2. Student S2, named Boris, is enrolled on course C1. Student S3, named Cindy, is enrolled on course C3.  In Chapter 3, “Predicates and Propositions”, we shall see exactly how such interpretations can be systematically formalized. In Chapter 4, “Relational Algebra The Foundation”, and Chapter 5, “Building on The Foundation”, we shall see how they help us to formulate correct queries to derive useful information from a relational database.  We have added the name, ENROLMENT, above the table, and we have added an extra row. ENROLMENT is a variable. Perhaps the table we saw earlier was once its value. If so, it (the variable) has been updated since then the row for S4 has been added. Our interpretation of Figure 1.1 now has to be revised to include the sentence represented by that additional row: Student S1, named Anne, is enrolled on course C1. Student S1, named Anne, is enrolled on course C2. Student S2, named Boris, is enrolled on course C1. Student S3, named Cindy, is enrolled on course C3. Student S4, named Devinder, is enrolled on course C1. Notice that in English we can join all these sentences together to form a single sentence, using conjunctions like “and”, “or”, “because” and so on. If we join them using “and” in particular, we get a single sentence that is logically equivalent to the given set of sentences in the sense that it is true if each one of them is true (and false if any one of them is false). A database, then, can be thought of as a representation of an account of the enterprise expressed as a single sentence! (But it’s more usual to think in terms of a collection of individual sentences.) We might also be able to conclude that the following sentences (for example) are false: Student S2, named Boris, is enrolled on course C2. Student S2, named Beth, is enrolled on course C1.  ﻿In this chapter we look at the four fundamental concepts on which most computer languages are based. We acquire some useful terminology to help us talk about these concepts in a precise way, and we begin to see how the concepts apply to relational database languages in particular. It is quite possible that you are already very familiar with these concepts indeed, if you have done any computer programming they cannot be totally new to you but I urge you to study the chapter carefully anyway, as not everybody uses exactly the same terminology (and not everybody is as careful about their use of terminology as we need to be in the present context). And in any case I also define some special terms, introduced by C.J. Date and myself in the 1990s, which have perhaps not yet achieved wide usage for example, selector and possrep. I wrote “most computer languages” because some languages dispense with variables. Database languages typically do not dispense with variables because it seems to be the very nature of what we call a database that it varies over time in keeping with changes in the enterprise. Money changes hands, employees come and go, get salary rises, change jobs, and so on. A language that supports variables is said to be an imperative language (and one that does not is a functional language). The term “imperative” appeals to the notion of commands that such a language needs for purposes such as updating variables. A command is an instruction, written in some computer language, to tell the system to do something. The terms statement (very commonly) and imperative (rarely) are used instead of command. In this book I use statement quite frequently, bowing to common usage, but I really prefer command because it is more appropriate; also, in normal discourse statement refers to a sentence of the very important kind described in Chapter 3 and does not instruct anybody to do anything.  Figure 2.1 shows a simple command the assignment, Y := X + 1 dissected into its component parts. The annotations show the terms we use for those components.  It is important to distinguish carefully between the concepts and the language constructs that represent (denote) those concepts. It is the distinction between what is written and what it means syntax and semantics.  Each annotated component in Figure 1 is an example of a certain language construct. The annotation shows the term used for the language construct and also the term for the concept it denotes. Honouring this distinction at all times can lead to laborious prose. Furthermore, we don’t always have distinct terms for the language construct and the corresponding concept. For example, there is no single-word term for an expression denoting an argument. We can write “argument expression” when we need to be absolutely clear and there is any danger of ambiguity, but normally we would just say, for example, that X+1 is an argument to that invocation of the operator “:=” shown in Figure 2.1. (The real argument is the result of evaluating X+1.) The update operator “:=” is known as assignment. The command Y := X+1 is an invocation of assignment, often referred to as just an assignment. The effect of that assignment is to evaluate the expression X+1, yielding some numerical result r and then to assign r to the variable Y. Subsequent references to Y therefore yield r (until some command is given to assign something else to Y). Note the two operands of the assignment: Y is the target, X+1 the source. The terms target and source here are names for the parameters of the operator. In the example, the argument expression Y is substituted for the parameter target and the argument expression X+1 is substituted for the parameter source. We say that target is subject to update, meaning that any argument expression substituted for it must denote a variable. The other parameter, source, is not subject to update, so any argument expression substituted must denote a value, not a variable. Y denotes a variable and X+1 denotes a value. When the assignment is evaluated (or, as we sometimes say of commands, executed), the variable denoted by Y becomes the argument substituted for target, and the current value of X+1 becomes the argument substituted for source. Whereas the Y in Y := X + 1 denotes a variable, as I have explained, the X in Y := X + 1 does not, as I am about to explain. So now let’s analyse the expression X+1. It is an invocation of the read-only operator +, which has two parameters, perhaps named a and b. Neither a nor b is subject to update. A read-only operator is one that has no parameter that is subject to update. Evaluation of an invocation of a read-only operator yields a value and updates nothing. The arguments to the invocation, in this example denoted by the expressions X and 1, are the values denoted by those two expressions. 1 is a literal, denoting the numerical value that it always denotes; X is a variable reference, denoting the value currently assigned to X. A literal is an expression that denotes a value and does not contain any variable references. But we do not use that term for all such expressions: for example, the expression 1+2, denoting the number 3, is not a literal. I defer a precise definition of literal to later in the present chapter.  The following very important distinctions emerge from the previous section and should be firmly taken on board: •Syntax versus semantics •Value versus variable •Variable versus variable reference •Update operator versus read-only operator •Operator versus invocation •Parameter versus argument •Parameter subject to update versus parameter not subject to update Each of these distinctions is illustrated in Figure 2.1, as follows: •Value versus variable: Y denotes a variable, X denotes the value currently assigned to the variable X. 1 denotes a value. Although X and Y are both symbols referencing variables, what they denote depends in the context in which those references appear. Y appears as an update target and thus denotes the variable of that name, whereas X appears where an expression denoting a value is expected and that position denotes the current value of the referenced variable. Note that variables, by definition, are subject to change (in value) from time to time. A value, by contrast, exists independently of time and space and is not subject to change.  ﻿In Chapter 1 I defined a database to be “… an organised, machine-readable collection of symbols, to be interpreted as a true account of some enterprise.” I also gave this example (extracted from Figure 1.1): I suggested that those green symbols, organised as they are with respect to the blue ones, might be understood to mean: “Student S1, named Anne, is enrolled on course C1.” In this chapter I explain exactly how such an interpretation can be justified. In fact, I describe the general method under which data organized in the form of relations is to be interpreted to yield information, as some people say. This method of interpretation is firmly based in the science of logic. Relational database theory is based very directly on logic. Predicates and propositions are the fundamental concepts that logic deals with. Fortunately, we need to understand only the few basic principles on which logic is founded. You may well already have a good grasp of the principles in question, but even if you do, please do not skip this chapter. For one thing, the textbooks on logic do not all use exactly the same terminology and I have chosen the terms and definitions that seem most suitable for the purpose at hand. For another, I do of course concentrate on the points that are particularly relevant to relational theory; you need to know which points those are and to understand exactly why they are so relevant. Predicates, one might say, are what logic is all about. And yet the textbooks do not speak with one voice when it comes to pinning down exactly what the term refers to! I choose the definition that appears to me to fit best, so to speak, with relational database theory. We start by looking again at that possible interpretation of the symbols S1, Anne, and C1, placed the way they are in Figure 1.1: “Student S1, named Anne, is enrolled on course C1.” This is a sentence. Sentences are what human beings typically use to communicate with each other, using language. We express our interpretations of the data using sentences in human language and we use relations to organize the data to be interpreted. Logic bridges the gap between relations and sentences. Our example sentence can be recast into two simpler sentences, “Student S1 is named Anne” and “Student S1 is enrolled on course C1”. Let’s focus on the second: The symbols S1 and C1 appear both in this sentence and in the data whose meaning it expresses. Because they each designate, or refer to, a particular thing S1 a particular student, C1 a particular course they are called designators. The word Anne is another designator, referring to a particular forename. “An Introduction to Relational Database Theory” is also a designator, referring to a particular book, and so is, for example, -7, referring to a particular number. Now, suppose we replace S1 and C1 in Example 3.1 by another pair of symbols, taken from the same columns of Figure 1.1 but a different row. Then we might obtain A pattern is clearly emerging. For every row in Figure 1.1, considering just the columns headed StudentId and CourseId, we can obtain a sentence in the form of Examples 3.1 and 3.2. The words “Student … is enrolled on course …” appear in that order in each case and in each case the gaps indicated by … sometimes called placeholders are replaced by appropriate designators. If we now replace each placeholder by the name given in the heading of the column from which the appropriate designator is to be drawn, we obtain this: Example 3.3 succinctly expresses the way in which the named columns in each row of Figure 1.1 are probably to be interpreted. And we now know that those names, StudentId and CourseId, in the column headings are the names of two of the attributes of the relation that Figure 1.1 depicts in tabular form. Now, the sentences in Examples 3.1 and 3.2 are in fact statements. They state something of which it can be said, “That is true”, or “That is not true”, or “I believe that”, or “I don’t believe that”. Not all sentences are statements. A good informal test, in English, to determine whether a sentence is a statement is to place “Is it true that” in front of it. If the result is a grammatical English question, then the original sentence is indeed a statement; otherwise it is not. Here are some sentences that are not statements: •“Let’s all get drunk.” •“Will you marry me?” •“Please pass me the salt.” •“If music be the food of love, play on.” They each fail the test. In fact one of them is a question itself and the other three are imperatives, but we have no need of such sentences in our interpretation of relations because we seek only information, in the form of statements statements that we are prepared to believe are statements of fact; in other words, statements we believe to be true. We do not expect a database to be interpreted as asking questions or giving orders. We expect it to be stating facts (or at least what are believed to be facts). As an aside, I must own up to the fact that some sentences that would be accepted as statements in English don’t really pass the test as they stand. Here are two cases in point, from Shakespeare: •“O for a muse of fire that would ascend the highest heaven of invention.” •“To be or not to be that is the question.” The first appears to lack a verb, but we know that “O for a …” is a poetical way of expressing a wish for something on the part of the speaker, so we can paraphrase it fairly accurately by replacing “O” by “I wish”, and the sentence thus revised passes the test. In the second case we have only to delete the word “that”, whose presence serves only for emphasis (and scansion, of course!), and alter the punctuation slightly: “It is true that ‘to be or not to be?’ is the question.” Now, a statement is a sentence that is declarative in form: it declares something that is supposed to be true. Example 3.3, “Student StudentId is enrolled on course CourseId”, is not a statement it does not pass the test. It does, however, have the grammatical form of a statement. We can say that, like a statement, it is declarative in form. ﻿Headers and Footers can be switched on or off here. I use headers to display the document title. In the Footer I place my ©copyright notice, page number, page count, date and so on. Footnote and endnote preferences can be set here. If you don’t know what these are you probably do not need them, unless you are a student, then you will need to find out about this. Whilst eluding to academic practice Pages supports the Thomson Reuters EndNote and Design Science’s MathsType plug–ins. Hyphenation and ligatures are typographic terms. For left aligned type there is no need to check the Hyphenation button. Though I recommend using ligatures. The bottom section is Require Password To Open. For additional security a password can be entered to lock a Pages document. The password would then be required every-time the document is opened. The next tab is TOC, or Table Of Contents. There is practical exercise on using Table Of Contents in the Sharing Your Work chapter. Table Of Contents only makes sense when Style Sheets are properly understood. We’ll be looking at Style Sheets later. The last tab is Info. Completing the fields for Author, Title, Keywords, and Comments is recommended practice. It makes documents easier to search for, especially if using the Mac’s Spotlight search. All of these fields are for metadata. Author and Title are self-explanatory. Adding Keywords helps classify a document, for example I wrote this Bookboon book using Pages. So this Pages document could have the keywords, ‘Apple’, ‘Pages’, ‘Bookboon’, ‘software training’ and ‘free download version’. The list could go on. There are five keywords here. Keywords are denoted by commas, ‘Apple, Pages, Bookboon’. Also note that ‘software training’ and ‘free download version’ are both single keywords. Searches can be case-sensitive, but using ‘apple’ and ‘Apple’ as keywords is not strictly necessary. When choosing keywords it is helpful to invoke the spirit of the librarian and embrace formal cataloguing techniques. A less rigorous form of applying metadata is using the Comments field. Here paragraphs of descriptive text can be added. In the example of this book, the comments field could read, ‘Bookboon software training handbook for Apple Pages part of iWork.’ The next part of Info displays statistics for the current Pages document; the word count, amount of pages et cetera. The Range menu only becomes active is text is selected. When Range is greyed out you are reviewing statistics for the entire document. Document statistics are also available at the bottom of the document window. Next to this word count the current page number and total pages is listed. Click on this text and type a page number to jump to that page. • Layout Inspector With two tabs, Layout and Section, the Layout Inspector requires an understanding of page design and long–document management to fully understand its relevance. Layout has a Column control. The Format Bar also has a Column menu, but the Inspector allows you to create columns of differing widths. Columns can be inset to give you some of the control graphic designers require from desktop publishing’s heavy–hitters, QuarkXPress and Adobe InDesign. The Section tab is useful for long–document management. If this were a larger book I would have split the chapters into separate sections and from the Configuration options set Section starts on: Right Page. Page Numbers can be configured so that a book’s preface uses a different numbering sequence than the main body of the book. To repeat an earlier point, this Inspector will mean more to those with page design knowledge, or those that need to manage longer documents. • Wrap Inspector The Wrap Inspector controls how text interacts with graphic objects, including photographs. For most documents the Object Placement should be set to Inline (moves with text). This causes in graphics to sit in the body of the text. If text is added or deleted the graphic will move with the change; keeping its relative position in the text. Inline (moves with text) automatically wraps text, so there is no need to adjust the Object causes wrap parameters. If you chose Object Placement, Floating (doesn’t move with text) text can be obscured by the object. As the name suggests Floating lets you position the object anywhere on the page, but it will not keep its relative position in the text; the object remains where you placed it. To get text to wrap select the object and chose one of the wrap options. The wrap button icons are clues to what they do. Text Fit is a little more esoteric. Look at the two buttons for Text Fit. The button on the left shows a triangle in the middle of some text (that is what the lines indicate). The text wraps to a square, because the imported object sit on a square canvas. Text will wrap to the boundaries of the imported object and not the image shape on that object. Imported objects can have invisible borders applied, known as Alpha Channels. Pages can access Alpha Channel information to draw irregular text wrap borders, see the green star text wrap example. The second Text Fit button shows the Text Wrap conforming to the triangle shape. Alpha channels can be applied to pictures from within Pages. Extra Space forces text to wrap further away from the wrap object. With wraps made on an alpha channel, the Alpha parameter changes the wrap based on the transparency of the alpha object. Rarely would you need to change the Alpha Text Fit value. • Text Inspector – Text can be controlled from five locations: • Menu Bar > Fonts • Menu Bar > Text • The Fonts window, launched from the Tool Bar or using Command T • The Format Bar • Or in the Text Inspector. Each location has some unique and some shared functions. The Text Inspector has four tabs, Text, List Tabs and More. Text is probably the most readily understood, followed by List. Although many people use Tabs, or Tabulation. I have found that many people only vaguely understand the underpinning concept of tabulation, therefore I will explain some tabulation basics later. So, back to the Text tab. This can be used for Character alignment horizontally, and also vertically. The colour of type can be changed too. Similar controls can be found in the Format Bar. The unique parameters here are the Spacing controls. Character Spacing, also known as Tracking, adjusts the space between letters, whereas Line Spacing, also known as Leading, adjusts the height of lines of type.$$$﻿Headers and Footers can be switched on or off here. I use headers to display the document title. In the Footer I place my ©copyright notice, page number, page count, date and so on. Footnote and endnote preferences can be set here. If you don’t know what these are you probably do not need them, unless you are a student, then you will need to find out about this. Whilst eluding to academic practice Pages supports the Thomson Reuters EndNote and Design Science’s MathsType plug–ins. Hyphenation and ligatures are typographic terms. For left aligned type there is no need to check the Hyphenation button. Though I recommend using ligatures. The bottom section is Require Password To Open. For additional security a password can be entered to lock a Pages document. The password would then be required every-time the document is opened. The next tab is TOC, or Table Of Contents. There is practical exercise on using Table Of Contents in the Sharing Your Work chapter. Table Of Contents only makes sense when Style Sheets are properly understood. We’ll be looking at Style Sheets later. The last tab is Info. Completing the fields for Author, Title, Keywords, and Comments is recommended practice. It makes documents easier to search for, especially if using the Mac’s Spotlight search. All of these fields are for metadata. Author and Title are self-explanatory. Adding Keywords helps classify a document, for example I wrote this Bookboon book using Pages. So this Pages document could have the keywords, ‘Apple’, ‘Pages’, ‘Bookboon’, ‘software training’ and ‘free download version’. The list could go on. There are five keywords here. Keywords are denoted by commas, ‘Apple, Pages, Bookboon’. Also note that ‘software training’ and ‘free download version’ are both single keywords. Searches can be case-sensitive, but using ‘apple’ and ‘Apple’ as keywords is not strictly necessary. When choosing keywords it is helpful to invoke the spirit of the librarian and embrace formal cataloguing techniques. A less rigorous form of applying metadata is using the Comments field. Here paragraphs of descriptive text can be added. In the example of this book, the comments field could read, ‘Bookboon software training handbook for Apple Pages part of iWork.’ The next part of Info displays statistics for the current Pages document; the word count, amount of pages et cetera. The Range menu only becomes active is text is selected. When Range is greyed out you are reviewing statistics for the entire document. Document statistics are also available at the bottom of the document window. Next to this word count the current page number and total pages is listed. Click on this text and type a page number to jump to that page. • Layout Inspector With two tabs, Layout and Section, the Layout Inspector requires an understanding of page design and long–document management to fully understand its relevance. Layout has a Column control. The Format Bar also has a Column menu, but the Inspector allows you to create columns of differing widths. Columns can be inset to give you some of the control graphic designers require from desktop publishing’s heavy–hitters, QuarkXPress and Adobe InDesign. The Section tab is useful for long–document management. If this were a larger book I would have split the chapters into separate sections and from the Configuration options set Section starts on: Right Page. Page Numbers can be configured so that a book’s preface uses a different numbering sequence than the main body of the book. To repeat an earlier point, this Inspector will mean more to those with page design knowledge, or those that need to manage longer documents. • Wrap Inspector The Wrap Inspector controls how text interacts with graphic objects, including photographs. For most documents the Object Placement should be set to Inline (moves with text). This causes in graphics to sit in the body of the text. If text is added or deleted the graphic will move with the change; keeping its relative position in the text. Inline (moves with text) automatically wraps text, so there is no need to adjust the Object causes wrap parameters. If you chose Object Placement, Floating (doesn’t move with text) text can be obscured by the object. As the name suggests Floating lets you position the object anywhere on the page, but it will not keep its relative position in the text; the object remains where you placed it. To get text to wrap select the object and chose one of the wrap options. The wrap button icons are clues to what they do. Text Fit is a little more esoteric. Look at the two buttons for Text Fit. The button on the left shows a triangle in the middle of some text (that is what the lines indicate). The text wraps to a square, because the imported object sit on a square canvas. Text will wrap to the boundaries of the imported object and not the image shape on that object. Imported objects can have invisible borders applied, known as Alpha Channels. Pages can access Alpha Channel information to draw irregular text wrap borders, see the green star text wrap example. The second Text Fit button shows the Text Wrap conforming to the triangle shape. Alpha channels can be applied to pictures from within Pages. Extra Space forces text to wrap further away from the wrap object. With wraps made on an alpha channel, the Alpha parameter changes the wrap based on the transparency of the alpha object. Rarely would you need to change the Alpha Text Fit value. • Text Inspector – Text can be controlled from five locations: • Menu Bar > Fonts • Menu Bar > Text • The Fonts window, launched from the Tool Bar or using Command T • The Format Bar • Or in the Text Inspector. Each location has some unique and some shared functions. The Text Inspector has four tabs, Text, List Tabs and More. Text is probably the most readily understood, followed by List. Although many people use Tabs, or Tabulation. I have found that many people only vaguely understand the underpinning concept of tabulation, therefore I will explain some tabulation basics later. So, back to the Text tab. This can be used for Character alignment horizontally, and also vertically. The colour of type can be changed too. Similar controls can be found in the Format Bar. The unique parameters here are the Spacing controls. Character Spacing, also known as Tracking, adjusts the space between letters, whereas Line Spacing, also known as Leading, adjusts the height of lines of type.
EN35	1	﻿Since its creation in 1987 Perl has become one of the most widely used programming languages. One measure of this is the frequency with which various languages are mentioned in job adverts. The site www.indeed.com monitors trends: in 2010 it shows that the only languages receiving more mentions on job sites are C and its offshoots C++ and C#, Java, and JavaScript. Perl is a general-purpose programming language, but it has outstanding strengths in processing text files: often one can easily achieve in a line or two of Perl code some text-processing task that might take half a page of C or Java. In consequence, Perl is heavily used for computer-centre system admin, and for Web development – Web pages are HTML text files. Another factor in the popularity of Perl is simply that many programmers find it fun to work with. Compared with Perl, other leading languages can feel worthy but tedious. Perl is a language in which it is easy to get started, but – because it offers handy ways to do very many different things – it takes a long time before anyone finishes learning Perl (if they do ever finish). One standard reference, Steven Holzner’s Perl Black Book (second edn, Paraglyph Press, 2001) is about 1300 dense pages long. So, for the beginner, it is important to focus on the core of the language, and avoid being distracted by all the other features which are there, but are not essential in the early stages. This book helps the reader to do that. It covers everything he or she needs to know in order to write successful Perl programs and grow in confidence with the language, while shielding him or her from confusing inessentials.1 Later chapters contain pointers towards various topics which have deliberately been omitted here. When the core of the language has been thoroughly mastered, that will be soon enough to begin broadening one’s knowledge. Many productive Perl programmers have gaps in their awareness of the full range of language features. The book is intended for beginners: readers who are new to Perl, and probably new to computer programming. The book takes care to spell out concepts that would be very familiar to anyone who already has experience of programming in some other language. However, there will be readers who use this book to begin learning Perl, but who have worked with another language in the past. For the benefit of that group, I include occasional brief passages drawing attention to features of Perl that could be confusing to someone with a background in another language. Programming neophytes can skim over those passages.  The reader I had in mind as I was writing this book was a reader much like myself: someone who is not particularly interested in the fine points of programming languages for their own sake, but who wants to use a programming language because he has work he wants to get done, and programming is a necessary step towards doing it. As it happens, I am a linguist by training, and much of my own working life is spent studying patterns in the way the English language is used in everyday talk. For this I need to write software to analyse files of transcribed tape-recordings, and Perl is a very suitable language to use for this. Often I am well aware that the program I have written is not the most elegant possible solution to some task at hand, but so long as it works correctly I really don’t care. If some geeky type offered to show me how I could eliminate several lines of code, or make my program run twice as fast, by exploiting some little-known feature of the language which would yield a program delivering exactly the same results, I would not be very interested. Too many computing books are written by geeks who lose sight of the fact that, for the rest of us, computers are tools to get work done rather than ends in themselves. Making programs short is good if it makes them easier to grasp and hence easier to get right; but if brevity is achieved at the cost of obscurity, it is bad. As for speed: computer programs run so fast that, for most of us, speeding them up further would be pointless. (For every second of time my programs take to run, I probably spend a day thinking about the results they produce.) That does not mean that, in writing this book, I would have been justified in focusing only on those particular elements of Perl which happen to be useful in my own work and ignoring the rest – certainly not. Readers will have their own tasks for which they want to write software, which will often be very different from my tasks and will sometimes make heavy use of aspects of Perl that I rarely exploit. I aim to cover those aspects, as well as the ones which I use frequently. But it does mean that the book is oriented towards Perl programming as a practical tool – rather than as a labyrinth of fascinating intellectual arcana. If, after working through this book, you decide to make serious use of Perl, sooner or later you will need to consult some larger-scale Perl book – one organized more as a reference manual than a teaching introduction. This short book cannot pretend to cover the reference function, but there is a wide choice of books which do. (And of course there are plenty of online reference sources.) Many Perl users will not need to go all the way to Steven Holzner’s 1300-pager quoted above. The manual which I use constantly is a shorter one by the same author, Perl Core Language Little Black Book (second edn, Paraglyph Press, 2004) – I find Holzner’s approach particularly well suited to my own style of learning, but readers whose learning styles differ might find that other titles suit them better. Because the present book deliberately limits the aspects of Perl which it covers, it is important that readers should not fall into the trap of thinking “Doesn’t Perl have a such-and-such function, then? – that sounds like an awkward gap to have to work round”. Whatever such-and-such may be, very likely Perl has got it, but it is one of the things which this book has chosen not to cover. ﻿For the purposes of this textbook, I shall assume that you have access to a computer system on which Perl is available, and that you know how to log on to the system and get to a point where the system is displaying a prompt and inviting you to enter a command. Perl is free, and versions are available for all the usual operating systems, so if you are working in a multi-user environment such as a university computer centre then Perl is almost sure to be on your system already. (It would take us too far out of our way to go through the details of installing Perl on a home computer which does not already have it; though, if the home computer is a Mac running OS X, it will already have Perl – available from the Terminal utility under Applications  Utilities.) Assuming, then, that you have access to Perl, let us get started by creating and running a very simple program.2 Adding two and two is perhaps as simple as it gets. This could be a very short Perl program indeed, but I’ll offer a slightly longer one which illustrates some basics of the language. First, create a file with the following contents. Use a text editor to create it, not a word-processing application such as Word – files created via WP apps contain a lot of extra, hidden material apart from the wording typed by the user and displayed on the screen, but we need a file containing just the characters shown below and no others.  Save it under some suitable name – twoandtwo.pl is as good a name as any. The .pl extension is optional – Perl itself does not care about the format of filenames, and it would respond to the program just the same if you called it simply twoandtwo – but some operating systems want to see filename extensions in some circumstances, so it is probably sensible to get in the habit of including .pl in the names of your Perl programs. Your twoandtwo.pl file will contain just what is shown above. But later in this book, when we look at more extended examples of Perl code I shall give them a label in brackets and number the lines, like this:  These labels will be purely for convenience in discussing the code, for instance I shall write “line 1.3” to identify the line print $b. The labels are not part of what you will type to create a program. However, when your programs grow longer you may find it helpful to create them using an editor which shows line-numbers; the error messages generated by the Perl interpreter will use line numbers to identify places where it finds problems.  In (1), the symbols $a and $b are variables – names for pigeonholes containing values (in this case, numbers). Line 1.1 means “assign the value 2 to the variable $a”. Line 1.2 means “assign the result of adding the value of $a to itself to the variable $b”. Line 1.3 means “display the value of $b”. Note that each instruction (the usual word is statement) ends in a semicolon. To run the program, enter the command  to which the system will respond (I’ll show system responses in italics) with  Actually, if your system prompt is, say, %, what you see will be  – since nothing in the twoandtwo.pl program has told the system to output a newline after displaying the result and before displaying the next prompt. For that matter, nothing in our little program has told the system how much precision to include in displaying the answer; rather than responding with 4, some systems might respond with 4.00000000000000 (which is a more precise way of saying the same thing). In due course we shall see how to include extra material in a program to deal with issues like these. For now, the point is that the job in hand has been correctly done.  If you have typed the code exactly as shown and Perl does not respond correctly (or at all) when you try running it, various system-dependent problems may be to blame. I assume that, where you are working, there will be someone responsible for telling you what is needed to run Perl on your local system. But meanwhile, I can offer two suggestions. It may be that your program needs to tell the system where the Perl interpreter is located (this is likely if you are seeing an error message suggesting that the command perl is not recognized). In that case it is worth trying the following. Include as the first line of your program this “magic line”:3  This will not be the right “magic line” for every system, but for many systems it will be. Secondly, if Perl appears to run without generating error messages, but outputs no result, or outputs material suggesting that it stopped reading your program before the end, it may be that your editor is supplying the wrong newline symbols – so that the sequence of lines looks to the system like one long line. That will often lead to problems; for instance, if the first line of your program is the above “magic line”, but Perl sees your whole program as one long line, then nothing will happen when you run it, because the Perl interpreter will only begin to operate on the line following the “magic line”. Set your editor to use Unix (decimal 10) newlines. If neither of these solutions works, then, sorry, you really will need to find that computer-support staff member to tell you how to run Perl on the particular system you are working at!  Let’s now go back to the contents of program (1). One point which may have surprised you about our first program is the dollar signs in the variable names $a and $b. Why not simply name our variables a and b? In many programming languages, these latter names would be fine, but in Perl they are not. One of the rules of Perl is that any variable name must begin with a special character identifying what kind of entity it is, and for individual variables – names for single separate pigeonholes, as opposed to names for whole sets of pigeonholes – the identifying character is a dollar sign. ﻿Programming, in any language, involves creating named entities within the machine and manipulating them – using their values to calculate the value for a new entity, changing the values of existing entities, and so forth. Some languages recognize many different kinds of entity, and require the programmer to be very explicit and meticulous about “declaring” what entities he will use and what kind each one will be before anything is actually done with them.4 In C, for instance, if a variable represents a number, one must say what kind of number – whether an integer (a whole number) or a “floating-point number” (what in everyday life we call a decimal), and if the latter then to what degree of precision it is recorded. (Mathematically, a decimal may have any number of digits after the decimal point, but computers have to use approximations which round numbers off after some specific number of digits.) Perl is very free and easy about these things. It recognizes essentially just three types of entity: individual items, and two kinds of sets of items – arrays, and hashes. Individual entities are called scalars (for mathematical reasons which we can afford to ignore here – just think of “scalar” as Perl-ese for an individual data item); a scalar can have any kind of value – it can be a whole number, a decimal, a single character, a string of characters (for instance, an English word or sentence) … We have already seen that variable names representing scalars (the only variables we shall be considering for the time being) begin with the $ symbol; for arrays and hashes, which we shall discuss in chapters 12 and 17, the corresponding symbols are @ and % respectively.  Furthermore, Perl does not require us to declare entity names before using them. In the mini-program (1), the scalars $a and $b came into existence when they were assigned values; we gave no prior notice that these variable names were going to be used. In program (1), the variable $b ended up with the value 4. But, if we had added a further line:  then $b would have ceased to stand for a number and begun to stand for a character-string – both are scalars, so Perl is perfectly willing to switch between these different kinds of value. That does not mean that it is a good idea to do this in practice; as a programmer you will need to bear in mind what your different variable names are intended to represent, which might be hard to do if some of them switch between numerical and alphabetic values. But the fact that one can do this makes the point that Perl does not force us to be finicky about housekeeping details. Indeed, it is even legal to use a variable’s value before we have given it a value. If line 1.2 of (1) were changed to $b = $a + $c, then $b would be given the sum of 2 plus the previously-unmentioned scalar $c. Because $c has not been given a value by the programmer, its value will be taken as zero (so $b will end up with the value 2). Relying on Perl to initialize our variables in this way is definitely a bad idea – even if we need a particular variable to have the initial value zero, it is much less confusing in the long run to get into the habit of always saying so explicitly. But Perl will not force us to give our variables values before we use them. Because this free-and-easy programming ethos makes it tempting to fall into bad habits, Perl gives us a way of reminding ourselves to avoid them. We ran program (1) with the command:  The perl command can be modified by various options beginning with hyphens, one of which is -w for “give warnings”. If we ran the program using the command:  then, when Perl encounters the line $b = $a + $c in which $c is used without having been assigned a value, it will obey the instruction but will also print out a warning:  If a skilled programmer gets that warning, it is very likely to be because he thinks he has given $c a value but in fact has omitted to do so. And perl -w gives other warnings about things in our code which, while legal, might well be symptoms of programming errors. It is a good idea routinely to use perl -w to run your programs, and to modify the programs in response to warning messages until the warnings no longer appear – even if the programs seem to be giving the right results.  In program (1) we saw the operator +, which as you would expect takes a pair of numerical values and gives their sum. Likewise - is used as a minus sign. Some further operators (not a complete list, but the ones you are most likely to need) include: * multiplication / division ** exponentiation: 2 ** 3 means 23, i.e. eight These operators apply to numerical values, but others apply to character-strings. Notably, the full stop . represents concatenation (making one string out of two):  (Beware of possible confusion here. Some programming languages make the plus sign do double duty, to represent concatenation of strings as well as addition of numbers, but in Perl the plus sign is used only for numerical values.) Another string operator is x (the letter x), which is used to concatenate a string with itself a given number of times: "a" x 6 is equivalent to "aaaaaa", "pom" x 3 is equivalent to "pompompom". (And "pom" x 0 would yield the empty string – the length-zero string containing no characters – which is more straightforwardly specified as "".) Note, by the way, that for Perl a single character is just a string of length one – there is no difference, as there is for instance in C, between "a" and 'a', these are equivalent ways of representing the length-one string containing just the character a. However, single and double quotation marks are not always equivalent. Perl uses backslash as an escape character to create codes for string elements which would be awkward to type: for instance, \n represents a newline character, and \t a tab. Between double quotation marks these sequences are interpreted as codes: ﻿We have seen the word if used to control which instruction is executed next. Commonly, we want to do one thing in one case and another thing in a different case. An if can be followed by an elsif (or more than one elsif), with an else at the end to catch any remaining possibilities:  When any one of the tests is passed, the remaining tests are ignored; if $price is 200, then since 200  100 Perl will print It's expensive, and the message in 4.7 will not be printed even though it is also true that 200 > 0. Curly brackets are used to keep together the block of code to be executed if a test is passed. Notice that (unlike in some programming languages) even if the block contains just a single line of code, that line must still have curly brackets round it. The last statement before the } does not actually have to end in a semicolon, but it is sensible to include one anyway. We might want to modify our code by adding further statements, in which case it would be easy to overlook the need to add a missing semicolon.  Not everyone sets out the curly brackets on separate lines, as I did in (4) above. Within reason, Perl does not care where in a program we put whitespace (spaces, tabs, and newline characters). Obviously we cannot put a space in the middle of a number – 56237 cannot be written 56 237, or Perl would have no way to tell that it was all one number 8 – and likewise putting a space in the middle of a string within quotation marks turns it into a different string. But we can set the program out on the page however we please: around the basic elements such as numbers, strings, variable names, and brackets of different types, Perl will ignore extra whitespace. Perl will even supply implied spacing in many cases where elements are run together – thus ++ $a can alternatively be written ++$a. Because Perl does not enforce layout conventions (as some languages do), you need to choose some system and use it consistently – so that you can grasp the overall structure of your program listings at a glance. The main question is about how to indent blocks; different people use different conventions. First, you need to decide how much space you are going to use for one level of indentation (common choices are one tab, or two spaces). But then, where exactly should the indents go? Perl manuals often put the opening curly bracket on the line which introduces it, indent the contents of the block, and then place the closing curly bracket level with the beginning of that first line:  This takes fewer lines than other conventions, but it is not particularly easy to read, and it is perhaps illogical in placing the pair of brackets at unrelated positions. Alternatively, one can give both curly brackets lines of their own – in which case they either both line up under the start of the introducing line, or are both indented to align with their contents:  Whichever convention you choose, if you apply it consistently you can catch and correct programming errors as you type. You may have a block which is indented within a block that is itself indented within a top-level block. When you type what you thought was the final }, if it doesn’t align properly with the item which it ought to line up with in the first line, then something has gone wrong – perhaps one of your opening brackets has not been given a closing partner?  As for which of the three styles you choose, that is entirely up to you. According to Thomas Plum, a survey of programmers working with the similar language C found a slight majority favouring the last of the three conventions.9 That is the style used in this book. Indenting consistently also has an advantage when, inevitably, one’s program as first written turns out not to run correctly. A common debugging technique is to insert instructions to print out the values of particular variables at key points, so that one can check whether their values are as expected. Once the bugs are found and eliminated, we naturally want to eliminate these diagnostic lines too – we don’t want our program spewing out a lot of irrelevancies when it is running correctly. My practice is to write diagnostic lines unindented, so that they stand out visually in the middle of an indented block, making them easy to locate and delete.  The reason to adopt a consistent style for program layout is to make it easier for a human programmer to understand what is going on within a sea of program code – the computer itself does not care about the layout. Another aid to human understanding is comments: explanatory notes written by the programmer to himself (or to those who come after him and have to maintain his code) which the machine ignores. In Perl, comments begin with the hash character. A comment can be:  or it can be added to a line to the right of code intended for the computer:  Either way, everything from the hash symbol to the end of the line is ignored by the machine.  Earlier, we saw that Perl has various “operators” represented by mathematical-type symbols. Sometimes these are the same symbols used in familiar school maths, such as + for addition and - for subtraction; sometimes they are slightly different symbols adapted to the constraints of computer keyboards, such as * for multiplication and ** for raising to a power; and sometimes the symbols represent operations that we do not usually come across in maths lessons, e.g. “.” for concatenation.  Perl has many more built-in functions that could conveniently be represented by special symbols, though.10 Most are represented by alphabetic codes. For instance, taking the square root of a number is a standard arithmetic operation, but the usual mathematical symbol, √, is nothing like any character in the ASCII character-set, so instead Perl represents it as sqrt. ﻿Sometimes we want to repeat an action, perhaps with variations. One way to do this is with the word for. Suppose we want to print out a hundred lines containing the messages:  Here is a code snippet which does that:  The brackets following for contain: a variable created for the purpose of this for loop and given an initial value; a condition for repeating the loop; and an action to be executed after each pass. The variable $i begins with the value 1, ++$i increments it by one on each pass, and the instruction within the curly brackets is executed for each value of $i until $i reaches 101, when control moves on to whatever follows the closing curly bracket. We saw earlier that, within double quotation marks, a symbol like \n is translated into what it stands for (newline, in this case), rather than being taken literally as the two characters \ followed by n. Similarly, a variable name such as $i is translated into its current value; the lines displayed by the code above read e.g. Next number is 3, not Next number is $i. If you really wanted the latter, you would need to “escape” the dollar sign:  The little examples in earlier chapters often ended with statements such as  In practice, it would usually be far preferable to write  so that the result appears on a line of its own, rather than jammed together with the next system prompt. Within the output of the above code snippet, 1 is not a “next” number but the first number. So we might want the message on the first line to read differently. By now, we know various ways to achieve that. Here are two – a straightforward, plodding way, and a more concise way:  or (quicker to type, though less clear when you come back to it weeks later):  Another way to set up a repeating loop is the while construction. Here is another code snippet which achieves the same as the two we have just looked at:  Here, $i is incremented within the loop body, and control falls out of the loop after the pass in which $i begins with the value 99. The while condition reads $i < 100, not $i <= 100: within the curly brackets, $i is incremented before its value is displayed, so if <= had been used in the while line, the lines displayed would have reached 101. The while construction is often used for reading input lines in from a text file, so the next chapter will show us how that is done.  In general, a file you want to get data into your program from will not necessarily be in the same directory as the program itself; it may have to be located by a pathname which could be long and complicated. The structure of pathnames differs between operating systems; if you are working in a Unix environment, for instance, the pathname might be something like:  Whatever pathnames look like in your computing environment, to read data into a Perl program you have to begin by defining a convenient handle which the program will use to stand for that pathname. For instance, if your program will be using only one input file, you might choose the handle INFILE (it is usual to use capitals for filehandles). The code:  says that, from now until we hit a line close(INFILE), any reference to INFILE in the program will be reading in data from the annualRecords file specified in the pathname.  Having “opened” a file for input, we use the symbol <> to actually read a line in. Thus:  will read in a line from the annualRecords file and assign that string of characters as the value of $a. A line from a multi-line file will terminate in one or more line-end characters, and the identity of these may depend on the system which created the file (different operating systems use different line-end characters). Commonly, before doing anything else with the line we will want to convert it into an ordinary string by removing the line-end characters, and the built-in function chomp() does that. This is an example of a function whose main purpose is to change its argument rather than to return a value; chomp() does in fact return a value, namely the number of line-end characters found and removed, but programs will often ignore that value – they will say e.g. chomp($line), rather than saying e.g. $n = chomp($line), with follow-up code using the value of $n. (If no filehandle is specified, $a = <> will read in from the keyboard – the program will wait for the user to type a sequence of characters ending in a newline, and will assign that sequence to $a.11) Assuming that we are reading data from a file rather than from the keyboard, what we often want to do is to read in the whole of the input file, line by line, doing something or other with each successive line. An easy way to achieve that is like this:  The word while tests for the truth of a condition; in this case, it tests whether the assignment statement, and hence the expression <INFILE>, is true or false. So long as lines are being read in from the input file, <INFILE> counts as “true”, but when the file is exhausted <INFILE> will give the value “false”. Hence while ($a = <INFILE>) assigns each line of the input file in turn to $a, and ceases reading when there is nothing more to read. (It is a good idea then to include an explicit close(INFILE) statement, though that is not strictly necessary.) Our open … statement assumed that the annualRecords file was waiting ready to be opened at the place identified by the pathname. But, of course, that kind of assumption is liable to be confounded! Even supposing we copied the pathname accurately when we typed out the program, if that was a while ago then perhaps the annualRecords file has subsequently been moved, or even deleted. In practice it is virtually mandatory, whenever we try to open a file, to provide for the possibility that it does not get opened – normally, by using a die statement, which causes the program to terminate after printing a message about the problem encountered. A good way to code the open statement will be: ﻿English law distinguishes two kinds of defamation: slander (in speech) and libel (in writing); because writing is permanent, libel is treated as being more seriously damaging than slander. Emails and the like are often composed as casually and carelessly as spoken remarks, but they can be preserved indefinitely and so the applicable law is libel law. English libel law is strict: compared to other countries, it is easy for someone who feels damaged by the written word to win a case against whoever is responsible, and awards for loss of reputation have traditionally been large (though recent changes have moderated that to some extent). As business first used the Web, libel law was scarcely relevant. Commercial websites were concerned with promoting their own businesses, not normally with knocking their competitors. But the Web is coming to be used in new ways. We have heard a great deal recently about “Web 2.0”. This is a vague, hype-laden piece of terminology, but one thing it commonly refers to is the idea that websites – including commercial websites – are ceasing to be outlets for one-way communication exclusively, and turning into two-way, conversational affairs, where for instance a company will draw its customers and other interested parties into participation via blogs, chatrooms, and similar mechanisms. There are several business reasons why the “virtual communities” fostered by interactive websites are potentially beneficial for a company. However, if members of the public are encouraged to post material on a company website, the legal danger is that some individuals’ postings might include defamatory remarks about third parties. We know that electronic communication tends to encourage a kind of “flaming” that is rare in other media. For the firm owning the website, it would be regrettable enough to find one of its customers having to defend a lawsuit as a consequence of contributing to a blog which that firm had set up. Even worse would be the possibility of itself defending a defamation suit, if it is held responsible for others’ contributions to its site. A plaintiff who hopes for a large damages award will be more interested in going after the firm than the individual; the firm is more likely to be able to pay. So the question arises what legal responsibility a website owner has for material posted by others. Questions like this arose before Web 2.0 days, in connexion with ISPs and operators of bulletin boards. One way that lawyers think about the issue is to compare that kind of electronic communication infrastructure with the world of newspapers and magazines, and to ask whether the organizations are more like distributors (such as newsagents) or publishers. If a newspaper contains a libellous article, the newsagents who sell the paper to readers would not normally be held liable – they have no control over what appears in the paper and may not even be aware of it; but the newspaper publisher has editorial control over what its journalists write, so will routinely be treated as equally responsible with them for any libel. In the case of electronic bulletin boards, some are moderated and others not. Ironically, although providing moderation would normally be seen as the responsible thing for a bulletin board operator to do, legally it might be a rather dangerous thing to do: it implies taking on a role more like publisher than distributor. In the USA (although libel law is far milder there) this situation was seen as creating such risks for organizations which undertake the socially-valuable task of promoting electronic communication that the risks were eliminated by statute (section 230 of the Telecommunications Act 1996). This broadly says that service providers are not to be held responsible for content posted by others, and that no liability arises from the moderating role. Without a blanket exemption such as American law provides, a website run by a commercial firm would be more likely to be held responsible for its contents than some bulletin board run by amateur enthusiasts – the site contributes to business profits, so there would be little excuse for not taking the trouble to moderate it. English law contains nothing parallel to §230 of the US Telecommunications Act. Our Defamation Act 1996 provides that no-one is liable for the contents of electronic communications if they act purely as unwitting distributors, but if they act as “publishers” they are liable; a commercial website owner, like a newspaper publisher, would have a duty to take reasonable care about what it publishes. Even an ISP, with no commercial interest of its own in the contents of material it hosts, will probably not escape liability under the 1996 Act if it has been told about defamatory material on its servers (so that it can no longer claim to be an unwitting distributor). Consider Godfrey v. Demon Internet Ltd (1999). Dr Godfrey was a British computer science lecturer who allegedly made a hobby of starting online flame wars and then bringing libel actions when people responded to his flames by being nasty about him. In 1997 he faxed the MD of the leading British ISP Demon demanding the removal of a scurrilous newsgroup posting which had come in from the USA. Demon routinely deleted newsgroup postings after a fortnight, so the issue concerned only the ten days between Godfrey’s fax and the normal deletion date; during that period, Demon failed to act (apparently the fax never reached the MD’s desk). In view of this delay, the court found in preliminary hearings in 1999 that Demon could not satisfy the requirement about taking reasonable care – at which point Demon threw in the towel and settled out of court, paying Godfrey about a quarter of a million pounds. Although Godfrey v. Demon set no formal legal precedent (because it was settled rather than fought out to a conclusion), the terms on which it was settled sent a thrill of fear through the industry. It seems that (unless an ISP is prepared to investigate and satisfy itself that a complaint is legally unfounded, which would often be difficult or impossible for it to achieve), its only safe response to any complaint will be automatically to take down the material complained about. This is what British ISPs have been tending to do.$$$﻿English law distinguishes two kinds of defamation: slander (in speech) and libel (in writing); because writing is permanent, libel is treated as being more seriously damaging than slander. Emails and the like are often composed as casually and carelessly as spoken remarks, but they can be preserved indefinitely and so the applicable law is libel law. English libel law is strict: compared to other countries, it is easy for someone who feels damaged by the written word to win a case against whoever is responsible, and awards for loss of reputation have traditionally been large (though recent changes have moderated that to some extent). As business first used the Web, libel law was scarcely relevant. Commercial websites were concerned with promoting their own businesses, not normally with knocking their competitors. But the Web is coming to be used in new ways. We have heard a great deal recently about “Web 2.0”. This is a vague, hype-laden piece of terminology, but one thing it commonly refers to is the idea that websites – including commercial websites – are ceasing to be outlets for one-way communication exclusively, and turning into two-way, conversational affairs, where for instance a company will draw its customers and other interested parties into participation via blogs, chatrooms, and similar mechanisms. There are several business reasons why the “virtual communities” fostered by interactive websites are potentially beneficial for a company. However, if members of the public are encouraged to post material on a company website, the legal danger is that some individuals’ postings might include defamatory remarks about third parties. We know that electronic communication tends to encourage a kind of “flaming” that is rare in other media. For the firm owning the website, it would be regrettable enough to find one of its customers having to defend a lawsuit as a consequence of contributing to a blog which that firm had set up. Even worse would be the possibility of itself defending a defamation suit, if it is held responsible for others’ contributions to its site. A plaintiff who hopes for a large damages award will be more interested in going after the firm than the individual; the firm is more likely to be able to pay. So the question arises what legal responsibility a website owner has for material posted by others. Questions like this arose before Web 2.0 days, in connexion with ISPs and operators of bulletin boards. One way that lawyers think about the issue is to compare that kind of electronic communication infrastructure with the world of newspapers and magazines, and to ask whether the organizations are more like distributors (such as newsagents) or publishers. If a newspaper contains a libellous article, the newsagents who sell the paper to readers would not normally be held liable – they have no control over what appears in the paper and may not even be aware of it; but the newspaper publisher has editorial control over what its journalists write, so will routinely be treated as equally responsible with them for any libel. In the case of electronic bulletin boards, some are moderated and others not. Ironically, although providing moderation would normally be seen as the responsible thing for a bulletin board operator to do, legally it might be a rather dangerous thing to do: it implies taking on a role more like publisher than distributor. In the USA (although libel law is far milder there) this situation was seen as creating such risks for organizations which undertake the socially-valuable task of promoting electronic communication that the risks were eliminated by statute (section 230 of the Telecommunications Act 1996). This broadly says that service providers are not to be held responsible for content posted by others, and that no liability arises from the moderating role. Without a blanket exemption such as American law provides, a website run by a commercial firm would be more likely to be held responsible for its contents than some bulletin board run by amateur enthusiasts – the site contributes to business profits, so there would be little excuse for not taking the trouble to moderate it. English law contains nothing parallel to §230 of the US Telecommunications Act. Our Defamation Act 1996 provides that no-one is liable for the contents of electronic communications if they act purely as unwitting distributors, but if they act as “publishers” they are liable; a commercial website owner, like a newspaper publisher, would have a duty to take reasonable care about what it publishes. Even an ISP, with no commercial interest of its own in the contents of material it hosts, will probably not escape liability under the 1996 Act if it has been told about defamatory material on its servers (so that it can no longer claim to be an unwitting distributor). Consider Godfrey v. Demon Internet Ltd (1999). Dr Godfrey was a British computer science lecturer who allegedly made a hobby of starting online flame wars and then bringing libel actions when people responded to his flames by being nasty about him. In 1997 he faxed the MD of the leading British ISP Demon demanding the removal of a scurrilous newsgroup posting which had come in from the USA. Demon routinely deleted newsgroup postings after a fortnight, so the issue concerned only the ten days between Godfrey’s fax and the normal deletion date; during that period, Demon failed to act (apparently the fax never reached the MD’s desk). In view of this delay, the court found in preliminary hearings in 1999 that Demon could not satisfy the requirement about taking reasonable care – at which point Demon threw in the towel and settled out of court, paying Godfrey about a quarter of a million pounds. Although Godfrey v. Demon set no formal legal precedent (because it was settled rather than fought out to a conclusion), the terms on which it was settled sent a thrill of fear through the industry. It seems that (unless an ISP is prepared to investigate and satisfy itself that a complaint is legally unfounded, which would often be difficult or impossible for it to achieve), its only safe response to any complaint will be automatically to take down the material complained about. This is what British ISPs have been tending to do.
EN10	1	﻿Using the File Manager (in KDE, Konqueror or in Gnome, Nautilus) create a new directory somewhere in your home directory called something appropriate for all the examples in this book, perhaps “Programming_In_Linux” without any spaces in the name. Open an editor (in KDE, kate, or in Gnome, gedit) and type in (or copy from the supplied source code zip bundle) the following: Save the text as chapter1_1.c in the new folder you created in your home directory. Open a terminal window and type: gcc -o hello chapter1_1.c to compile the program into a form that can be executed. Now type “ls -l” to list the details of all the files in this directory. You should see that chapter1_2.c is there and a file called “hello” which is the compiled C program you have just written. Now type: ./hello to execute, or run the program and it should return the text: "Hello you are learning C!!". If this worked, congratulations, you are now a programmer! The part inside /*** ***/ is a comment and is not compiled but just for information and reference. The “#include...” part tells the compiler which system libraries are needed and which header files are being referenced by this program. In our case “printf” is used and this is defined in the stdio.h header. The “int main(int argc, char *argv[])” part is the start of the actual program. This is an entrypoint and most C programs have a main function. The “int argc” is an argument to the function “main” which is an integer count of the number of character string arguments passed in “char *argv[]” (a list of pointers to character strings) that might be passed at the command line when we run it. A pointer to some thing is a name given to a memory address for this kind of data type. We can have a pointer to an integer: int *iptr, or a floating point number: float *fPtr. Any list of things is described by [], and if we know exactly how big this list is we might declare it as [200]. In this case we know that the second argument is a list of pointers to character strings. Everything else in the curly brackets is the main function and in this case the entire program expressed as lines. Each line or statement end with a semi-colon “;”. We have function calls like “printf(...)” which is a call to the standard input / output library defined in the header file stdio.h. At the end of the program “return 0” ends the program by returning a zero to the system. Return values are often used to indicate the success or status should the program not run correctly. Taking this example a stage further, examine the start of the program at the declaration of the entry point function: int main(int argc, char *argv[]) In plain English this means: The function called “main”, which returns an integer, takes two arguments, an integer called “argc” which is a count of the number of command arguments then *argv[] which is a list or array of pointers to strings which are the actual arguments typed in when you run the program from the command line. Let's rewrite the program to see what all this means before we start to panic. Save the text as chapter1_2.c in the same folder. Open a terminal window and type: gcc -o hello2 chapter1_2.c to compile the program into a form that can be executed. Now type ls -l to list the details of all the files in this directory. You should see that chapter1_2.c is there and a file called hello2 which is the compiled C program you have just written. Now type ./hello2 to execute, or run the program and it should return the text: We can see that the name of the program itself is counted as a command line argument and that the counting of things in the list or array of arguments starts at zero not at one. Now type ./hello2 my name is David to execute the program and it should return the text: So, what is happening here? It seems we are reading back each of the character strings (words) that were typed in to run the program. Lets get real and run this in a web page. Make the extra change adding the first output printf statement “Content-type:text/plain\n\n” which tells our server what kind of MIME type is going to be transmitted. Compile using gcc -o hello3 chapter1_3.c and copy the compiled file hello3 to your public_html/cgi-bin directory (or on your own machine as superuser copy the program to /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu)). Open a web browser and type in the URL http://localhost/cgi-bin/hello3?david+haskins and you should see that web content can be generated by a C program. A seldom documented feature of the function signature for “main” is that it can take three arguments and the last one we will now look at is char *env[ ] which is also a list of pointers to strings, but in this case these are the system environment variables available to the program at the time it is run Compile with gcc -o hello4 chapter1_4.c and as superuser copy the program to /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu). You can run this from the terminal where you compiled it with ./hello4 and you will see a long list of environment variables. In the browser when you enter http://localhost/cgi-bin/hello4 you will a different set altogether. We will soon find out that QUERY_STRING is an important environment variable for us in communicating with our program and in this case we see it has a value of “david+haskins” or everything after the “?” in the URL we typed. It is a valid way to send information to a common gateway interface (CGI) program like hello4 but we should restrict this to just one string. In our case we have used a “+” to join up two strings. If we typed: “david haskins” the browser would translate this so we would see: QUERY_STRING=david%20haskins We will learn later how complex sets of input values can be transmitted to our programs. ﻿When we write programs we have to make decisions or assertions about the nature of the world as we declare and describe variables to represent the kinds of things we want to include in our information processing. This process is deeply philosophical; we make ontological assertions that this or that thing exists and we make epistemological assertions when we select particular data types or collections of data types to use to describe the attributes of these things. Heavy stuff with a great responsibility and not to be lightly undertaken. As a practical example we might declare something that looks like the beginnings of a database record for geography. Here we are doing the following: - asserting that all the character strings we will ever encounter in this application will be 255 limited to characters so we define this with a preprocessor statement – these start with #. - assert that towns are associated with counties, and counties are associated with countries some hierarchical manner. - assert that the population is counted in whole numbers – no half-people. - assert the location is to be recorded in a particular variant (WGS84) of the convention of describing spots on the surface of the world in latitude and longitude that uses a decimal fraction for degrees, minutes, and seconds. Each of these statements allocates memory within the scope of the function in which it is declared. Each data declaration will occupy an amount of memory in bytes and give that bit of memory a label which is the variable name. Each data type has a specified size and the sizeof() library function will return this as an integer. In this case 3 x 256 characters, one integer, and two floats. The exact size is machine dependent but probably it is 780 bytes. Outside the function in which the data has been declared this data is inaccessible – this is the scope of declaration. If we had declared outside the main() function it would be global in scope and other functions could access it. C lets you do this kind of dangerous stuff if you want to, so be careful. Generally we keep a close eye on the scope of data, and pass either read-only copies, or labelled memory addresses to our data to parts of the programs that might need to do work on it and even change it. These labelled memory addresses are called pointers. We are using for output the printf family of library functions (sprintf for creating strings, fprintf for writing to files etc) which all use a common format string argument to specify how the data is to be represented. - %c character - %s string - %d integer - %f floating point number etc. The remaining series of variables in the arguments are placed in sequence into the format string as specified. In C it is a good idea to intialise any data you declare as the contents of the memory allocated for them is not cleared but may contain any old rubbish. Compile with: gcc -o data1 chapter2_1.c -lc Output of the program when called with : ./data1 Some programming languages like Java and C++ have a string data type that hides some of the complexity underneath what might seem a simple thing. An essential attribute of a character string is that it is a series of individual character elements of indeterminate length. Most of the individual characters we can type into a keyboard are represented by simple numerical ASCII codes and the C data type char is used to store character data. Strings are stored as arrays of characters ending with a NULL so an array must be large enough to hold the sequence of characters plus one. Remember array members are always counted from zero. In this example we can see 5 individual characters declared and initialised with values, and an empty character array set to “”. Take care to notice the difference between single quote marks ' used around characters and double quote marks “ used around character strings. Compile with: gcc -o data2 chapter2_2.c -lc Output of the program when called with : ./data2 Anything at all – name given to a variable and its meaning or its use is entirely in the mind of the beholder. Try this Download free ebooks at bookboon.com C Programming in Linux 29 Data and Memory Compile with: gcc -o data3 chapter2_3.c -lc As superuser copy the program to your public_html/cgi-bin directory (or /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu)). In the browser enter: http://localhost/cgi-bin/data3?red what you should see is this: Or if send a parameter of anything at all you will get surprising results: What we are doing here is using the string parameter argv[1] as a background colour code inside an HTML body tag. We change the Content-type specification to text/html and miraculously now our program is generating HTML content. A language being expressed inside another language. Web browsers understand a limited set of colour terms and colours can be also defined hexadecimal codes such as #FFFFFF (white) #FF0000 (red) #00FF00 (green) #0000FF (blue). This fun exercise is not just a lightweight trick, the idea that one program can generate another in another language is very powerful and behind the whole power of the internet. When we generate HTML (or XML or anything else) from a common gateway interface program like this we are creating dynamic content that can be linked to live, changing data rather than static pre-edited web pages. In practice most web sites have a mix of dynamic and static content, but here we see just how this is done at a very simple level. Throughout this book we will use the browser as the preferred interface to our programs hence we will be generating HTML and binary image stream web content purely as a means to make immediate the power of our programs. Writing code that you peer at in a terminal screen is not too impressive, and writing window-type applications is not nearly so straightforward. In practice most of the software you may be asked to write will be running on the web so we might as well start with this idea straight away. Most web applications involve multiple languages too such as CSS, (X)HTML, XML, JavaScript, PHP, JAVA, JSP, ASP, .NET, SQL. If this sounds frightening, don't panic. A knowledge of C will show you that many of these languages, which all perform different functions, have a basis of C in their syntax. ﻿The entry point into all our programs is called main() and this is a function, or a piece of code that does something, usually returning some value. We structure programs into functions to stop them become long unreadable blocks of code than cannot be seen in one screen or page and also to ensure that we do not have repeated identical chunks of code all over the place. We can call library functions like printf or strtok which are part of the C language and we can call our own or other peoples functions and libraries of functions. We have to ensure that the appropriate header file exists and can be read by the preprocessor and that the source code or compiled library exists too and is accessible. As we learned before, the scope of data is restricted to the function in which is was declared, so we use pointers to data and blocks of data to pass to functions that we wish to do some work on our data. We have seen already that strings are handled as pointers to arrays of single characters terminated with a NULL character. In this example we can repeatedly call the function “doit” that takes two integer arguments and reurns the result of some mathematical calculation. (by now you should be maintaining a Makefile as you progress, adding targets to compile examples as you go.) The result in a browser looks like this called with “func1?5:5”. In this case the arguments to our function are sent as copies and are not modified in the function but used. If we want to actual modify a variable we would have to send its pointer to a function. We send the address of the variable 'result' with &result, and in the function doit we de-reference the pointer with *result to get at the float and change its value, outside its scope inside main . This gives identical output to chapter3_1.c. C contains a number of built-in functions for doing commonly used tasks. So far we have used atoi, printf, sizeof, strtok, and sqrt. To get full details of any built-in library function all we have to do is type for example: and we will see all this: Which pretty-well tells you everything you need to know about this function and how to use it and variants of it. Most importantly it tells you which header file to include. There is no point in learning about library functions until you find you need to do something which then leads you to look for a function or a library of functions that has been written for this purpose. You will need to understand the function signature – or what the argument list means and how to use it and what will be returned by the function or done to variables passed as pointers to functions. Sometimes we wish to manage a set of variable as a group, perhaps taking all the values from a database record and passing the whole record around our program to process it. To do this we can group data into structures. This program uses a struct to define a set of properties for something called a player. The main function contains a declaration and instantiation of an array of 5 players. We pass a pointer to each array member in turn to a function to rank each one. This uses a switch statement to examine the first letter of each player name to make an arbitrary ranking. Then we pass a pointer to each array member in turn to a function that prints out the details. The results are shown here, as usual in a browser: This is a very powerful technique that is quite advanced but you will need to be aware of it. The idea of structures leads directly to the idea of classes and objects. We can see that using a struct greatly simplifies the business task of passing the data elements around the program to have different work done. If we make a change to the definition of the struct it will still work and we simply have to add code to handle new properties rather than having to change the argument lists or signatures of the functions doing the work. The definition of the structure does not actually create any data, but just sets out the formal shape of what we can instantiate. In the main function we can express this instantiation in the form shown creating a list of sequences of data elements that conform to the definition we have made. You can probably see that a struct with additional functions or methods is essentially what a class is in Java, and this is also the case in C++. Object Oriented languages start here and in fact many early systems described as “object oriented” were in fact just built using C language structs. If you take a look for example, at the Apache server development header files you will see a lot of structs for example in this fragment of httpd.h : Dont worry about what this all means – just notice that this is a very common and very powerful technique, and the design of data structures, just like the design of database tables to which it is closely related are the core, key, vital task for you to understand as a programmer. You make the philosophical decisions that the world is like this and can be modelled in this way. A heavy responsibility - in philosophy this work is called ontology (what exists?) and epistemology (how we can know about it?). I bet you never thought that this was what you were doing! We have used some simple data types to represent some information and transmit input to a program and to organise and display some visual output. We have used HTML embedded in output strings to make output visible in a web browser. We have learned about creating libraries of functions for reuse. We have learning about data structures and the use of pointers to pass them around a program. ﻿This creates a folder of the name you give it (labelmaker) and a Makefile, a modules.mk file which can be used by the Make utility, and a file called mod_labelmaker.c. The C file generated is kind of like a Hello World for Apache. It may look like a complex thing but it does supply a long explanatory comment header which is worth reading. The idea is that when Apache starts any modules in a specified location which are configured as needing to be loaded in the server configuration files, will be loaded. The *_register_hooks function lists the names and signatures of functions that can be called at specific stages in the Apache server process. In this case if the name http://localhost/labelmaker is called this module will be asked to handle whatever happens in the *_handler function.  The configuration of the server can be a bit fiddly but in OpenSuse we have to add this to the file  and in /etc/config.sys/apache2 we add the name of our module labelmaker to long comma-separated list in the line starting APACHE_MODULES=”.....,labelmaker” Now go to the folder labelmaker and type:  Now we can plug in the work we did for the graphics library in Chapter 6 as a replacement handler function (in the code Chapter7_1.c there are BOTH handlers, one commented out). Note the (highlighted) call to a modified decode_value function that uses the r->args pointer to get the QUERY_STRING rather than getenv() . Also Apache handles the output a bit differently too – get get a pointer to the array of bytes in the image by calling gdImageGifPtr then the ap_rwrite function outputs the data. We have to free the pointer with gdFree after the output call.  Whilst tricky to write and debug, this is probably the most rewarding and esoteric area where you can do real, commerically useful and safely deployable web content generation. It is easy to see how this example could be extended with parameters for colours and fonts to make a useful web content tool. There is very little clear simple material about apache modukles but start with the on-line documentation at http://httpd.apache.org/docs/2.2/developer/ One recent book worth looking at is “The Apache Modules Book” Nick Kew, Prentice Hall.  The ability to write short programs in C to automate tedious tasks or to do things that would otherwise take hours of fiddling about with cumbersome tools such as doing mail-merge, is one on the things you will be most pleased you have learned how to do. This project is such a time-saver. Ghost is a lightweight PHP generator for you to customise. If you find yourself having to build PHP web sites all the time, a quick way to generate all the parameter-passing, decoding, forms building and database management code in one step would be useful. Tools like Ruby on Rails offer such functionality but are infinitely more complex to set up and run and you end up with needing to learn yet another language to go any further. Probably the best way to start with this tool is to compile and run it. Unzip the ghost.zip source into your public_html folder which creates a folder called ghost. The Makefile contains a target g1 that compiles and links ghost. So go to public_html/ghost and type: make g1 . To run the site generator type: - ./ghost testwebsite data1 data2 data1 data3 data4 data6 data6 - This will create: - a folder public_html/testwebsite - a mysql database table called testwebsite with text fields data1 data2 data1 data3 data4 data6 data6 - atestwebsite.css file - empty header.html and footer.html pages - index.php that demonstrates a form handling entry, edit & update, and delete to the database table for the data items specified. In a browser what you see is this at http://localhost/~yourname/testwebsite The idea behind this is that all the mechanical bits to create and manage the form content are done and can be customised. This screen shot shows the result of submitting one record. Thde top row is for entering new data, the lower row(s) allow editing or deleting of records. It is a framework that allows you to take and use parts in your own website design. Let us examine this code in sections. The first section declares the required data and creates the folder and CSS file. Next the header.html and footer.html files are generated. These files is loaded by the PHP file and could be used as a generic common header and footers. The CSS file is referenced from the header.html file. Next we create the data base. The complicated part starts now, of generating a php script. The best way to understand this is to examine the actual output of the program when we view the source of the page in the browser. The top row is a form with a text box for each column defined in the table generated by running the ghost program. For each row in the table we now generate a form allowing editing of the data and an anchor link to do a delete operation. Close examination of the file index.php will allow you to see where all this happens, and to work backward to find where in the ghost.c source code this PHP code is generated. A good idea is to use a highlighter pen on a printout as we are embedding a language (HTML) inside another language (PHP) which is in turn inside another language so very very careful use is made of the escape characters '\ 'to express quotation marks both single and double where necessary to make it all work. This may seem complex – but the speedy prototyping that ghost permits makes it worthwhile to spend time customising the C code so the PHP that you want and the database you want come out the way you want it. Here is the part of the PHP file index.php which generates the edit or delete rows. The static HTML is highlighted and the other parts are inserted by MySQL PHP function calls. As you can see a great deal of tedious and repetitive works has been automated. You can move on by modifying the PHP code or go deeper to customise the C program which generates all of it. I personally use ghost frequently to save time on site-building and this is why I wrote it. I got bored making mistakes writing virtually identical code to decode HTML forms and populate or update databases.$$$﻿This creates a folder of the name you give it (labelmaker) and a Makefile, a modules.mk file which can be used by the Make utility, and a file called mod_labelmaker.c. The C file generated is kind of like a Hello World for Apache. It may look like a complex thing but it does supply a long explanatory comment header which is worth reading. The idea is that when Apache starts any modules in a specified location which are configured as needing to be loaded in the server configuration files, will be loaded. The *_register_hooks function lists the names and signatures of functions that can be called at specific stages in the Apache server process. In this case if the name http://localhost/labelmaker is called this module will be asked to handle whatever happens in the *_handler function.  The configuration of the server can be a bit fiddly but in OpenSuse we have to add this to the file  and in /etc/config.sys/apache2 we add the name of our module labelmaker to long comma-separated list in the line starting APACHE_MODULES=”.....,labelmaker” Now go to the folder labelmaker and type:  Now we can plug in the work we did for the graphics library in Chapter 6 as a replacement handler function (in the code Chapter7_1.c there are BOTH handlers, one commented out). Note the (highlighted) call to a modified decode_value function that uses the r->args pointer to get the QUERY_STRING rather than getenv() . Also Apache handles the output a bit differently too – get get a pointer to the array of bytes in the image by calling gdImageGifPtr then the ap_rwrite function outputs the data. We have to free the pointer with gdFree after the output call.  Whilst tricky to write and debug, this is probably the most rewarding and esoteric area where you can do real, commerically useful and safely deployable web content generation. It is easy to see how this example could be extended with parameters for colours and fonts to make a useful web content tool. There is very little clear simple material about apache modukles but start with the on-line documentation at http://httpd.apache.org/docs/2.2/developer/ One recent book worth looking at is “The Apache Modules Book” Nick Kew, Prentice Hall.  The ability to write short programs in C to automate tedious tasks or to do things that would otherwise take hours of fiddling about with cumbersome tools such as doing mail-merge, is one on the things you will be most pleased you have learned how to do. This project is such a time-saver. Ghost is a lightweight PHP generator for you to customise. If you find yourself having to build PHP web sites all the time, a quick way to generate all the parameter-passing, decoding, forms building and database management code in one step would be useful. Tools like Ruby on Rails offer such functionality but are infinitely more complex to set up and run and you end up with needing to learn yet another language to go any further. Probably the best way to start with this tool is to compile and run it. Unzip the ghost.zip source into your public_html folder which creates a folder called ghost. The Makefile contains a target g1 that compiles and links ghost. So go to public_html/ghost and type: make g1 . To run the site generator type: - ./ghost testwebsite data1 data2 data1 data3 data4 data6 data6 - This will create: - a folder public_html/testwebsite - a mysql database table called testwebsite with text fields data1 data2 data1 data3 data4 data6 data6 - atestwebsite.css file - empty header.html and footer.html pages - index.php that demonstrates a form handling entry, edit & update, and delete to the database table for the data items specified. In a browser what you see is this at http://localhost/~yourname/testwebsite The idea behind this is that all the mechanical bits to create and manage the form content are done and can be customised. This screen shot shows the result of submitting one record. Thde top row is for entering new data, the lower row(s) allow editing or deleting of records. It is a framework that allows you to take and use parts in your own website design. Let us examine this code in sections. The first section declares the required data and creates the folder and CSS file. Next the header.html and footer.html files are generated. These files is loaded by the PHP file and could be used as a generic common header and footers. The CSS file is referenced from the header.html file. Next we create the data base. The complicated part starts now, of generating a php script. The best way to understand this is to examine the actual output of the program when we view the source of the page in the browser. The top row is a form with a text box for each column defined in the table generated by running the ghost program. For each row in the table we now generate a form allowing editing of the data and an anchor link to do a delete operation. Close examination of the file index.php will allow you to see where all this happens, and to work backward to find where in the ghost.c source code this PHP code is generated. A good idea is to use a highlighter pen on a printout as we are embedding a language (HTML) inside another language (PHP) which is in turn inside another language so very very careful use is made of the escape characters '\ 'to express quotation marks both single and double where necessary to make it all work. This may seem complex – but the speedy prototyping that ghost permits makes it worthwhile to spend time customising the C code so the PHP that you want and the database you want come out the way you want it. Here is the part of the PHP file index.php which generates the edit or delete rows. The static HTML is highlighted and the other parts are inserted by MySQL PHP function calls. As you can see a great deal of tedious and repetitive works has been automated. You can move on by modifying the PHP code or go deeper to customise the C program which generates all of it. I personally use ghost frequently to save time on site-building and this is why I wrote it. I got bored making mistakes writing virtually identical code to decode HTML forms and populate or update databases.
EN37	0	﻿As its name implies control engineering involves the design of an engineering product or system where a requirement is to accurately control some quantity, say the temperature in a room or the position or speed of an electric motor. To do this one needs to know the value of the quantity being controlled, so that being able to measure is fundamental to control. In principle one can control a quantity in a so called open loop manner where ‘knowledge’ has been built up on what input will produce the required output, say the voltage required to be input to an electric motor for it to run at a certain speed. This works well if the ‘knowledge’ is accurate but if the motor is driving a pump which has a load highly dependent on the temperature of the fluid being pumped then the ‘knowledge’ will not be accurate unless information is obtained for different fluid temperatures. But this may not be the only practical aspect that affects the load on the motor and therefore the speed at which it will run for a given input, so if accurate speed control is required an alternative approach is necessary. This alternative approach is the use of feedback whereby the quantity to be controlled, say C, is measured, compared with the desired value, R, and the error between the two, E = R - C used to adjust C. This gives the classical feedback loop structure of Figure 1.1. In the case of the control of motor speed, where the required speed, R, known as the reference is either fixed or moved between fixed values, the control is often known as a regulatory control, as the action of the loop allows accurate speed control of the motor for the aforementioned situation in spite of the changes in temperature of the pump fluid which affects the motor load. In other instances the output C may be required to follow a changing R, which for example, might be the required position movement of a robot arm. The system is then often known as a servomechanism and many early textbooks in the control engineering field used the word servomechanism in their title rather than control. The use of feedback to regulate a system has a long history [1.1, 1.2], one of the earliest concepts, used in Ancient Greece, was the float regulator to control water level, which is still used today in water tanks. The first automatic regulator for an industrial process is believed to have been the flyball governor developed in 1769 by James Watt. It was not, however, until the wartime period beginning in 1939, that control engineering really started to develop with the demand for servomechanisms for munitions fire control and guidance. With the major improvements in technology since that time the applications of control have grown rapidly and can be found in all walks of life. Control engineering has, in fact, been referred to as the ‘unseen technology’ as so often people are unaware of its existence until something goes wrong. Few people are, for instance, aware of its contribution to the development of storage media in digital computers where accurate head positioning is required. This started with the magnetic drum in the 50’s and is required today in disk drives where position accuracy is of the order of 1μm and movement between tracks must be done in a few ms. Feedback is, of course, not just a feature of industrial control but is found in biological, economic and many other forms of system, so that theories relating to feedback control can be applied to many walks of life. The book is concerned with theoretical methods for continuous linear feedback control system design, and is primarily restricted to single-input single-output systems. Continuous linear time invariant systems have linear differential equation mathematical models and are always an approximation to a real device or system. All real systems will change with time due to age and environmental changes and may only operate reasonably linearly over a restricted range of operation. There is, however, a rich theory for the analysis of linear systems which can provide excellent approximations for the analysis and design of real world situations when used within the correct context. Further simulation is now an excellent means to support linear theoretical studies as model errors, such as the affects of neglected nonlinearity, can easily be assessed. There are total of 11 chapters and some appendices, the major one being Appendix A on Laplace transforms. The next chapter provides a brief description of the forms of mathematical model representations used in control engineering analysis and design. It does not deal with mathematical modelling of engineering devices, which is a huge subject and is best dealt with in the discipline covering the subject, since the devices or components could be electrical, mechanical, hydraulic etc. Suffice to say that one hopes to obtain an approximate linear mathematical model for these components so that their effect in a system can be investigated using linear control theory. The mathematical models discussed are the linear differential equation, the transfer function and a state space representation, together with the notations used for them in MATLAB. Chapter 3 discusses transfer functions, their zeros and poles, and their responses to different inputs. The following chapter discusses in detail the various methods for plotting steady state frequency responses with Bode, Nyquist and Nichols plots being illustrated in MATLAB. Hopefully sufficient detail, which is brief when compared with many textbooks, is given so that the reader clearly understands the information these plots provide and more importantly understands the form of frequency response expected from a specific transfer function. The material of chapters 2-4 could be covered in other courses as it is basic systems theory, there having been no mention of control, which starts in chapter 5. The basic feedback loop structure shown in Figure 1.1 is commented on further, followed by a discussion of typical performance specifications which might have to be met in both the time and frequency domains. Steady state errors are considered both for input and disturbance signals and the importance and properties of an integrator are discussed from a physical as well as mathematical viewpoint. The chapter concludes with a discussion on stability and a presentation of several results including the Mikhailov criterion, which is rarely mentioned in English language texts. ﻿Control systems exist in many fields of engineering so that components of a control system may be electrical, mechanical, hydraulic etc. devices. If a system has to be designed to perform in a specific way then one needs to develop descriptions of how the outputs of the individual components, which make up the system, will react to changes in their inputs. This is known as mathematical modelling and can be done either from the basic laws of physics or from processing the input and output signals in which case it is known as identification. Examples of physical modelling include deriving differential equations for electrical circuits involving resistance, inductance and capacitance and for combinations of masses, springs and dampers in mechanical systems. It is not the intent here to derive models for various devices which may be used in control systems but to assume that a suitable approximation will be a linear differential equation. In practice an improved model might include nonlinear effects, for example Hooke’s Law for a spring in a mechanical system is only linear over a certain range; or account for time variations of components. Mathematical models of any device will always be approximate, even if nonlinear effects and time variations are also included by using more general nonlinear or time varying differential equations. Thus, it is always important in using mathematical models to have an appreciation of the conditions under which they are valid and to what accuracy. Starting therefore with the assumption that our model is a linear differential equation then in general it will have the form:- where D denotes the differential operator d/dt. A(D) and B(D) are polynomials in D with Di d i / dt i , the ith derivative, u(t) is the model input and y(t) its output. So that one can write where the a and b coefficients will be real numbers. The orders of the polynomials A and B are assumed to be n and m, respectively, with n m. Thus, for example, the differential equation with the dependence of y and u on t assumed can be written  In order to solve an nth order differential equation, that is determine the output y for a given input u, one must know the initial conditions of y and its first n-1 derivatives. For example if a projectile is falling under gravity, that is constant acceleration, so that D2y= constant, where y is the height, then in order to find the time taken to fall to a lower height, one must know not only the initial height, normally assumed to be at time zero, but the initial velocity, dy/dt, that is two initial conditions as the equation is second order (n = 2). Control engineers typically study solutions to differential equations using either Laplace transforms or a state space representation. A short introduction to the Laplace transformation is given in Appendix A for the reader who is not familiar with its use. It is an integral transformation and its major, but not sole use, is for differential equations where the independent time variable t is transformed to the complex variable s by the expression Since the exponential term has no units the units of s are seconds-1, that is using mks notation s has units of s-1. If denotes the Laplace transform then one may write [f(t)] = F(s) and -1[F(s)] = f(t). The relationship is unique in that for every f(t), [F(s)], there is a unique F(s), [f(t)]. It is shown in Appendix A that when the n-1 initial conditions, Dn-1y(0) are zero the Laplace transform of Dny(t) is snY(s). Thus the Laplace transform of the differential equation (2.1) with zero initial conditions can be written with the assumed notation that signals as functions of time are denoted by lower case letters and as functions of s by the corresponding capital letter. If equation (2.8) is written then this is known as the transfer function, G(s), between the input and output of the ‘system’, that is whatever is modelled by equation (2.1). B(s), of order m, is referred to as the numerator polynomial and A(s), of order n, as the denominator polynomial and are from equations (2.2) and (2.3) Since the a and b coefficients of the polynomials are real numbers the roots of the polynomials are either real or complex pairs. The transfer function is zero for those values of s which are the roots of B(s), so these values of s are called the zeros of the transfer function. Similarly, the transfer function will be infinite at the roots of the denominator polynomial A(s), and these values are called the poles of the transfer function. The general transfer function (2.9) thus has m zeros and n poles and is said to have a relative degree of n-m, which can be shown from physical realisation considerations cannot be negative. Further for n > m it is referred to as a strictly proper transfer function and for n m as a proper transfer function. When the input u(t) to the differential equation of (2.1) is constant the output y(t) becomes constant when all the derivatives of the output are zero. Thus the steady state gain, or since the input is often thought of as a signal the term d.c. gain (although it is more often a voltage than a current!) is used, and is given by If the n roots of A(s) are i , i = 1….n and of B(s) are j, j = 1….m, then the transfer function may be written in the zero-pole form When the transfer function is known in the zero-pole form then the location of its zeros and poles can be shown on an s plane zero-pole plot, where the zeros are marked with a circle and the poles by a cross. The information on this plot then completely defines the transfer function apart from the gain K. In most instances engineers prefer to keep any complex roots in quadratic form, thus for example writing  ﻿As mentioned previously a major reason for wishing to obtain a mathematical model of a device is to be able to evaluate the output in response to a given input. Using the transfer function and Laplace transforms provides a particularly elegant way of doing this. This is because for a block with input U(s) and transfer function G(s) the output Y(s) = G(s)U(s). When the input, u(t), is a unit impulse which is conventionally denoted by (t), U(s) = 1 so that the output Y(s) = G(s). Thus in the time domain, y(t) = g(t), the inverse Laplace transform of G(s), which is called the impulse response or weighting function of the block. The evaluation of y(t) for any input u(t) can be done in the time domain using the convolution integral (see Appendix A, theorem (ix)) but it is normally much easier to use the transform relationship Y(s) = G(s)U(s). To do this one needs to find the Laplace transform of the input u(t), form the product G(s)U(s) and then find its inverse Laplace transform. G(s)U(s) will be a ratio of polynomials in s and to find the inverse Laplace transform, the roots of the denominator polynomial must be found to allow the expression to be put into partial fractions with each term involving one denominator root (pole). Assuming, for example, the input is a unit step so that U(s) = 1/s then putting G(s)U(s) into partial fractions will result in an expression for Y(s) of the form where in the transfer function G(s) = B(s)/A(s), the n poles of G(s) [zeros of A(s)] are i, i = 1…n and the coefficients C0 and Ci, i = 1…n, will depend on the numerator polynomial B(s), and are known as the residues at the poles. Taking the inverse Laplace transform yields The first term is a constant C0, sometimes written C0u0(t) because the Laplace transform is defined for t 0, where u0(t) denotes the unit step at time zero. Each of the other terms is an exponential, which provided the real part of i is negative will decay to zero as t becomes large. In this case the transfer function is said to be stable as a bounded input has produced a bounded output. Thus a transfer function is stable if all its poles lie in the left hand side (lhs) of the s plane zero-pole plot illustrated in Figure 2.1. The larger the negative value of i the more rapidly the contribution from the ith term decays to zero. Since any poles which are complex occur in complex pairs, say of the form 1, 2 = ± j , then the corresponding two residues C1 and C2 will be complex pairs and the two terms will combine to give a term of the form Ce t sin( t ) . This is a damped oscillatory exponential term where , which will be negative for a stable transfer function, determines the damping and the frequency [strictly angular frequency] of the oscillation. For a specific calculation most engineers, as mentioned earlier, will leave a complex pair of roots as a quadratic factor in the partial factorization process, as illustrated in the Laplace transform inversion example given in Appendix A. For any other input to G(s), as with the step input, the poles of the Laplace transform of the input will occur in a term of the partial fraction expansion (3.2), [as for the C0/s term above], and will therefore produce a bounded output for a bounded input. In control engineering the major deterministic input signals that one may wish to obtain responses to are a step, an impulse, a ramp and a constant frequency input. The purpose of this section is to discuss step responses of specific transfer functions, hopefully imparting an understanding of what can be expected from a knowledge of the zeros and poles of the transfer function without going into detailed mathematics. A transfer function with a single pole is s a G s K  ( ) 1 , which may also be written in the socalled time constant form sT G s K   1 ( ) , where K K / a 1 and T 1/ a The steady state gainG(0) K , that is the final value of the response, and T is called the time constant as it determines the speed of the response. K will have units relating the input quantity to the output quantity, for example °C/V, if the input is a voltage and the output temperature. T will have the same units of time as s-1, normally seconds. The output, Y(s), for a unit step input is given by Taking the inverse Laplace transform gives the result The larger the value of T (i.e. the smaller the value of a), the slower the exponential response. It can easily be shown that y(T) 0.632K , T dt dy(0) and y(5T) 0.993K or in words, the output reaches 63.2% of the final value after a time T, the initial slope of the response is T and the response has essentially reached the final value after a time 5T. The step response in MATLAB can be obtained by the command step(num,den). The figure below shows the step response for the transfer function with K = 1 on a normalised time scale. Here the transfer function G(s) is often assumed to be of the form It has a unit steady state gain, i.e G(0) = 1, and poles at    1 2 o o s j , which are complex when  1. For a unit step input the output Y(s), can be shown after some algebra, which has been done so that the inverse Laplace transforms of the second and third terms are damped cosinusoidal and sinusoidal expressions, to be given by Taking the inverse Laplace transform it yields, again after some algebra, where cos 1 . is known as the damping ratio. It can also be seen that the angle to the negative real axis from the origin to the pole with positive imaginary part is tan 1 (1 2 )1/ 2 /  cos 1  .  ﻿The frequency response of a transfer function G(j ) was introduced in the last chapter. As G(j ) is a complex number with a magnitude and argument (phase) if one wishes to show its behaviour over a frequency range then one has 3 parameters to deal with the frequency, , the magnitude, M, and the phase . Engineers use three common ways to plot the information, which are known as Bode diagrams, Nyquist diagrams and Nichols diagrams in honour of the people who introduced them. All portray the same information and can be readily drawn in MATLAB for a system transfer function object G(s). One diagram may prove more convenient for a particular application, although engineers often have a preference. In the early days when computing facilities were not available Bode diagrams, for example, had some popularity because of the ease with which they could, in many instances, be rapidly approximated. All the plots will be discussed below, quoting many results without going into mathematical detail, in the hope that the reader will obtain enough knowledge to know whether MATLAB plots obtained are of the general shape expected. A Bode diagram consists of two separate plots the magnitude, M, as a function of frequency and the phase as a function of frequency. For both plots the frequency is plotted on a logarithmic (log) scale along the x axis. A log scale has the property that the midpoint between two frequencies 1 and 2 is the frequency 1 2    . A decade of frequency is from a value to ten times that value and an octave from a value to twice that value. The magnitude is plotted either on a log scale or in decibels (dB), where dB M 10 20log . The phase is plotted on a linear scale. Bode showed that for a transfer function with no right hand side (rhs) s-plane zeros the phase is related to the slope of the magnitude characteristic by the relationship It can be further shown from this expression that a relatively good approximation is that the phase at any frequency is 15° times the slope of the magnitude curve in dB/octave. This was a useful concept to avoid drawing both diagrams when no computer facilities were available. For two transfer functions G1 and G2 in series the resultant transfer function, G, is their product, this means for their frequency response which in terms of their magnitudes and phases can be written Thus since a log scale is used on the magnitude of a Bode diagram this means Bode magnitude plots for two transfer functions in series can be added, as also their phases on the phase diagram. Hence a transfer function in zero-pole form can be plotted on the magnitude and phase Bode diagrams simple by adding the individual contributions from each zero and pole. It is thus only necessary to know the Bode plots of single roots and quadratic factors to put together Bode plots for a complicated transfer function if it is known in zero-pole form. The single pole transfer function is normally considered in time constant form with unit steady state gain, that is It is easy to show that this transfer function can be approximated by two straight lines, one constant at 0 dB, as G(0) = 1, until the frequency, 1/T, known as the break point, and then from that point by a line with slope -6dB/octave. The actual curve and the approximation are shown in Figure 4.1 together with the phase curve. The differences between the exact magnitude curve and the approximation are symmetrical, that is a maximum at the breakpoint of 3dB, 1dB one octave each side of the breakpoint, 0.3 dB two octaves away etc. The phase changes between 0° and - 90° again with symmetry about the breakpoint phase of -45°. Note a steady slope of -6 dB/octave has a corresponding phase of -90° The Bode magnitude plot of a single zero time constant, that is is simply a reflection in the 0 dB axis of the pole plot. That is the approximate magnitude curve is flat at 0 dB until the break point frequency, 1/T, and then increases at 6 dB/octave. Theoretically as the frequency tends to infinity so does its gain so that it is not physically realisable. The phase curve goes from 0° to +90° The transfer function of an integrator, which is a pole at the origin in the zero-pole plot, is 1/s. It is sometimes taken with a gain K, i.e.K/s. Here K will be replaced by 1/T to give the transfer function On a Bode diagram the magnitude is a constant slope of -6 dB/octave passing through 0 dB at the frequency 1/T. Note that on a log scale for frequency, zero frequency where the integrator has infinite gain (the transfer function can only be produced electronically by an active device) is never reached. The phase is -90° at all frequencies. A differentiator has a transfer function of sT which gives a gain characteristic with a slope of 6 dB/octave passing through 0dB at a frequency of 1/T. Theoretically it produces infinite gain at infinite frequency so again it is not physically realisable. It has a phase of +90° at all frequencies. The quadratic factor form is again taken for two complex poles with < 1 as in equation (3.7), that is Again G(0) = 1 so the response starts at 0 dB and can be approximated by a straight line at 0 dB until o and by a line from o at -12 dB/octave. However, this is a very coarse approximation as the behaviour around o is highly dependent on . It can be shown that the magnitude reaches a maximum value of which is approximately 1/2 for small , at a frequency of This frequency is thus always less than o and only exists for < 0.707. The response with = 0.707 always has magnitude, M < 1. The phase curve goes from 0° to - 180° as expected from the original and final slopes of the magnitude curve, it has a phase shift of -90° at the frequency o independent of and changes more rapidly near o for smaller , as expected due to the more rapid change in the slope of the corresponding magnitude curve. ﻿It is said the future of publishing rests with iPad and its like; there is a lot of money being invested in it and ways to make money from ePub, iOS and Android Apps. For those wishing to make free content available on multiple platforms creating interactive PDFs is a safe option. Later I’ll illustrate how to generate ePUB content. Comparing these options will allow you to assess the merits of PDF versus ePUB. This section illustrates how to use Pages, style-sheets and hyperlinks to create a PDF with internal hyperlinked navigation that can be synced via iTunes to an iPad, or similar. PDF syncing via iTunes is really for single-users but it’s included so you’ll have a quick way to test your PDF on iPAD before distributing it. We’ll also look at setting up a Print PDF to Tunes option as any printable document on a Mac can be synced via iTunes to iOS’s Book App. The Book App is the default book reading software for iPad, iPhones and iPod Touch. Launch Pages and from the Template Chooser select a Word Processing, Blank portrait orientation document. Go to File > Page Setup… or selecting Page Setup… in the Document tab of the Document Inspector check the Document format. In Europe that’s likely to be A4. You can also create the iPad 4:3 ratio document. To create the iPad 4:3 ratio chose Manage Custom Sizes from Page Size in Page Setup… and enter appropriate values. Part of this exercise investigates navigation. For this we need multiple pages. On page one type ‘Contents’ then go to the Insert menu and chose Page Break. This makes a second page, here type Chapter 1, repeat the Page Break and typing exercise three more times, adding the text ‘Chapters 2, 3 and 4’ at the top of each page of your document. With long documents use page thumbnails. To show Page Thumbnails use the View button in the Tool Bar. This makes navigating long documents much easier. Click Page Two. From the Tool Bar click Media Browser icon, labelled Media and from the Media Browser panel select the Photos tab. Hopefully you’ll have pictures in iPhoto or Aperture; they’ll appear here if you do. Drag one onto your document. You could add video or audio from the Media Browser also. Insert some sample text, I’ve used the sampler Lorem Ipsum. Use anything you can find to fill out out the document. I inserted text into Chapter 2, then typed in three subheadings, called Sections 1, 2 and 3. This gives a second tier to the navigation scheme. Use Paragraph Styles. Navigate back to where you typed Chapter 1. Triple-click to select that line. Then from the Format Bar choose Heading 1 from paragraph style. (See the screen grab for its location.) Then navigate to where you typed Chapter 2, 3, and 4 and add the Heading 1 paragraph style to them. Now go back to where you typed the words Section 1 and triple click those words and apply the Heading 2 paragraph style. Repeat this operation for the other sub-headers. The primary navigation for the PDF comes from creating a Table Of Contents. Go back to the first page. Make sure the text insertion point is blinking just after the word Contents. Then press the return key. From the Insert Menu choose Table Of Contents. Pages’ Document Inspector jumps to the TOC tab. Have a good look at this. Both Heading 1 and 2 are ticked, as is Make Page Numbers Links. By default Pages adds hyperlinks to the Table Of Contents, allowing you to jump to any of your Chapters or Section Headers, just by clicking on a page number. However, once you jump away from the Table Of Contents there is no link back. To make the link back, double-click the word Contents you typed on page 1. From the Insert Menu choose Bookmark. Note: you are not stuck with the fonts and formatting of Headers and Table Of Contents as they can be changed and their styles sheets refreshed. With the Contents page Bookmarked you can use the document’s Footer to add the usual stuff; page number, page count et cetera, but in addition type ‘Link To Contents Page’. Then on the Link Inspector choose the Hyperlink tab. Select the words ‘Link To Contents Page’. With the text selected click the Enable As A Hyperlink button on the Link Inspector. The Link To: menu in the Inspector becomes active so choose Bookmark. Then, from Name: choose Contents. You’ve now created a link back to the Table Of Contents. Your Pages document now has a navigation system. Only text elements can be made into a hyperlink. In a real world situation you will probably have taken time to add images throughout your document. This increases file size, and slows down page loading times. Save a new version of your Pages document; this preserves the higher resolution version on the Pages document. Using the duplicate copy go to File and Reduce File Size. Pages shows a dialogue box listing the file size reduction. There are two options to Share As PDF. File > Export… or Share > Export, both open the Export dialogue box. Click PDF and experiment between Better and Best quality for further file size reduction. Also open the Security Options disclosure triangle to review PDF security options. Stopping copying from the document is worth considering (note this Bookboon Book doesn’t allow text to be copied). Click Next, save the PDF then open it in Preview. In Preview you can text the hyperlink navigation and assess image quality. Once you’re happy you can Print the PDF to iTunes. Stay in Preview, as this PDF has been optimised. Earlier I mentioned I’d added an iTunes option to the PDF menu, this is how to do it. Go to File > Print. In the Print dialogue box click on the PDF menu, bottom left, and chose Edit Menu. Download free ebooks at bookboon.com Please click the advert iWork - Pages 59 Sharing Your Work Go to File > Print. In the Print dialogue box click on the PDF menu, bottom left, and chose Edit Menu. Click the + symbol in the Edit PDF Menu. A Finder window appears. Use it to navigate to your Applications folder and double-click your iTunes app.$$$﻿It is said the future of publishing rests with iPad and its like; there is a lot of money being invested in it and ways to make money from ePub, iOS and Android Apps. For those wishing to make free content available on multiple platforms creating interactive PDFs is a safe option. Later I’ll illustrate how to generate ePUB content. Comparing these options will allow you to assess the merits of PDF versus ePUB. This section illustrates how to use Pages, style-sheets and hyperlinks to create a PDF with internal hyperlinked navigation that can be synced via iTunes to an iPad, or similar. PDF syncing via iTunes is really for single-users but it’s included so you’ll have a quick way to test your PDF on iPAD before distributing it. We’ll also look at setting up a Print PDF to Tunes option as any printable document on a Mac can be synced via iTunes to iOS’s Book App. The Book App is the default book reading software for iPad, iPhones and iPod Touch. Launch Pages and from the Template Chooser select a Word Processing, Blank portrait orientation document. Go to File > Page Setup… or selecting Page Setup… in the Document tab of the Document Inspector check the Document format. In Europe that’s likely to be A4. You can also create the iPad 4:3 ratio document. To create the iPad 4:3 ratio chose Manage Custom Sizes from Page Size in Page Setup… and enter appropriate values. Part of this exercise investigates navigation. For this we need multiple pages. On page one type ‘Contents’ then go to the Insert menu and chose Page Break. This makes a second page, here type Chapter 1, repeat the Page Break and typing exercise three more times, adding the text ‘Chapters 2, 3 and 4’ at the top of each page of your document. With long documents use page thumbnails. To show Page Thumbnails use the View button in the Tool Bar. This makes navigating long documents much easier. Click Page Two. From the Tool Bar click Media Browser icon, labelled Media and from the Media Browser panel select the Photos tab. Hopefully you’ll have pictures in iPhoto or Aperture; they’ll appear here if you do. Drag one onto your document. You could add video or audio from the Media Browser also. Insert some sample text, I’ve used the sampler Lorem Ipsum. Use anything you can find to fill out out the document. I inserted text into Chapter 2, then typed in three subheadings, called Sections 1, 2 and 3. This gives a second tier to the navigation scheme. Use Paragraph Styles. Navigate back to where you typed Chapter 1. Triple-click to select that line. Then from the Format Bar choose Heading 1 from paragraph style. (See the screen grab for its location.) Then navigate to where you typed Chapter 2, 3, and 4 and add the Heading 1 paragraph style to them. Now go back to where you typed the words Section 1 and triple click those words and apply the Heading 2 paragraph style. Repeat this operation for the other sub-headers. The primary navigation for the PDF comes from creating a Table Of Contents. Go back to the first page. Make sure the text insertion point is blinking just after the word Contents. Then press the return key. From the Insert Menu choose Table Of Contents. Pages’ Document Inspector jumps to the TOC tab. Have a good look at this. Both Heading 1 and 2 are ticked, as is Make Page Numbers Links. By default Pages adds hyperlinks to the Table Of Contents, allowing you to jump to any of your Chapters or Section Headers, just by clicking on a page number. However, once you jump away from the Table Of Contents there is no link back. To make the link back, double-click the word Contents you typed on page 1. From the Insert Menu choose Bookmark. Note: you are not stuck with the fonts and formatting of Headers and Table Of Contents as they can be changed and their styles sheets refreshed. With the Contents page Bookmarked you can use the document’s Footer to add the usual stuff; page number, page count et cetera, but in addition type ‘Link To Contents Page’. Then on the Link Inspector choose the Hyperlink tab. Select the words ‘Link To Contents Page’. With the text selected click the Enable As A Hyperlink button on the Link Inspector. The Link To: menu in the Inspector becomes active so choose Bookmark. Then, from Name: choose Contents. You’ve now created a link back to the Table Of Contents. Your Pages document now has a navigation system. Only text elements can be made into a hyperlink. In a real world situation you will probably have taken time to add images throughout your document. This increases file size, and slows down page loading times. Save a new version of your Pages document; this preserves the higher resolution version on the Pages document. Using the duplicate copy go to File and Reduce File Size. Pages shows a dialogue box listing the file size reduction. There are two options to Share As PDF. File > Export… or Share > Export, both open the Export dialogue box. Click PDF and experiment between Better and Best quality for further file size reduction. Also open the Security Options disclosure triangle to review PDF security options. Stopping copying from the document is worth considering (note this Bookboon Book doesn’t allow text to be copied). Click Next, save the PDF then open it in Preview. In Preview you can text the hyperlink navigation and assess image quality. Once you’re happy you can Print the PDF to iTunes. Stay in Preview, as this PDF has been optimised. Earlier I mentioned I’d added an iTunes option to the PDF menu, this is how to do it. Go to File > Print. In the Print dialogue box click on the PDF menu, bottom left, and chose Edit Menu. Download free ebooks at bookboon.com Please click the advert iWork - Pages 59 Sharing Your Work Go to File > Print. In the Print dialogue box click on the PDF menu, bottom left, and chose Edit Menu. Click the + symbol in the Edit PDF Menu. A Finder window appears. Use it to navigate to your Applications folder and double-click your iTunes app.
EN16	1	﻿Since its creation in 1987 Perl has become one of the most widely used programming languages. One measure of this is the frequency with which various languages are mentioned in job adverts. The site www.indeed.com monitors trends: in 2010 it shows that the only languages receiving more mentions on job sites are C and its offshoots C++ and C#, Java, and JavaScript. Perl is a general-purpose programming language, but it has outstanding strengths in processing text files: often one can easily achieve in a line or two of Perl code some text-processing task that might take half a page of C or Java. In consequence, Perl is heavily used for computer-centre system admin, and for Web development – Web pages are HTML text files. Another factor in the popularity of Perl is simply that many programmers find it fun to work with. Compared with Perl, other leading languages can feel worthy but tedious. Perl is a language in which it is easy to get started, but – because it offers handy ways to do very many different things – it takes a long time before anyone finishes learning Perl (if they do ever finish). One standard reference, Steven Holzner’s Perl Black Book (second edn, Paraglyph Press, 2001) is about 1300 dense pages long. So, for the beginner, it is important to focus on the core of the language, and avoid being distracted by all the other features which are there, but are not essential in the early stages. This book helps the reader to do that. It covers everything he or she needs to know in order to write successful Perl programs and grow in confidence with the language, while shielding him or her from confusing inessentials.1 Later chapters contain pointers towards various topics which have deliberately been omitted here. When the core of the language has been thoroughly mastered, that will be soon enough to begin broadening one’s knowledge. Many productive Perl programmers have gaps in their awareness of the full range of language features. The book is intended for beginners: readers who are new to Perl, and probably new to computer programming. The book takes care to spell out concepts that would be very familiar to anyone who already has experience of programming in some other language. However, there will be readers who use this book to begin learning Perl, but who have worked with another language in the past. For the benefit of that group, I include occasional brief passages drawing attention to features of Perl that could be confusing to someone with a background in another language. Programming neophytes can skim over those passages.  The reader I had in mind as I was writing this book was a reader much like myself: someone who is not particularly interested in the fine points of programming languages for their own sake, but who wants to use a programming language because he has work he wants to get done, and programming is a necessary step towards doing it. As it happens, I am a linguist by training, and much of my own working life is spent studying patterns in the way the English language is used in everyday talk. For this I need to write software to analyse files of transcribed tape-recordings, and Perl is a very suitable language to use for this. Often I am well aware that the program I have written is not the most elegant possible solution to some task at hand, but so long as it works correctly I really don’t care. If some geeky type offered to show me how I could eliminate several lines of code, or make my program run twice as fast, by exploiting some little-known feature of the language which would yield a program delivering exactly the same results, I would not be very interested. Too many computing books are written by geeks who lose sight of the fact that, for the rest of us, computers are tools to get work done rather than ends in themselves. Making programs short is good if it makes them easier to grasp and hence easier to get right; but if brevity is achieved at the cost of obscurity, it is bad. As for speed: computer programs run so fast that, for most of us, speeding them up further would be pointless. (For every second of time my programs take to run, I probably spend a day thinking about the results they produce.) That does not mean that, in writing this book, I would have been justified in focusing only on those particular elements of Perl which happen to be useful in my own work and ignoring the rest – certainly not. Readers will have their own tasks for which they want to write software, which will often be very different from my tasks and will sometimes make heavy use of aspects of Perl that I rarely exploit. I aim to cover those aspects, as well as the ones which I use frequently. But it does mean that the book is oriented towards Perl programming as a practical tool – rather than as a labyrinth of fascinating intellectual arcana. If, after working through this book, you decide to make serious use of Perl, sooner or later you will need to consult some larger-scale Perl book – one organized more as a reference manual than a teaching introduction. This short book cannot pretend to cover the reference function, but there is a wide choice of books which do. (And of course there are plenty of online reference sources.) Many Perl users will not need to go all the way to Steven Holzner’s 1300-pager quoted above. The manual which I use constantly is a shorter one by the same author, Perl Core Language Little Black Book (second edn, Paraglyph Press, 2004) – I find Holzner’s approach particularly well suited to my own style of learning, but readers whose learning styles differ might find that other titles suit them better. Because the present book deliberately limits the aspects of Perl which it covers, it is important that readers should not fall into the trap of thinking “Doesn’t Perl have a such-and-such function, then? – that sounds like an awkward gap to have to work round”. Whatever such-and-such may be, very likely Perl has got it, but it is one of the things which this book has chosen not to cover. ﻿For the purposes of this textbook, I shall assume that you have access to a computer system on which Perl is available, and that you know how to log on to the system and get to a point where the system is displaying a prompt and inviting you to enter a command. Perl is free, and versions are available for all the usual operating systems, so if you are working in a multi-user environment such as a university computer centre then Perl is almost sure to be on your system already. (It would take us too far out of our way to go through the details of installing Perl on a home computer which does not already have it; though, if the home computer is a Mac running OS X, it will already have Perl – available from the Terminal utility under Applications Utilities.) Assuming, then, that you have access to Perl, let us get started by creating and running a very simple program.2 Adding two and two is perhaps as simple as it gets. This could be a very short Perl program indeed, but I’ll offer a slightly longer one which illustrates some basics of the language. First, create a file with the following contents. Use a text editor to create it, not a word-processing application such as Word – files created via WP apps contain a lot of extra, hidden material apart from the wording typed by the user and displayed on the screen, but we need a file containing just the characters shown below and no others.  Save it under some suitable name – twoandtwo.pl is as good a name as any. The .pl extension is optional – Perl itself does not care about the format of filenames, and it would respond to the program just the same if you called it simply twoandtwo – but some operating systems want to see filename extensions in some circumstances, so it is probably sensible to get in the habit of including .pl in the names of your Perl programs. Your twoandtwo.pl file will contain just what is shown above. But later in this book, when we look at more extended examples of Perl code I shall give them a label in brackets and number the lines, like this:  These labels will be purely for convenience in discussing the code, for instance I shall write “line 1.3” to identify the line print $b. The labels are not part of what you will type to create a program. However, when your programs grow longer you may find it helpful to create them using an editor which shows line-numbers; the error messages generated by the Perl interpreter will use line numbers to identify places where it finds problems.  In (1), the symbols $a and $b are variables – names for pigeonholes containing values (in this case, numbers). Line 1.1 means “assign the value 2 to the variable $a”. Line 1.2 means “assign the result of adding the value of $a to itself to the variable $b”. Line 1.3 means “display the value of $b”. Note that each instruction (the usual word is statement) ends in a semicolon. To run the program, enter the command  to which the system will respond (I’ll show system responses in italics) with  Actually, if your system prompt is, say, %, what you see will be  – since nothing in the twoandtwo.pl program has told the system to output a newline after displaying the result and before displaying the next prompt. For that matter, nothing in our little program has told the system how much precision to include in displaying the answer; rather than responding with 4, some systems might respond with 4.00000000000000 (which is a more precise way of saying the same thing). In due course we shall see how to include extra material in a program to deal with issues like these. For now, the point is that the job in hand has been correctly done.  If you have typed the code exactly as shown and Perl does not respond correctly (or at all) when you try running it, various system-dependent problems may be to blame. I assume that, where you are working, there will be someone responsible for telling you what is needed to run Perl on your local system. But meanwhile, I can offer two suggestions. It may be that your program needs to tell the system where the Perl interpreter is located (this is likely if you are seeing an error message suggesting that the command perl is not recognized). In that case it is worth trying the following. Include as the first line of your program this “magic line”:3  This will not be the right “magic line” for every system, but for many systems it will be. Secondly, if Perl appears to run without generating error messages, but outputs no result, or outputs material suggesting that it stopped reading your program before the end, it may be that your editor is supplying the wrong newline symbols – so that the sequence of lines looks to the system like one long line. That will often lead to problems; for instance, if the first line of your program is the above “magic line”, but Perl sees your whole program as one long line, then nothing will happen when you run it, because the Perl interpreter will only begin to operate on the line following the “magic line”. Set your editor to use Unix (decimal 10) newlines. If neither of these solutions works, then, sorry, you really will need to find that computer-support staff member to tell you how to run Perl on the particular system you are working at!  Let’s now go back to the contents of program (1). One point which may have surprised you about our first program is the dollar signs in the variable names $a and $b. Why not simply name our variables a and b? In many programming languages, these latter names would be fine, but in Perl they are not. One of the rules of Perl is that any variable name must begin with a special character identifying what kind of entity it is, and for individual variables – names for single separate pigeonholes, as opposed to names for whole sets of pigeonholes – the identifying character is a dollar sign.  ﻿Programming, in any language, involves creating named entities within the machine and manipulating them – using their values to calculate the value for a new entity, changing the values of existing entities, and so forth. Some languages recognize many different kinds of entity, and require the programmer to be very explicit and meticulous about “declaring” what entities he will use and what kind each one will be before anything is actually done with them.4 In C, for instance, if a variable represents a number, one must say what kind of number – whether an integer (a whole number) or a “floating-point number” (what in everyday life we call a decimal), and if the latter then to what degree of precision it is recorded. (Mathematically, a decimal may have any number of digits after the decimal point, but computers have to use approximations which round numbers off after some specific number of digits.) Perl is very free and easy about these things. It recognizes essentially just three types of entity: individual items, and two kinds of sets of items – arrays, and hashes. Individual entities are called scalars (for mathematical reasons which we can afford to ignore here – just think of “scalar” as Perl-ese for an individual data item); a scalar can have any kind of value – it can be a whole number, a decimal, a single character, a string of characters (for instance, an English word or sentence) … We have already seen that variable names representing scalars (the only variables we shall be considering for the time being) begin with the $ symbol; for arrays and hashes, which we shall discuss in chapters 12 and 17, the corresponding symbols are @ and % respectively.  Furthermore, Perl does not require us to declare entity names before using them. In the mini-program (1), the scalars $a and $b came into existence when they were assigned values; we gave no prior notice that these variable names were going to be used. In program (1), the variable $b ended up with the value 4. But, if we had added a further line:  then $b would have ceased to stand for a number and begun to stand for a character-string – both are scalars, so Perl is perfectly willing to switch between these different kinds of value. That does not mean that it is a good idea to do this in practice; as a programmer you will need to bear in mind what your different variable names are intended to represent, which might be hard to do if some of them switch between numerical and alphabetic values. But the fact that one can do this makes the point that Perl does not force us to be finicky about housekeeping details. Indeed, it is even legal to use a variable’s value before we have given it a value. If line 1.2 of (1) were changed to $b = $a + $c, then $b would be given the sum of 2 plus the previously-unmentioned scalar $c. Because $c has not been given a value by the programmer, its value will be taken as zero (so $b will end up with the value 2). Relying on Perl to initialize our variables in this way is definitely a bad idea – even if we need a particular variable to have the initial value zero, it is much less confusing in the long run to get into the habit of always saying so explicitly. But Perl will not force us to give our variables values before we use them. Because this free-and-easy programming ethos makes it tempting to fall into bad habits, Perl gives us a way of reminding ourselves to avoid them. We ran program (1) with the command:  The perl command can be modified by various options beginning with hyphens, one of which is -w for “give warnings”. If we ran the program using the command:  then, when Perl encounters the line $b = $a + $c in which $c is used without having been assigned a value, it will obey the instruction but will also print out a warning:  If a skilled programmer gets that warning, it is very likely to be because he thinks he has given $c a value but in fact has omitted to do so. And perl -w gives other warnings about things in our code which, while legal, might well be symptoms of programming errors. It is a good idea routinely to use perl -w to run your programs, and to modify the programs in response to warning messages until the warnings no longer appear – even if the programs seem to be giving the right results.  In program (1) we saw the operator +, which as you would expect takes a pair of numerical values and gives their sum. Likewise - is used as a minus sign. Some further operators (not a complete list, but the ones you are most likely to need) include: * multiplication / division ** exponentiation: 2 ** 3 means 23, i.e. eight These operators apply to numerical values, but others apply to character-strings. Notably, the full stop . represents concatenation (making one string out of two):  (Beware of possible confusion here. Some programming languages make the plus sign do double duty, to represent concatenation of strings as well as addition of numbers, but in Perl the plus sign is used only for numerical values.) Another string operator is x (the letter x), which is used to concatenate a string with itself a given number of times: "a" x 6 is equivalent to "aaaaaa", "pom" x 3 is equivalent to "pompompom". (And "pom" x 0 would yield the empty string – the length-zero string containing no characters – which is more straightforwardly specified as "".) Note, by the way, that for Perl a single character is just a string of length one – there is no difference, as there is for instance in C, between "a" and 'a', these are equivalent ways of representing the length-one string containing just the character a. However, single and double quotation marks are not always equivalent. Perl uses backslash as an escape character to create codes for string elements which would be awkward to type: for instance, \n represents a newline character, and \t a tab. Between double quotation marks these sequences are interpreted as codes: ﻿We have seen the word if used to control which instruction is executed next. Commonly, we want to do one thing in one case and another thing in a different case. An if can be followed by an elsif (or more than one elsif), with an else at the end to catch any remaining possibilities:  When any one of the tests is passed, the remaining tests are ignored; if $price is 200, then since 200 100 Perl will print It's expensive, and the message in 4.7 will not be printed even though it is also true that 200 > 0. Curly brackets are used to keep together the block of code to be executed if a test is passed. Notice that (unlike in some programming languages) even if the block contains just a single line of code, that line must still have curly brackets round it. The last statement before the } does not actually have to end in a semicolon, but it is sensible to include one anyway. We might want to modify our code by adding further statements, in which case it would be easy to overlook the need to add a missing semicolon.  Not everyone sets out the curly brackets on separate lines, as I did in (4) above. Within reason, Perl does not care where in a program we put whitespace (spaces, tabs, and newline characters). Obviously we cannot put a space in the middle of a number – 56237 cannot be written 56 237, or Perl would have no way to tell that it was all one number 8 – and likewise putting a space in the middle of a string within quotation marks turns it into a different string. But we can set the program out on the page however we please: around the basic elements such as numbers, strings, variable names, and brackets of different types, Perl will ignore extra whitespace. Perl will even supply implied spacing in many cases where elements are run together – thus ++ $a can alternatively be written ++$a. Because Perl does not enforce layout conventions (as some languages do), you need to choose some system and use it consistently – so that you can grasp the overall structure of your program listings at a glance. The main question is about how to indent blocks; different people use different conventions. First, you need to decide how much space you are going to use for one level of indentation (common choices are one tab, or two spaces). But then, where exactly should the indents go? Perl manuals often put the opening curly bracket on the line which introduces it, indent the contents of the block, and then place the closing curly bracket level with the beginning of that first line:  This takes fewer lines than other conventions, but it is not particularly easy to read, and it is perhaps illogical in placing the pair of brackets at unrelated positions. Alternatively, one can give both curly brackets lines of their own – in which case they either both line up under the start of the introducing line, or are both indented to align with their contents:  Whichever convention you choose, if you apply it consistently you can catch and correct programming errors as you type. You may have a block which is indented within a block that is itself indented within a top-level block. When you type what you thought was the final }, if it doesn’t align properly with the item which it ought to line up with in the first line, then something has gone wrong – perhaps one of your opening brackets has not been given a closing partner?  As for which of the three styles you choose, that is entirely up to you. According to Thomas Plum, a survey of programmers working with the similar language C found a slight majority favouring the last of the three conventions.9 That is the style used in this book. Indenting consistently also has an advantage when, inevitably, one’s program as first written turns out not to run correctly. A common debugging technique is to insert instructions to print out the values of particular variables at key points, so that one can check whether their values are as expected. Once the bugs are found and eliminated, we naturally want to eliminate these diagnostic lines too – we don’t want our program spewing out a lot of irrelevancies when it is running correctly. My practice is to write diagnostic lines unindented, so that they stand out visually in the middle of an indented block, making them easy to locate and delete.  The reason to adopt a consistent style for program layout is to make it easier for a human programmer to understand what is going on within a sea of program code – the computer itself does not care about the layout. Another aid to human understanding is comments: explanatory notes written by the programmer to himself (or to those who come after him and have to maintain his code) which the machine ignores. In Perl, comments begin with the hash character. A comment can be:  or it can be added to a line to the right of code intended for the computer:  Either way, everything from the hash symbol to the end of the line is ignored by the machine.  Earlier, we saw that Perl has various “operators” represented by mathematical-type symbols. Sometimes these are the same symbols used in familiar school maths, such as + for addition and - for subtraction; sometimes they are slightly different symbols adapted to the constraints of computer keyboards, such as * for multiplication and ** for raising to a power; and sometimes the symbols represent operations that we do not usually come across in maths lessons, e.g. “.” for concatenation.  Perl has many more built-in functions that could conveniently be represented by special symbols, though.10 Most are represented by alphabetic codes. For instance, taking the square root of a number is a standard arithmetic operation, but the usual mathematical symbol, √, is nothing like any character in the ASCII character-set, so instead Perl represents it as sqrt.  ﻿Sometimes we want to repeat an action, perhaps with variations. One way to do this is with the word for. Suppose we want to print out a hundred lines containing the messages:  Here is a code snippet which does that:  The brackets following for contain: a variable created for the purpose of this for loop and given an initial value; a condition for repeating the loop; and an action to be executed after each pass. The variable $i begins with the value 1, ++$i increments it by one on each pass, and the instruction within the curly brackets is executed for each value of $i until $i reaches 101, when control moves on to whatever follows the closing curly bracket. We saw earlier that, within double quotation marks, a symbol like \n is translated into what it stands for (newline, in this case), rather than being taken literally as the two characters \ followed by n. Similarly, a variable name such as $i is translated into its current value; the lines displayed by the code above read e.g. Next number is 3, not Next number is $i. If you really wanted the latter, you would need to “escape” the dollar sign:  The little examples in earlier chapters often ended with statements such as  In practice, it would usually be far preferable to write  so that the result appears on a line of its own, rather than jammed together with the next system prompt. Within the output of the above code snippet, 1 is not a “next” number but the first number. So we might want the message on the first line to read differently. By now, we know various ways to achieve that. Here are two – a straightforward, plodding way, and a more concise way:  or (quicker to type, though less clear when you come back to it weeks later):  Another way to set up a repeating loop is the while construction. Here is another code snippet which achieves the same as the two we have just looked at:  Here, $i is incremented within the loop body, and control falls out of the loop after the pass in which $i begins with the value 99. The while condition reads $i < 100, not $i <= 100: within the curly brackets, $i is incremented before its value is displayed, so if <= had been used in the while line, the lines displayed would have reached 101. The while construction is often used for reading input lines in from a text file, so the next chapter will show us how that is done.  In general, a file you want to get data into your program from will not necessarily be in the same directory as the program itself; it may have to be located by a pathname which could be long and complicated. The structure of pathnames differs between operating systems; if you are working in a Unix environment, for instance, the pathname might be something like:  Whatever pathnames look like in your computing environment, to read data into a Perl program you have to begin by defining a convenient handle which the program will use to stand for that pathname. For instance, if your program will be using only one input file, you might choose the handle INFILE (it is usual to use capitals for filehandles). The code:  says that, from now until we hit a line close(INFILE), any reference to INFILE in the program will be reading in data from the annualRecords file specified in the pathname.  Having “opened” a file for input, we use the symbol <> to actually read a line in. Thus:  will read in a line from the annualRecords file and assign that string of characters as the value of $a. A line from a multi-line file will terminate in one or more line-end characters, and the identity of these may depend on the system which created the file (different operating systems use different line-end characters). Commonly, before doing anything else with the line we will want to convert it into an ordinary string by removing the line-end characters, and the built-in function chomp() does that. This is an example of a function whose main purpose is to change its argument rather than to return a value; chomp() does in fact return a value, namely the number of line-end characters found and removed, but programs will often ignore that value – they will say e.g. chomp($line), rather than saying e.g. $n = chomp($line), with follow-up code using the value of $n. (If no filehandle is specified, $a = <> will read in from the keyboard – the program will wait for the user to type a sequence of characters ending in a newline, and will assign that sequence to $a.11) Assuming that we are reading data from a file rather than from the keyboard, what we often want to do is to read in the whole of the input file, line by line, doing something or other with each successive line. An easy way to achieve that is like this:  The word while tests for the truth of a condition; in this case, it tests whether the assignment statement, and hence the expression <INFILE>, is true or false. So long as lines are being read in from the input file, <INFILE> counts as “true”, but when the file is exhausted <INFILE> will give the value “false”. Hence while ($a = <INFILE>) assigns each line of the input file in turn to $a, and ceases reading when there is nothing more to read. (It is a good idea then to include an explicit close(INFILE) statement, though that is not strictly necessary.) Our open … statement assumed that the annualRecords file was waiting ready to be opened at the place identified by the pathname. But, of course, that kind of assumption is liable to be confounded! Even supposing we copied the pathname accurately when we typed out the program, if that was a while ago then perhaps the annualRecords file has subsequently been moved, or even deleted. In practice it is virtually mandatory, whenever we try to open a file, to provide for the possibility that it does not get opened – normally, by using a die statement, which causes the program to terminate after printing a message about the problem encountered. A good way to code the open statement will be: ﻿Readers will appreciate that the concept of property is crucial for business. A firm needs to know what it owns (and can therefore use freely, and/or charge others who want to use it), and what belongs to others (so that if it needs to use those things it must do deals with their respective owners). Business looks to law to define property rights and enable them to be enforced. Before the IT revolution, the things over which firms needed to assert ownership were usually tangible things – goods, land, and so forth. The law of “intellectual property”, under which for instance a company might own a patent on a newly-devised industrial process, was a fairly obscure legal backwater. Information technology has changed this, by hugely raising the profile of intangibles. Ever since the Industrial Revolution, the economies of nations like Britain and the USA had been dominated by manufacturing. But by the late 1980s, the share of GDP (gross domestic product) attributable to manufacturing fell below half in both nations, and it has continued to fall – outweighed partly by growth in services, but also by growth in trading of intangibles. By now, intangibles form a large proportion of the assets of a typical firm, as measured by the prices which the market sets on them. Gordon Brown, then Chancellor of the Exchequer, said in 2006: Twenty-five years ago the market value of our top companies was no more than the value of just their physical assets. Today the market value of Britain’s top companies is five times their physical assets, demonstrating the economic power of knowledge, ideas and innovation.20 What Brown was saying was that most property of the “top companies” is now intellectual property. It is largely IT which has brought about this change; and it naturally means that intellectual property law has become a very significant area of business law, which is having to develop in response to developments in the technology. The topic which might perhaps come first to a student reader’s mind is the way that sharing music over peer-to-peer networks has been undermining the copyrights owned by music companies, which have been struggling either to invoke the law to defend their position, or to develop novel business models that allow them to make money within the new technological environment. But for present purposes, this area is not actually very significant. The law of copyright as it applies to music is clear; the only change introduced by IT lies in making the law easy to break and hard to enforce. More interesting, for this textbook, are areas where the property itself (not just the means used to reproduce it or move it around) consists of things like computer software or electronic databanks. In those areas, it is often far from clear how the existing laws of intellectual property apply. Courts are adapting laws that were written long ago for other purposes in order to develop an intellectual-property régime for the IT industry, and so far this is not working too well. The issues are not about enforcement – unlike with music filesharing, where many of the individuals involved do not care whether their activity is legal, provided they feel safe from detection! In civilized societies, most organizations by and large aim to keep within the law and respect one another’s property rights – but they need to know what those rights are. It would be hard for a business to be profitable, if it made a habit of not insisting on rights which it did legally possess. There are two longstanding legal devices for defining and protecting different sorts of intellectual property: copyright, and patent. Copyright was originally introduced to define ownership in “literary works”, such as novels, poems, or non-fiction books, but came to be extended by analogy to things like musical compositions, films, and so forth. Patents relate to newly-invented machines or industrial processes. Neither copyright nor patent law was part of the Common Law; both devices were introduced by statute. (Indeed, the USA has had a general law of copyright only since the 1890s – it was a standing grievance for Victorian novelists that no sooner did the fruits of their labour emerge from the press than American publishers’ agents would rush single copies across the Atlantic, where they would be reprinted and sold without reward to the author.) The original motivation of both copyright and patent law was the same: they were intended to stimulate advances (in literature, and in industry) which would benefit society, by creating concrete incentives for the innovators. The kinds of protection offered by the two areas of law are different. Copyright is something that the author of a “literary work” acquires automatically in producing the work, and it forbids anyone else to make a copy of the work (for a set number of years into the future, and with various provisos that do not matter here) without the right-holder’s permission. Thus an author’s copyright is a piece of property which he can sell or license for money; in the case of books, typically a publishing company contracts with an author for permission to publish his book in exchange for royalties paid to him on copies sold. With newer media such as films, the business models are different, but the underlying law (which is what concerns us) is essentially the same. A patent, on the other hand, is not acquired automatically by the inventor (or anyone else). Taking out a patent is a complicated and expensive undertaking, but if a patent is granted, it forbids anyone (again, for a set future period) from exploiting the process or mechanism without the patent-holder’s permission; so again the patent is an economically-valuable piece of property, which can be sold or licensed to companies wanting to use the innovation in their business. The legal contrast between copyright and patent was neatly summed up by Tim Press: A document setting out a novel chemical process would attract copyright protection, but that protection would protect the document against copying, not the process from being carried out. A patent for the process would prevent it from being carried out but not from being written about or broadcast.21 Computer programs are “text” which defines and controls “processes”. So on the face of it there is a question about which kind of intellectual-property protection is more relevant to software. Over the years during which IT has been economically important, the answer has been shifting.$$$﻿Readers will appreciate that the concept of property is crucial for business. A firm needs to know what it owns (and can therefore use freely, and/or charge others who want to use it), and what belongs to others (so that if it needs to use those things it must do deals with their respective owners). Business looks to law to define property rights and enable them to be enforced. Before the IT revolution, the things over which firms needed to assert ownership were usually tangible things – goods, land, and so forth. The law of “intellectual property”, under which for instance a company might own a patent on a newly-devised industrial process, was a fairly obscure legal backwater. Information technology has changed this, by hugely raising the profile of intangibles. Ever since the Industrial Revolution, the economies of nations like Britain and the USA had been dominated by manufacturing. But by the late 1980s, the share of GDP (gross domestic product) attributable to manufacturing fell below half in both nations, and it has continued to fall – outweighed partly by growth in services, but also by growth in trading of intangibles. By now, intangibles form a large proportion of the assets of a typical firm, as measured by the prices which the market sets on them. Gordon Brown, then Chancellor of the Exchequer, said in 2006: Twenty-five years ago the market value of our top companies was no more than the value of just their physical assets. Today the market value of Britain’s top companies is five times their physical assets, demonstrating the economic power of knowledge, ideas and innovation.20 What Brown was saying was that most property of the “top companies” is now intellectual property. It is largely IT which has brought about this change; and it naturally means that intellectual property law has become a very significant area of business law, which is having to develop in response to developments in the technology. The topic which might perhaps come first to a student reader’s mind is the way that sharing music over peer-to-peer networks has been undermining the copyrights owned by music companies, which have been struggling either to invoke the law to defend their position, or to develop novel business models that allow them to make money within the new technological environment. But for present purposes, this area is not actually very significant. The law of copyright as it applies to music is clear; the only change introduced by IT lies in making the law easy to break and hard to enforce. More interesting, for this textbook, are areas where the property itself (not just the means used to reproduce it or move it around) consists of things like computer software or electronic databanks. In those areas, it is often far from clear how the existing laws of intellectual property apply. Courts are adapting laws that were written long ago for other purposes in order to develop an intellectual-property régime for the IT industry, and so far this is not working too well. The issues are not about enforcement – unlike with music filesharing, where many of the individuals involved do not care whether their activity is legal, provided they feel safe from detection! In civilized societies, most organizations by and large aim to keep within the law and respect one another’s property rights – but they need to know what those rights are. It would be hard for a business to be profitable, if it made a habit of not insisting on rights which it did legally possess. There are two longstanding legal devices for defining and protecting different sorts of intellectual property: copyright, and patent. Copyright was originally introduced to define ownership in “literary works”, such as novels, poems, or non-fiction books, but came to be extended by analogy to things like musical compositions, films, and so forth. Patents relate to newly-invented machines or industrial processes. Neither copyright nor patent law was part of the Common Law; both devices were introduced by statute. (Indeed, the USA has had a general law of copyright only since the 1890s – it was a standing grievance for Victorian novelists that no sooner did the fruits of their labour emerge from the press than American publishers’ agents would rush single copies across the Atlantic, where they would be reprinted and sold without reward to the author.) The original motivation of both copyright and patent law was the same: they were intended to stimulate advances (in literature, and in industry) which would benefit society, by creating concrete incentives for the innovators. The kinds of protection offered by the two areas of law are different. Copyright is something that the author of a “literary work” acquires automatically in producing the work, and it forbids anyone else to make a copy of the work (for a set number of years into the future, and with various provisos that do not matter here) without the right-holder’s permission. Thus an author’s copyright is a piece of property which he can sell or license for money; in the case of books, typically a publishing company contracts with an author for permission to publish his book in exchange for royalties paid to him on copies sold. With newer media such as films, the business models are different, but the underlying law (which is what concerns us) is essentially the same. A patent, on the other hand, is not acquired automatically by the inventor (or anyone else). Taking out a patent is a complicated and expensive undertaking, but if a patent is granted, it forbids anyone (again, for a set future period) from exploiting the process or mechanism without the patent-holder’s permission; so again the patent is an economically-valuable piece of property, which can be sold or licensed to companies wanting to use the innovation in their business. The legal contrast between copyright and patent was neatly summed up by Tim Press: A document setting out a novel chemical process would attract copyright protection, but that protection would protect the document against copying, not the process from being carried out. A patent for the process would prevent it from being carried out but not from being written about or broadcast.21 Computer programs are “text” which defines and controls “processes”. So on the face of it there is a question about which kind of intellectual-property protection is more relevant to software. Over the years during which IT has been economically important, the answer has been shifting.
EN31	1	﻿A regular feature in the New Scientist magazine is Enigma, a weekly puzzle entry which readers are invited to solve. In the 8 February 2003 issue [1] the following puzzle was published. First, draw a chessboard. Now number the horizontal rows 1, 2, ..., 8, from top to bottom and number the vertical columns 1, 2, ..., 8, from left to right.You have to put a whole number in each of the sixty-four squares, subject to the following: 1. No two rows are exactly the same. 2. Each row is equal to one of the columns, but not to the column with the same number as the row. 3. If N is the largest number you write on the chessboard then you must also write 1, 2, ...,N −1 on the chessboard. The sum of the sixty-four numbers you write on the chessboard is called your total. What is the largest total you can obtain? We are going to solve this puzzle here using Prolog. The solution to be described will illustrate two techniques: unification and generate-and-test. Unification is a built-in pattern matching mechanism in Prolog which has been used in [9]; for example, the difference list technique essentially depended on it. For our approach here, unification will again be crucial in that the proposed method of solution hinges on the availability of built-in unification. It will be used as a kind of concise symbolic pattern generating facility without which the current approach wouldn’t be viable. Generate-and-test is easily implemented in Prolog. Prolog’s backtracking mechanism is used to generate candidate solutions to the problem which then are tested to see whether certain of the problem-specific constraints are satisfied. Fig. 1.1 shows a board arrangement with all required constraints satisfied. It is seen that the first requirement is satisfied since the rows are all distinct. The second condition is also seen to hold whereby rows and columns are interrelated in the following fashion: We use the permutation to denote the corresponding column–to–row transformation. The board also satisfies the latter part of the second condition since no row is mapped to a column in the same position. In terms of permutations, this requirement implies that no entry remains fixed; these are those permutations which in our context are permissible. 2 The third condition is obviously also satisfied with N = 6. The board’s total is 301, not the maximum, which, as we shall see later, is 544. The solution scheme described below in i–v is based on first generating all feasible solutions (an example of which was seen in Sect. 1.2) and then choosing a one with the maximum total. i. Take an admissible permutation, such as π in (1.1). ii. Find an 8 ×8 matrix with symbolic entries whose rows and columns are interrelated by the permutation in i. As an example, let us consider for the permutation π two such matrices, M1 and M2, with M1 and M2 both satisfy conditions 1 and 2. We also observe that the pattern of M2 may be obtained from that of M1 by specialization (by matching the variables X1 and X6). Thus, any total achievable for M2 is also achievable for M1. For any given permissible permutation, we can therefore concentrate on the most general pattern of variables, M. (We term a pattern of variables most general if it cannot be obtained by specialization from a more general one.) All this is reminiscent of ‘unification’ and the ‘most general unifier’, and we will indeed be using Prolog’s unification mechanism in this step. iii. Verify condition 1 for the symbolic matrix M. 3 Once this test is passed, we are sure that also the latter part of condition 2 is satisfied. 4 iv. We now evaluate the pattern M. If N symbols have been used in M, assign the values 1, ...,N to them in reverse order by first assigning N to the most frequently occurring symbol, N − 1 to the second most frequently occurring symbol etc. The total thus achieved will be a maximum for the given pattern M. v. The problem is finally solved by generating and evaluating all patterns according to i–iv and selecting a one with the maximum total. The original formulation from the New Scientist uses a chessboard but the problem can be equally set with a square board of any size. In our implementation, we shall allow for any board size since this will allow the limitations of the method employed to be explored. We write matrices in Prolog as lists of their rows which themselves are lists. Permutations will be represented by the list of the bottom entries of their two-line representation; thus, [2, 3, 1, 5, 6, 7, 8, 4] stands for π in (1.1). First, we want to generate all permutations of a list. Let us assume that we want to do this by the predicate permute(+List,-Perm) and let us see how List = [1, 2, 3, 4] might be permuted. A permuted list, Perm = [3, 4, 1, 2] say, may be obtained by • Removing from List the entry E = 3, leaving the reduced list R = [1, 2, 4] • Permuting the reduced list R to get P = [4, 1, 2] • Assembling the permuted list as [E|P] = [3, 4, 1, 2] . Lists with a single entry are left unchanged. This gives rise to the definition with the predicate remove one(+List,?Entry,?Reduced) defined by (Here we remove either the head or an entry from the tail.) For a permutation to be admissible, all entries must have changed position. We implement this by To generate a list of N unbound variables, L, we use var list(+N,-L) which is defined in terms of length(-L,+N) By Matrices with distinct symbolic entries may now be produced by mapping; for example, a 3 × 2 matrix is obtained by It is now that Prolog shows its true strength: we use unification to generate symbolic square matrices with certain patterns.5 For example, we may produce a 3 × 3 symmetric matrix thus ﻿Many problems in Artificial Intelligence (AI) can be formulated as network search problems. The crudest algorithms for solving problems of this kind, the so called blind search algorithms, use the network’s connectivity information only. We are going to consider examples, applications and Prolog implementations of blind search algorithms in this chapter. Since implementing solutions of problems based on search usually involves code of some complexity, modularization will enhance clarity, code reusability and readibility. In preparation for these more complex tasks in this chapter, Prolog’s module system will be discussed in the next section. In some (mostly larger) applications there will be a need to use several input files for a Prolog project. We have met an example thereof already in Fig. 3.5 of [9, p. 85] where consult/1 was used as a directive to include in the database definitions of predicates from other than the top level source file. As a result, all predicates thus defined became visible to the user: had we wished to introduce some further predicates, we would have had to choose the names so as to avoid those already used. Clearly, there are situations where it is preferable to make available (that is, to export ) only those predicates to the outside world which will be used by other non-local predicates and to hide the rest. This can be achieved by the built-in predicates module/2 and use module/1 . As an illustrative example, consider the network in Fig. 2.1.1 The network connectivity in links.pl is defined by the predicate link/2 which uses the auxiliary predicate connect/2 (Fig. 2.2). The first line of links.pl is the module directive indicating that the module name is edges and that the predicate link/2 is to be exported. All other predicates defined in links.pl (here: connect/2) are local to the module and (normally) not visible outside this module. Suppose now that in some other source file, link/2 is used in the definition of some new predicate (Fig. 2.3). Then, the (visible) predicates from links.pl will be imported by means of the directive The new predicate thus defined may be used as usual: In our example, the predicate connect/2 will not be available for use (since it is local to the module edges that resides in links.pl). A local predicate may be accessed, however, by prefixing its name by the module name in the following fashion:3 Let us assume that for the network in Fig. 2.1 we want to find a path from the start node s to the goal node g. The search may be conducted by using the (associated) search tree shown in Fig. 2.4. It is seen that the search tree is infinite but highly repetitive. The start node s is at the root node (level 0). At level 1, all tree nodes are labelled by those network nodes which can be reached in one step from the start node. In general, a node labelled n in the tree at level _ has successor (or child ) nodes labelled s1, s2, . . . if the nodes s1, s2, . . . in the network can be reached in one step from node n. These successor nodes are said to be at level _ + 1. The node labelled n is said to be a parent of the nodes s1, s2, . . .. In Fig. 2.4, to avoid repetition, those parts of the tree which can be generated by expanding a node from some level above have been omitted. Some Further Terminology • The connections between the nodes in a network are called links. • The connections in a tree are called branches. • In a tree, a node is said to be the ancestor of another if there is a chain of branches (upwards) which connects the latter node to the former. In a tree, a node is said to be a descendant of another node if the latter is an ancestor of the former. In Fig. 2.5 we show, for later reference, the fully developed (and ’pruned’) search tree. It is obtained from Fig. 2.4 by arranging that in any chain of branches (corresponding to a path in the network) there should be no two nodes with the same label (implying that in the network no node be visited more than once). All information pertinent to the present problem is recorded thus in the file links.pl (Fig. 2.2) by link/2. Notice that the order in which child nodes are generated by link/2 will govern the development of the trees in Figs. 2.4 and 2.5: children of the same node are written down from left to right in the order as they would be obtained by backtracking; for example, the node labelled d at level 1 in Fig. 2.4 is expanded by (The same may be deduced, of course, by inspection from links.pl, Fig. 2.2.) link/2 will serve as input to the implementations of the search algorithms to be discussed next. The most concise and easy to remember illustration of Depth First is by the conduit model (Fig. 2.6). We start with the search tree in Fig. 2.5 which is assumed to be a network of pipes with inlet at the root node s. The tree is rotated by 90◦ counterclockwise and connected to a valve which is initially closed. The valve is then opened and the system is observed as it gets flooded under the influence of gravity. The order in which the nodes are wetted corresponds to Depth First. We may be tempted to use Prolog’s backtracking mechanism to furnish a solution by recursion; our attempt is shown in Fig. 2.7.4 However, it turns out that the implementation does not work due to cycling in the network. The query shown below illustrates the problems arising. We implement Depth First search incrementally using a new approach. The idea is keeping track of the nodes to be visited by means of a list, the so called list of open nodes, also called the agenda. This book–keeping measure will turn out to be amenable to generalization; in fact, it will be seen that the various search algorithms differ only in the way the agenda is updated. ﻿In this chapter we are going to discuss graph search algorithms and applications thereof for finding a minimum cost path from a start node to the goal node. The network search problem in Sect. 2.2 (Fig. 2.1) was devoid of any cost information. Let us now assume that the costs to traverse the edges of the graph in Fig. 2.1 are as indicated in Fig. 3.1. There are two possible interpretations of the figures in Fig. 3.1: they can be thought of as costs of edge traversal or, alternatively, as edge lengths. (We prefer the latter interpretation in which case, of course, Fig. 3.1 is not to scale.) The task is to determine a minimum length path connecting s and g, or, more generally, minimum length paths connecting any two nodes. The algorithms considered in this chapter assume the knowledge of an heuristic distance measure, H, between nodes. Values of H for the network in Fig. 3.1 are shown in Table 3.1. They are taken to be the estimated straight line distances between nodes and may be obtained by drawing the network in Fig. 3.1 to scale and taking measurements. Three algorithms will be introduced here: the A–Algorithm, Iterative Deepening A∗ and Iterative Deepening A∗–_. An estimated overall cost measure, calculated by the heuristic evaluation function F, will be attached to every path; it is represented as where G is the actual cost incurred thus far by travelling from the start node to the current node and H, the heuristic, is the estimated cost of getting from the current node to the goal node. Assume, for example, that in the network shown in Fig. 3.1 we start in d and want to end up in c. Equation (3.1) then reads for the path d → s → a (with obvious notation) as follows We know from Chap. 2 that for blind search algorithms the updating of the agenda is crucial: Breadth First comes about by appending the list of extended paths to the list of open paths; Depth First requires these lists to be concatenated the other way round. For the A–Algorithm, the updating of the agenda is equally important. The new agenda is obtained from the old one in the steps 1 _ and 2 _ below. 1 _ Extend the head of the old agenda to get a list of successor paths. An intermediate, ‘working’ list will be formed by appending the tail of the old agenda to this list. 2 _ The new agenda is obtained by sorting the paths in the working list from 1 _ in ascending order of their F–values. 3 _ The steps 1 _ and 2 _ are iterated until the path at the head of the agenda leads to the goal node. In the example shown in Fig. 3.2, the paths are prefixed by their respective F–values and postfixed by their respective G–values. Using this notation and the cost information, the example path in (3.2) is now denoted by 242 − [a, s, d] − 147. Notice that this path also features in Fig. 3.2. It can be shown (e.g. [23]) that if the heuristic H is admissible, i.e. it never overestimates the actual minimum distance travelled between two nodes, the A–Algorithm will deliver a minimum cost path if such a path exists.1In this case the A–Algorithm is referred to as an A∗–Algorithm and is termed admissible. (As the straight line distance is a minimum, the heuristic defined by Table 3.1 is admissible.) The predicate a search(+Start,+Goal,-PathFound) in asearches.pl implements the A–Algorithm. A few salient features of a search/3 will be discussed only; for details, the reader is referred to the source code which broadly follows the pattern of implementation of the blind search algorithms (Fig. 2.15, p. 65 and Fig. 2.20, p. 69). The implementation of the A–Algorithm in asearches.pl uses the built-in predicate keysort/2 to implement step 2 _ (see inset on p. 108). The module invoking a search/3 should have defined (or imported) the following predicates. • The connectivity predicate link/2 . For the network search problem, this is imported from links.pl (Fig. 2.2, p. 49). • The estimated cost defined by e cost/3 . For the network search problem, this is defined in graph a.pl by with dist/3 essentially implementing Table 3.1, The actual edge costs defined by edge cost/3 . For the network search problem, this is defined in graph a.pl by Application of the A–Algorithm to a more substantial example in Sect. 3.2 will reveal that the A–Algorithm may fail due to excessive memory requirements.2 Clearly, there is scope for improvement. In the mid 1980s, a new algorithm was conceived by Korf [20] combining the idea of Iterative Deepening (Sect. 2.6) with a heuristic evaluation function; the resulting algorithm is known as Iterative Deepening A∗ (IDA∗).3 The underlying idea is as follows. • Use Depth First as the ‘core’ of the algorithm. • Convert the core into a kind of Bounded Depth First Search with the bound (the horizon) now not being imposed on the length of the paths but on their F-values. • Finally, imbed this ‘modified’ Bounded Depth First Search into a framework which repeatedly invokes it with a sequence of increasing bounds. The corresponding sequence of bounds in Iterative Deepening was defined as a sequence of multiples of some constant increment; a unit increment in the model implementation. The approach here is more sophisticated. Now, in any given phase of the iteration, the next value of the bound is obtained as the minimum of the F-values of all those paths which had to be ignored in the present phase. This approach ensures that in the new iteration cycle the least number of paths is extended. The pseudocode of IDA∗ won’t be given here; it should be possible to reconstruct it from the above informal description. It can be shown that IDA∗ is admissible under the same assumptions as A∗. The so-called _–admissible version of IDA∗ (IDA∗–_) is a generalization of IDA∗. It is obtained by extending the F-horizon to ﻿Whereas the problems considered thus far were taken from Artificial Intelligence, we are going now to apply Prolog to problems in text processing. The present chapter is in three parts. First, the Prolog implementation is described of a tool for removing from a file sections of text situated between marker strings. (The tool is therefore a primitive static program slicer; [32] and [12].) This tool then is used in a practical context for removing sample solutions from the LATEX source code of a solved exam script. It is also shown in this context how SWI-Prolog code can be embedded into a Linux shell script. The second part addresses the question of how Prolog can be used to generate LATEX code for drawing parametric curves. Some new features of Prolog will thereby also be introduced. The final part comprises a sequence of solved Prolog exercises, implementing a tool for drawing families of parametric curves in LATEX. The exercises are of increasing complexity and finally describe how SWI-Prolog can interact with Linux through a shell script. I use LATEX on Linux for preparing examination papers. This is done in the following steps. 1. Create a LATEX source file in a text editor. 2. Translate the LATEX file into a a DVI file. 3. Translate the DVI file into a PDF file. 4. View the PDF file. These steps are performed for exam.tex by running the Linux commands in Fig. 4.1.1 Upon execution of the last line in Fig. 4.1, a new window will pop up and the exam paper may be viewed. External examiners require examination papers with model answers. I create therefore a PDF file with model solutions in the first instance where answers are appended to each subquestion. The answers are placed between some marker strings enabling me eventually to locate and remove all text between them when creating the final LATEX source leading to the printed PDF for students. It is this text removal process which is automated by the Prolog implementation to be discussed here. Write a predicate sieve(+Infile,-Outfile,+Startmarker,+Endmarker) of arity 4 for removing all text in the file named in Infile in between all occurrences of lines starting with text in Startmarker and those starting with text in Endmarker. The result should be saved in the file named in Outfile . Outfile is without marker lines. If Outfile already exists, its old version should be overwritten, if it does not exist, it should be newly created. The file shown in Fig. 4.2 is an example of Infile with the marker phrases ‘water st’ and ‘water e’, say. (The file comprises a random collection of geographical names.) After the Prolog query the file without_waters will have been created. This is shown in Fig. 4.3. The main predicate sieve/4 is defined in terms of sieve/2 , both are shown in (P-4.1). The predicates get line/1 (and its auxiliary get line/2 ), switch off/1 and switch on/1 are defined in (P-4.2). For the SWI-Prolog built-ins atom chars/2 and atom codes/2 , the reader is referred respectively to pages 126 and 19 of [9]. Noteworthy are three more built-in predicates used here: the standard Prolog predicates see/1 , seen/0 (respectively for directing the input stream to a file and redirecting it) and get char/1 for reading a character; the example below illustrates their use by reading the first three characters of the file with_waters in Fig. 4.2. The predicate get line/1 in (P-4.2) is defined in terms of get line/2 by the accumulator technique. It reads into its argument the next line from the input stream. Example: The following observations apply. 1. It is seen from the above query that a line read by get line/1 is represented as a list of the characters it is composed of. 2. By definition the last character of each line in a file is the new line character ‘\n’. That explains the line break seen in the above query. 3. Finally (not demonstrated here), each file ends with the end-of-file marker ‘end_of_file’. The one-entry list [end_of_file] is deemed to be the last line of every file by the definition in (P-4.2). • The switches switch off/0 and switch on/0 are used, writing respectively switch(off) and switch(on) in the Prolog database, respectively for removal and retention of lines from the input file. • The main predicates are sieve/4 and sieve/2 in (P-4.1), the latter defined by recursion and called by the former. sieve/4 : this is the top level predicate. 1. Line 2 opens the input file. 2. The goals in lines 3-4 in (P-4.1) make sure that the earlier version of the output file (if there is such a file) is deleted. 3. In line 5, the new output stream is opened via append/1 3. 4. In line 6, the switch is set to the position (‘off’), anticipating that initially lines will be retained. 5. In line 7, sieve/2 is invoked and processing is carried out. 6. Lines 8 and 9 close respectively output and input. sieve/2 : this is called from sieve/4 . 1. Lines 14 and 18 contain the most interesting feature of this predicate: append/3 is used in them for pattern matching. For example, the goal succeeds if the initial segment of the list Line is Start_List. 2. atom chars/2 is used in sieve/2 to disassemble the start and end markers into lists in preparation for pattern matching. 3. Notice that the built-in predicate atom codes/2 can be used in two roles as the interactive session below demonstrates. In line 16 of (P-4.1), atom codes/2 is used in its first role, i.e. to convert a list of characters to an atom. This atom is the current line, it is written to the output file. 4. Recursion is stopped in sieve/2 (and control is returned to line 8 of sieve/4 ) when the end-of-file marker is read (line 15). Imbed the Prolog implementation from Sect. 4.1.3 into a Linux shell script for providing the same functionality as the predicate sieve/4 does. The application obtained thereby will run without explicitly having to use the SWI-Prolog system. The intended behaviour of the script is illustrated in Fig. 4.4. ﻿Six people are seated at a round table as shown in Fig. 3.1. The predicate right to/2 , defined in (P-3.1) by six facts, describes the seating arrangement in an obvious fashion. (a) Who is seated to the right of Adam? (b) To whom is Clara the right neighbour? (c) Who are the neighbours of George? Define Prolog rules for (d) ”... is seated to the left of ...” (e) ”... are the neighbours of ...” (f) ”... is seated opposite to ...” Hints. The envisaged solution for this exercise is elementary and concise and should make no use of lists. The following is suggested for solving part (f): • If we want to find the person seated opposite to Adam, say, it will help to imagine that the party are seated not at a round table but at a long rectangular one at the head of which is seated Adam (Fig. 3.2). • Define an auxiliary predicate facing/3 returning all pairs of people facing each other from one particular person’s point of view (here: Adam’s), and, eventually, facing that person himself. facing/3 should respond as follows. • Now implement opposite to/2 using facing/3 . • opposite to/2 should fail if the number of people around the table is odd. (a) Write a predicate guests/0 for displaying the names of all those at the table. (Use a failure driven loop; see inset on p. 77.) guests/0 should fail only if there aren’t any people at the table. (b) Use a failure driven loop to define a predicate opposites/0 for displaying all pairs seated opposite each other: (c) Use the accumulator technique to define a predicate look right(+Person) for displaying all the guests’ names counterclockwise, starting with a particular person. Example: Initially, we will have read the facts in (P-3.1), p. 75, into memory by consult/1 (or by some equivalent thereof). It is important at this stage to remember that the database comprises all predicates loaded in memory; these will be those defined by the user as well as the built-in ones. Let us now assume that we want to model the departure from, and the arrival to, the table of people by updating the database. Departures. Departures will obviously involve removal of clauses from the database. To model, for example, George’s departure, we shall have to remove all facts referencing George. In addition, former neighbours of George will now be seated next to each other, necessitating additions to the database. Thus, to record departures, we shall need both deletion from, and addition to, the database. Arrivals. Arrivals will clearly involve an augmentation of the definition of right to/2 by new facts. To model for example the arrival of Tracy and Joe, to be seated between Adam and Susan, we will have to add the three facts in (P-3.2) to the database. And, we will have to remove the fact indicating that Susan is Adam’s right-hand neighbour: Therefore, to account for arrivals, both deletion from, and addition to the database will need to be done. We now review a few basic built-in predicates for modifying the database. • We use retract/1 (or retractall/1) to remove a clause (or all clauses of a predicate) from the database. The predicate whose clause is retracted has to be declared dynamic , implemented either as a directive in one of the source files or by calling dynamic/1 as a goal just before retracting. This is achieved in our example either by including in one of the files consulted the directive or interactively by • We use assert/1 to add a new clause to the database. A predicate newly introduced by assert/1 is deemed dynamic. An existing static (i.e. non-dynamic) predicate may be augmented by a new clause via assert/1 only after declaring it dynamic. • retractall/1 is used to remove from the database all clauses whose head unifies with the pattern in its argument. As with retract/1, retractall/1 may revoke dynamic predicates only. The following queries may be used to achieve the intended changes to the database. • George leaves the table (Fig. 3.3). As is easily confirmed by the query ?- listing(right to/2). , the predicate right to/2 is now defined in the database by the facts in (P-3.3). (Notice, however, that the definition of right to/2 in its Prolog source file is not yet affected.) • Tracy and Joe join the table and are seated between Adam and Susan (Fig. 3.4). Notice that due to the previous query the predicate right to/2 is now dynamic. It is now defined in the database by the facts in (P-3.4).3 It is seen that assert/1 places the new clause behind the existing ones for the same predicate.4 Exercise 3.3. Thus far, we have carried out (for reasons of transparency) database changes interactively only. In this exercise, you are asked to define some predicates for manipulating the database. (a) Define a predicate swap neighbours(+Left,+Right) for recording in the database of two neighbours swapping places. (For this predicate to succeed, prior to the swap, the person named in Left should be seated to the left of the person named in Right .) If we assume, for example, that the seating arrangement is initially as shown in Fig. 3.1, then the swap of Clara and Adam will be accomplished by After this, the database will look as follows. (b) Define a predicate swap(+Person1,+Person2) for recording in the database of two people swapping places who need not be neighbours. To exemplify, assume again that the database is initially as shown in Fig. 3.1. Then, Adam and George’s swap is carried out by upon which the database is as shown below. Note. You may use the predicate swap neighbours/2 from part (a) in your definition of swap/2 Exercise 3.4. (Modelling a queue)5 A queue with at least two customers at a checkout is modelled by the Prolog predicate behind/2 which is defined in the file queue.pl as shown below. (behind/2 is declared a dynamic predicate in queue.pl.) (These facts have an obvious interpretation: the person named in the second argument stands behind the person named in the first argument.) (a) Define a predicate swap neighbours(+Person1,+Person2) for recording in the database of two neighbours swapping places. (For this predicate to succeed, prior to the swap, the person named in Person2 should be standing behind the person named in Person1 .) Example:$$$﻿Six people are seated at a round table as shown in Fig. 3.1. The predicate right to/2 , defined in (P-3.1) by six facts, describes the seating arrangement in an obvious fashion. (a) Who is seated to the right of Adam? (b) To whom is Clara the right neighbour? (c) Who are the neighbours of George? Define Prolog rules for (d) ”... is seated to the left of ...” (e) ”... are the neighbours of ...” (f) ”... is seated opposite to ...” Hints. The envisaged solution for this exercise is elementary and concise and should make no use of lists. The following is suggested for solving part (f): • If we want to find the person seated opposite to Adam, say, it will help to imagine that the party are seated not at a round table but at a long rectangular one at the head of which is seated Adam (Fig. 3.2). • Define an auxiliary predicate facing/3 returning all pairs of people facing each other from one particular person’s point of view (here: Adam’s), and, eventually, facing that person himself. facing/3 should respond as follows. • Now implement opposite to/2 using facing/3 . • opposite to/2 should fail if the number of people around the table is odd. (a) Write a predicate guests/0 for displaying the names of all those at the table. (Use a failure driven loop; see inset on p. 77.) guests/0 should fail only if there aren’t any people at the table. (b) Use a failure driven loop to define a predicate opposites/0 for displaying all pairs seated opposite each other: (c) Use the accumulator technique to define a predicate look right(+Person) for displaying all the guests’ names counterclockwise, starting with a particular person. Example: Initially, we will have read the facts in (P-3.1), p. 75, into memory by consult/1 (or by some equivalent thereof). It is important at this stage to remember that the database comprises all predicates loaded in memory; these will be those defined by the user as well as the built-in ones. Let us now assume that we want to model the departure from, and the arrival to, the table of people by updating the database. Departures. Departures will obviously involve removal of clauses from the database. To model, for example, George’s departure, we shall have to remove all facts referencing George. In addition, former neighbours of George will now be seated next to each other, necessitating additions to the database. Thus, to record departures, we shall need both deletion from, and addition to, the database. Arrivals. Arrivals will clearly involve an augmentation of the definition of right to/2 by new facts. To model for example the arrival of Tracy and Joe, to be seated between Adam and Susan, we will have to add the three facts in (P-3.2) to the database. And, we will have to remove the fact indicating that Susan is Adam’s right-hand neighbour: Therefore, to account for arrivals, both deletion from, and addition to the database will need to be done. We now review a few basic built-in predicates for modifying the database. • We use retract/1 (or retractall/1) to remove a clause (or all clauses of a predicate) from the database. The predicate whose clause is retracted has to be declared dynamic , implemented either as a directive in one of the source files or by calling dynamic/1 as a goal just before retracting. This is achieved in our example either by including in one of the files consulted the directive or interactively by • We use assert/1 to add a new clause to the database. A predicate newly introduced by assert/1 is deemed dynamic. An existing static (i.e. non-dynamic) predicate may be augmented by a new clause via assert/1 only after declaring it dynamic. • retractall/1 is used to remove from the database all clauses whose head unifies with the pattern in its argument. As with retract/1, retractall/1 may revoke dynamic predicates only. The following queries may be used to achieve the intended changes to the database. • George leaves the table (Fig. 3.3). As is easily confirmed by the query ?- listing(right to/2). , the predicate right to/2 is now defined in the database by the facts in (P-3.3). (Notice, however, that the definition of right to/2 in its Prolog source file is not yet affected.) • Tracy and Joe join the table and are seated between Adam and Susan (Fig. 3.4). Notice that due to the previous query the predicate right to/2 is now dynamic. It is now defined in the database by the facts in (P-3.4).3 It is seen that assert/1 places the new clause behind the existing ones for the same predicate.4 Exercise 3.3. Thus far, we have carried out (for reasons of transparency) database changes interactively only. In this exercise, you are asked to define some predicates for manipulating the database. (a) Define a predicate swap neighbours(+Left,+Right) for recording in the database of two neighbours swapping places. (For this predicate to succeed, prior to the swap, the person named in Left should be seated to the left of the person named in Right .) If we assume, for example, that the seating arrangement is initially as shown in Fig. 3.1, then the swap of Clara and Adam will be accomplished by After this, the database will look as follows. (b) Define a predicate swap(+Person1,+Person2) for recording in the database of two people swapping places who need not be neighbours. To exemplify, assume again that the database is initially as shown in Fig. 3.1. Then, Adam and George’s swap is carried out by upon which the database is as shown below. Note. You may use the predicate swap neighbours/2 from part (a) in your definition of swap/2 Exercise 3.4. (Modelling a queue)5 A queue with at least two customers at a checkout is modelled by the Prolog predicate behind/2 which is defined in the file queue.pl as shown below. (behind/2 is declared a dynamic predicate in queue.pl.) (These facts have an obvious interpretation: the person named in the second argument stands behind the person named in the first argument.) (a) Define a predicate swap neighbours(+Person1,+Person2) for recording in the database of two neighbours swapping places. (For this predicate to succeed, prior to the swap, the person named in Person2 should be standing behind the person named in Person1 .) Example:
EN14	1	﻿Computing is a constantly changing our world and our environment. In the 1960s large machines called mainframes were created to manage large volumes of data (numbers) efficiently. Bank account and payroll programs changed the way organisations worked and made parts of these organisations much more efficient. In the 1980s personal computers became common and changed the way many individuals worked. People started to own their own computers and many used word processors and spreadsheets applications (to write letters and to manage home accounts). In the 1990s email became common and the world wide web was born. These technologies revolutionised communications allowing individuals to publish information that could easily be accessed on a global scale. The ramifications of these new technologies are still not fully understood as society is adapting to opportunities of internet commerce, new social networking technologies (twitter, facebook, myspace, online gaming etc) and the challenges of internet related crime. Just as new computing technologies are changing our world so too are new techniques and ideas changing the way we develop computer systems. In the 1950s the use machine code (unsophisticated, complex and machine specific) languages were common. In the 1960s high level languages, which made programming simpler, became common. However these led to the development of large complex programs that were difficult to manage and maintain. In the 1970s the structured programming paradigm became the accepted standard for large complex computer programs. The structured programming paradigm proposed methods to logically structure the programs developed into separate smaller, more manageable components. Furthermore methods for analysing data were proposed that allowed large databases to be created that were efficient, preventing needless duplication of data and protected us against the risks associated with data becoming out of sync. However significant problems still persisted in a) understanding the systems we need to create and b) changing existing software as users requirements changed. In the 1980s ‘modular’ languages, such as Modula-2 and ADA were developed that became the precursor to modern Object Oriented languages. In the 1990s the Object Oriented paradigm and component-based software development ideas were developed and Object Oriented languages became the norm from 2000 onwards. The object oriented paradigm is based on many of the ideas developed over the previous 30 years of abstraction, encapsulation, generalisation and polymorphism and led to the development of software components where the operation of the software and the data it operates on are modelled together. Proponents of the Object Oriented software development paradigm argue that this leads to the development of software components that can be re-used in different applications thus saving significant development time and cost savings but more importantly allow better software models to be produced that make systems more maintainable and easier to understand. It should perhaps be noted that software development ideas are still evolving and new agile methods of working are being proposed and tested. Where these will lead us in 2020 and beyond remains to be seen. The structured programming paradigm proposed that programs could be developed in sensible blocks that make the program more understandable and easier to maintain. Activity 1 Assume you undertake the following activities on a daily basis. Arrange this list into a sensible order then split this list into three blocks of related activities and give each block a heading to summarise the activities carried out in that block. Get out of bed Eat breakfast Park the car Get dressed Get the car out of the garage Drive to work Find out what your boss wants you to do today Feedback to the boss on today’s results. Do what the boss wants you to do Feedback 1 You should have been able to organise these into groups of related activities and give each group a title that summarises those activities. Get up :- Get out of bed Get dressed Eat breakfast Go to Work :- Get the car out of the garage Drive to work Park the car Do your job :- Find out what your boss wants you to do today Do what the boss wants you to do Feedback to the boss on today’s results. By structuring our list of instructions and considering the overall structure of the day (Get up, go to work, do your job) we can change and improve one section of the instructions without changing the other parts. For example we could improve the instructions for going to work…. Listen to the local traffic and weather report Decide whether to go by bus or by car If going by car, get the car and drive to work. Else walk to the bus station and catch the bus without worrying about any potential impact this may have on ‘getting up’ or ‘doing your job’. In the same way structuring computer programs can make each part more understandable and make large programs easier to maintain. Feedback 2 With an address book we would want to be able to perform the following actions :- find out details of a friend i.e. their telephone number, add an address to the address book and, of course, delete an address. We can create a simple software component to store the data in the address book (i.e. list of names etc) and the operations, things we can do with the address book (i.e add address, find telephone number etc). By creating a simple software component to store and manage addresses of friends we can reuse this in another software system i.e. it could be used by a business manager to store and find details of customers. It could also become part of a library system to be used by a librarian to store and retrieve details of the users of the library. Thus in object oriented programming we can create re-usable software components (in this case an address book). While we can focus our attention on the actual program code we are writing, whatever development methodology is adopted, it is not the creation of the code that is generally the source of most problems. Most problems arise from :- • poor maintainability: the system is hard to understand and revise when, as is inevitable, requests for change arise. • Statistics show 70% of the cost of software is not incurred during its initial development phase but is incurred during subsequent years as the software is amended to meet the ever changing needs of the organisation for which it was developed. For this reason it is essential that software engineers do everything possible to ensure that software is easy to maintain during the years after its initial creation. ﻿The Unified Modelling Language, UML, is sometimes described as though it was a methodology. It is not! A methodology is a system of processes in order to achieve a particular outcome e.g. an organised sequence of activities in order to gather user requirements. UML does not describe the procedures a programmer should follow – hence it is not a methodology. It is, on the other hand, a precise diagramming notation that will allow program designs to be represented and discussed. As it is graphical in nature it becomes easy to visualise, understand and discuss the information presented in the diagram. However, as the diagrams represent technical information they must be precise and clear – in order for them to work - therefore there is a precise notation that must be followed. As UML is not a methodology it is left to the user to follow whatever processes they deem appropriate in order to generate the designs described by the diagrams. UML does not constrain this – it merely allows those designs to be expressed in an easy to use, but precise, graphical notation. A process will be explained in chapter 6 that will help you to generate good UML designs. Developing good designs is a skill that takes practise to this end the process is repeated in the case study (chapter 11). For now we will just concentrate on the UML notation not these processes. Classes are the basic components of any object oriented software system and UML class diagrams provide an easy way to represent these. As well as showing individual classes, in detail, class diagrams show multiple classes and how they are related to each other. Thus a class diagram shows the architecture of a system. A class consists of :- • a unique name (conventionally starting with an uppercase letter) • a list of attributes (int, double, boolean, String etc) • a list of methods This is shown in a simple box structure… For attributes and methods visibility modifiers are shown (+ for public access, – for private access). Attributes are normally kept private and methods are normally made public. Accessor methods are created to provide access to private attributes when required. Thus a public method SetTitle() can be created to change the value of a private attribute ‘title’. Thus a class Book, with String attributes of title and author, and the following methods SetTitle(), GetTitle(), SetAuthor(), GetAuthor() and ToString() would be shown as …. Note: String shown above is not a primitive data type but is itself a class. Hence it starts with a capital letter. Some programmers use words beginning in capitals to denote class names and words beginning in lowercase to represent attributes or methods (thus ToString() would be shown as toString()). This is a common convention when designing and writing programs in Java (another common OO language). However it is not a convention followed by C# programmers – where method names usually start in Uppercase. Method names can be distinguished from class names by the use of (). This in the example above. ‘Book’ is a class ‘title’ is an attribute and ‘SetTitle()’ is a method. UML diagrams are not language specific thus a software design, communicated via UML diagrams, can be implemented in a range of OO languages. Furthermore traditional accessor methods, getters and setters, are not required in C# programs as they are replaced by ‘properties’. Properties are in effect hidden accessor methods thus the getter and setter methods shown above, GetTitle(), SetTitle() etc are not required in a C# program. In C# an attribute would be defined called ‘title’ and a property would be defined as ‘Title’. This would allow us to set the ‘title’ directly by using the associated property ‘Title =…..;’. The UML diagrams shown in this book will use the naming convention common among C# programmers … for the simple reason that we will be writing sample code in C# to demonstrate the OO principles discussed here. Though initially we will show conventional assessor methods these will be replaced with properties when coding. UML allows us to suppress any information we do not wish to highlight in our diagrams – this allows us to suppress irrelevant detail and bring to the readers attention just the information we wish to focus on. Therefore the following are all valid class diagrams… Firstly with the access modifiers not shown…. Secondly with the access modifiers and the data types not shown….. And finally with the attributes and methods not shown….. i.e. there is a class called ‘BankAccount’ but the details of this are not being shown. Of course virtually all C# programs will be made up of many classes and classes will relate to each other – some classes will make use of other classes. These relationships are shown by arrows. Different type of arrow indicate different relationships (including inheritance and aggregation relationships). In addition to this class diagrams can make use of keywords, notes and comments. As we will see in examples that follow, a class diagram can show the following information :- • Classes --attributes --operations --visibility • Relationships --navigability --multiplicity --dependency --aggregation --composition • Generalization / specialization --inheritance --interfaces • Keywords • Notes and Comments As UML diagrams convey precise information there is a precise syntax that should be followed. Attributes should be shown as: visibility name : type multiplicity Where visibility is one of :- --‘+’ public --‘-’ private --‘#’ protected --‘~’ package and Multiplicity is one of :- --‘n’ exactly n --‘*’ zero or more --‘m..‘n’ between m and n The following are examples of attributes correctly specified using UML :- - custRef : int [1] a private attribute custRef is a single int value this would often be shown as - custRef : int However with no multiplicity shown we cannot safely assume a multiplicity of one was intended by the author. # itemCodes : String [1..*] a protected attribute itemCodes is one or more String values validCard : boolean an attribute validCard, of unspecified visibility, has unspecified multiplicity ﻿Many kinds of things in the world fall into related groups of ‘families’. ‘Inheritance’ is the idea ‘passing down’ characteristics from parent to child, and plays an important part in Object Oriented design and programming. While you are probably already familiar with constructors, and access control (public/private), there are particular issues in relating these to inheritance we need to consider. Additionally we need to consider the use of Abstract classes and method overriding as these are important concepts in the context of inheritance. Finally we will look at the ‘Object’ class which has a special role in relation to all other classes in C#. Classes are a generalized form from which objects with differing details can be created. Objects are thus ‘instances’ of their class. For example Student 051234567 is an instance of class Student. More concisely, 051234567 is a Student. Constructors are special methods that create an object from the class definition. Classes themselves can often be organised by a similar kind of relationship. One hierarchy, that we all have some familiarity with, is that which describes the animal kingdom :- • Kingdom (e.g. animals) • Phylum (e.g. vertebrates) • Class (e.g. mammal) • Order (e.g. carnivore) • Family (e.g. cat) • Genus (e.g. felix) • Species (e.g. felix leo) We can represent this hierarchy graphically …. Of course to draw the complete diagram would take more time and space than we have available. Here we can see one specific animal shown here :-‘Fred’. Fred is not a class of animal but an actual animal. Fred is a felix leo is a felix is a cat is a carnivore Carnivores eat meat so Fred has the characteristic ‘eats meat’. Fred is a felix leo is a felix is a cat is a carnivore is a mammal is a vertebrate Vertebrates have a backbone so Fred has the characteristic ‘has a backbone’. The ‘is a’ relationship links an individual to a hierarchy of characteristics. This sort of relationship applies to many real world entities, e.g. BonusSuperSaver is a SavingsAccount is a BankAccount. We specify the general characteristics high up in the hierarchy and more specific characteristics lower down. An important principle in OO – we call this generalization and specialization. All the characteristics from classes above in a class/object in the hierarchy are automatically featured in it – we call this inheritance. Consider books and magazines - both are specific types of publication. We can show classes to represent these on a UML class diagram. In doing so we can see some of the instance variables and methods these classes may have. Attributes ‘title’, ‘author’ and ‘price’ are obvious. Less obvious is ‘copies’ this is how many are currently in stock. For books, OrderCopies() takes a parameter specifying how many extra copies are added to stock. For magazines, orderQty is the number of copies received of each new issue and currIssue is the date/period of the current issue (e.g. “January 2011”, “Fri 6 Jan”, “Spring 2011” etc.) When a new issue is received the old issues are discarded and orderQty copies are placed in stock. Therefore RecNewIssue() sets currIssue to the date of new issue and restores copies to orderQty. AdjustQty() modifies orderQty to alter how many copies of subsequent issues will be stocked. We can separate out (‘factor out’) these common members of the classes into a superclass called Publication. In C# a superclass is often called a base class. The differences will need to be specified as additional members for the ‘subclasses’ Book and Magazine. In this is a UML Class Diagram the hollow-centred arrow denotes inheritance. Note the subclass has the generalized superclass (or base class) characteristics + additional specialized characteristics. Thus the Book class has four instance variables (title, price, copies and author) it also has two methods (SellCopy() and OrderCopies()). The inherited characteristics are not listed in subclasses. The arrow shows they are acquired from the superclass. No special features are required to create a superclass. Thus any class can be a superclass unless specifically prevented. A subclass specifies it is inheriting features from a superclass using the : symbol. For example…. Constructors are methods that create objects from a class. Each class (whether sub or super) should encapsulate its own initialization in a constructor, usually relating to setting the initial state of its instance variables. Constructors are methods given the same name as the class. A constructor for a superclass, or base class, should deal with general initialization. Each subclass can have its own constructor for specialised initialization but it must often invoke the behaviour of the base constructor. It does this using the keyword base. Usually some of the parameters passed to MySubClass will be initializer values for superclass instance variables, and these will simply be passed on to the superclass constructor as parameters. In other words super-parameters will be some (or all) of sub-parameters. Shown below are two constructors, one for the Publication class and one for Book. The book constructor requires four parameters three of which are immediately passed on to the base constructor to initialize its instance variables. Thus in creating a book object we first create a publication object. The constructor for Book does this by calling the constructor for Publication. Rules exist that govern the invocation of a superconstructor. If the superclass has a parameterless (or default) constructor this will be called automatically if no explicit call to base is made in the subclass constructor though an explicit call is still better style for reasons of clarity. However if the superclass has no parameterless constructor but does have a parameterized one, this must be called explicitly using : base. To illustrate this…. On the left above:- it is legal, though bad practice, to have a subclass with no constructor because superclass has a parameterless constructor. In the centre:- if subclass constructor doesn’t call the base constructor then the parameterless superclass constructor will be called. On the right:- because superclass has no paramterless constructor, subclass must have a constructor, it must call the super constructor using the keyword base and it must pass on the required paramter. This is simply because a (super) class with only a parameterized constructor can only be initialized by providing the required parameter(s). ﻿Within hierarchical classification of animals Pinky is a pig (species sus scrofa) Pinky is (also, more generally) a mammal Pinky is (also, even more generally) an animal We can specify the type of thing an organism is at different levels of detail: higher level = less specific lower level = more specific If you were asked to give someone a pig you could give them Pinky or any other pig. If you were asked to give someone a mammal you could give them Pinky, any other pig or any other mammal (e.g. any lion, or any mouse, or any cat). If you were asked to give someone an animal you could give them Pinky, any other pig, any other mammal, or any other animal (bird, fish, insect etc). The idea here is that an object in a classification hierarchy has an ‘is a’ relationship with every class from which it is descended and each classification represents a type of animal. This is true in object oriented programs as well. Every time we define a class we create a new ‘type’. Types determine compatibility between variables, parameters etc. A subclass type is a subtype of the superclass type and we can substitute a subtype wherever a ‘supertype’ is expected. Following this we can substitute objects of a subtype whenever objects of a supertype are required (as in the example above). The class diagram below shows a hierarchical relationship of types of object – or classes. In other words we can ‘substitute’ an object of any subclass where an object of a superclass is required. This is NOT true in reverse! When designing class/type hierarchies, the type mechanism allows us to place a subclass object where a superclass is specified. However this has implications for the design of subclasses – we need to make sure they are genuinely substitutable for the superclass. If a subclass object is substitutable then clearly it must implement all of the methods of the superclass – this is easy to guarantee as all of the methods defined in the superclass are inherited by the subclass. Thus while a subclass may have additional methods it must at least have all of the methods defined in the superclass and should therefore be substitutable. However what happens if a method is overridden in the subclass? When overriding methods we must ensure that they are still substitutable for the method being replaced. Therefore when overriding methods, while it is perfectly acceptable to tailor the method to the needs of the subclass a method should not be overridden with functionality which performs an inherently different operation. For example, RecNewIssue() in DiscMag overrides RecNewIssue() from Magazine but does the same basic job (“fulfils the contract”) as the inherited version with respect to updating the number of copies and the current issue. While it extends that functionality in a way specifically relevant to DiscMags by displaying a reminder to check the cover discs, essentially these two methods perform the same operation. What do we know about a ‘Publication’? Answer: It’s an object which supports (at least) the operations: void SellCopy() String ToString() and it has properties that allow us to set the price, get the number of copies set the number of copies. Inheritance guarantees that objects of any subclass of Publications provides at least these. Note that a subclass can never remove an operation inherited from its superclass(es) – this would break the guarantee. Because subclasses extend the capabilities of their superclasses, the superclass functionality can be assumed. It is quite likely that we would choose to override the ToString() method (initially defined within ‘Object’) within Publication and override it again within Magazine so that the String returned provides a better description of Publications and Magazines. However we should not override the ToString() method in order to return the price – this would be changing the functionality of the method so that the method performs an inherently different function. Doing this would break the substitutability principle. Because an instance of a subclass is an instance of its superclass we can handle subclass objects as if they were superclass objects. Furthermore because a superclass guarantees certain operations in its subclasses we can invoke those operations without caring which subclass the actual object is an instance of. This characteristic is termed ‘polymorphism’, originally meaning ‘having multiple shapes’. Thus a Publication comes in various shapes … it could be a Book, Magazine or DiscMag. We can invoke the SellCopy() method on any of these Publications irrespective of their specific details. Polymorphism is a fancy name for a common idea. Someone who knows how to drive can get into and drive most cars because they have a set of shared key characteristics – steering wheel, gear stick, pedals for clutch, brake and accelerator etc – which the driver knows how to use. There will be lots of differences between any two cars, but you can think of them as subclasses of a superclass which defines these crucial shared ‘operations’. If ‘p’ ‘is a’ Publication, it might be a Book or a Magazine or a DiscMag. Whichever it is we know that it has a SellCopy() method. So we can invoke p.SellCopy() without worrying about what exactly ‘p’ is. This can make life a lot simpler when we are manipulating objects within an inheritance hierarchy. We can create new types of Publication e.g. a Newspaper and invoke p,SellCopy() on a Newspaper without have to create any functionality within the new class – all the functionality required is already defined in Publication. Polymorphism makes it very easy to extend the functionality of our programs as we will see now and we will see this again in the case study (in Chapter 11). Huge sums of money are spent annually creating new computer programs but over the years even more is spent changing and adapting those programs to meet the changing needs of an organisation. Thus as professional software engineers we have a duty to facilitate this and help to make those programs easier to maintain and adapt. Of course the application of good programming standards, commenting and layout etc, have a part to play here but also polymorphism can help as it allows programs to be made that are easily extended. ﻿Historically in computer programs method names were required to be unique. Thus the compiler could identify which method was being invoked just by looking at its name. However several methods were often required to perform very similar functionality for example a method could add two integer numbers together and another method may be required to add two floating point numbers. If you have to give these two methods unique names which one would you call ‘Add()’? In order to give each method a unique name the names would need to be longer and more specific. We could therefore call one method AddInt() and the other AddFloat() but this could lead to a proliferation of names each one describing different methods that are essentially performing the same operation i.e. adding two numbers. To overcome this problem in C# you are not required to give each method a unique name – thus both of the methods above could be called Add(). However if method names are not unique the C# must have some other way of deciding which method to invoke at run time. i.e. when a call is made to Add(number1, number2) the machine must decide which of the two methods to use. It does this by looking at the parameter list. While the two methods may have the same name they can still be distinguished by looking at the parameter list. :- Add(int number1, int number2) Add(float number1, float number2) This is resolved at run time by looking at the method call and the actual parameters being passed. If two integers are being passed then the first method is invoked. However if two floating point numbers are passed then the second method is used. Overloading refers to the fact that several methods may share the same name. As method names are no longer uniquely identify the method then the name is ‘overloaded’. Having several methods that essentially perform the same operation, but which take different parameter lists, can lead to enhanced flexibility and robustness in a system. Imagine a University student management system. A method would probably be required to enrol, or register, a new student. Such a method could have the following signature … EnrollStudent(String name, String address, String coursecode) However if a student had just arrived in the city and had not yet sorted out where they were living would the University want to refuse to enrol the student? They could do so but would it not be better to allow such a student to enrol (and set the address to ‘unkown’)? To allow this the method EnrollStudent() could be overloaded and an alternative method provided as… EnrollStudent(String name, String coursecode) At run time the method invoked will depend upon the parameter list provided. Thus given a call to EnrollStudent(“Fred”, “123 Abbey Gardens”, “G700”) the first method would be used. Overloading methods don’t just provide more flexibility for the user they also provide more flexibility for programmers who may have the job of extending the system in the future and thus overloading methods can make the system more future proof and robust to changing requirements. Constructors can be overloaded as well as ordinary methods. We can make our programs more adaptable by overloading constructors and other methods. Even if we don’t initially use all of the different constructors, or methods, by providing them we are making our programs more flexible and adaptable to meet changing requirements. Method overloading is the name given to the concept that several methods may exist that essentially perform the same operation and thus have the same name. The CLR engine distinguishes these by looking at the parameter list. If two or more methods have the same name then their parameter list must be different. At run time each method call, which may be ambiguous, is resolved by the CLR engine by looking at the parameters passed and matching the data types with the method signatures defined in the class. By overloading constructors and ordinary methods we are providing extra flexibility to the programmers who may use our classes in the future. Even if these are not all used initially, providing these can help make the program more flexible to meet changing user requirements. The development of any computer program starts by identifying a need :- • An engineer who specialises in designing bridges may need some software to create three dimensional models of the designs so people can visualise the finished bridge long before it is actually built. • A manager may need a piece of software to keep track of personnel, what projects they are assigned to, what skills they have and what skills need to be developed etc. But how do we get from a ‘need’ for some software to an object oriented software design that will meet this need? Some software engineers specialise in the task of Requirement Analysis which is the task of clarifying exactly what is required of the software. Often this is done by iteratively performing the following tasks :- 1) interviewing clients and potential users of the system to find out what they say about the system needed 2) documenting the results of these conversations, 3) identifying the essential features of the required system 4) producing preliminary designs (and possibly prototypes of the system) 5) evaluating these initial plans with the client and potential users 6) repeating the steps above until a finished design has evolved. Performing requirements analysis is a specialised skill that is outside the scope of this text but here we will focus on steps three and four above ie. given a description of a system how do we convert this into a potential OO design. While we can hope to develop preliminary design skills experience is a significant factor in this task. Producing simple and elegant designs is important if we want the software to work well and be easy to develop however identifying good designs from weaker designs is not simple and experience is a key factor. A novice chess player may know all the rules but it takes experience to learn how to choose good moves from bad moves and experience is essential to becoming a skilled player. Similarly experience is essential to becoming skilled at performing user requirements analysis and in producing good designs. ﻿Before a computer can complete useful tasks for us (e.g. check the spelling in our documents) software needs to be written and implemented on the machine it will run on. Software implementation involves the writing of program source code and preparation for execution on a particular machine. Of course before the software is written it needs to be designed and at some point it needs to be tested. There are many iterative lifecycles to support the process of design, implementation and testing that involve multiple implementation phases. Of particular concern here are the three long established approaches to getting source code to execute on a particular machine… compilation into machine-language object code direct execution of source code by ‘interpreter’ program compilation into intermediate object code which is then interpreted by run-time system Implementing Java programs involves compiling the source code (Java) into intermediate object code which is then interpreted by a run-time system called the JRE. This approach has some advantages and disadvantages and it is worth comparing these three options in order to appreciate the implications for the Java developer. The compiler translates the source code into machine code for the relevant hardware/OS combination. Strictly speaking there are two stages: compilation of program units (usually files), followed by ‘linking’ when the complete executable program is put together including the separate program units and relevant library code etc. The compiled program then runs as a ‘native’ application for that platform. This is the oldest model, used by early languages like Fortran and Cobol, and many modern ones like C++. It allows fast execution speeds but requires re-compilation of the program each time the code is changed. Here the source code is not translated into machine code. Instead an interpreter reads the source code and performs the actions it specifies. We can say that the interpreter is like a ‘virtual machine’ whose machine language is the source code language. No re-compilation is required after changing the code, but the interpretation process inflicts a significant impact on execution speed. Scripting languages tend to work in this way. This model is a hybrid between the previous two. Compilation takes place to convert the source code into a more efficient intermediate representation which can be executed by a ‘run-time system’ (again a sort of ‘virtual machine’) more quickly that direct interpretation of the source code. However, the use of an intermediate code which is then executed by run-time system software allows the compilation process to be independent of the OS/hardware platform, i.e. the same intermediate code should run on different platforms so long as an appropriate run-time system is available for each platform. This approach is long-established (e.g. in Pascal from the early 1970s) and is how Java works. To run Java programs we must first generate intermediate code (called bytecode) using a compiler available as part of the Java Development Kit (JDK) – see section 8.4 below. A version of the Java Runtime Environment (JRE), which incorporates a Java Virtual machine (VM), is required to execute the bytecode and the Java library packages. Thus a JRE must be present on any machine which is to run Java programs. The Java bytecode is standard and platform independent and as JRE’s have been created for most computing devices (including PC’s, laptops, mobile devices, mobile phones, internet devices etc) this makes Java programs highly portable. Whatever mode of execution is employed, programmers can work with a variety of tools to create source code. Options include the use of simple discrete tools (e.g. editor, compiler, interpreter) invoked manually as required or alternatively the use of an Integrated Development Environment (IDE) which incorporates these implementation tools behind a seamless interface. Still more sophisticated CASE (Computer Aided Software Engineering) tools which integrate the implementation process with other phases of the development cycle – such software could take UML class diagrams and generate classes and method stubs automatically saving some of the effort required to write the Java code. When writing java programs each class (or interface) in a Java program has its own name.java file containing the source code. These are processed by the compiler to produce name.class files containing the corresponding bytecode. To actually run as an application, one of the classes must contain a main() method with the signature shown above. To develop Java programs you must first install the Java Development Kit (JDK). This was developed by Sun and is available freely from the internet via http://java.sun.com/. Prior to version 5.0 (or 1.5) this was known as the Java Software Development Kit (SDK). A Java IDE’s, e.g. Eclipse or NetBeans, sits on top of’ the JDK to add the IDE features - these may include interface development tools, code debugging tools, testing tools and refactoring tools (more on these later). When using an IDE it is easy to forget that much of the functionality is in fact part of the JDK and when the IDE is asked to compile a program it is infact just passing on this request to the JDK sitting underneath. We can use the JDK directly from the command line to compile and run Java programs though mostly it is easier to use the additional facilities offered by an IDE. Somewhat confusingly the current version of Java is known both as 6.0, 1.6 and even 1.6.0. These supposedly have subtly different meanings – don’t worry about it! There are many tools in the JDK. A description of each of these is available from http://java.sun.com/javase/downloads/ and following the links for Documentation, APIs and JDK Programmer guides. The two most important basic tools are: javac – the java compiler java – the java program launcher (that runs the VM) To compile MyProg.java we type javac MyProg.java If successful this will create MyProg.class To run Myprog (assuming it has a main() method) we type java MyProg Another, extremely useful tool, is javadoc - this uses comments in Java source code to generate automatic documentation for programs. Moving to an ‘industrial strength’ IDE is an important stepping stone in your progress as a software developer, like riding a bicycle without stabilisers for the first time. With some practice you will soon find it offers lots of helpful and time-saving facilities that you will not want to work without again… Eclipse is a flexible and extensible IDE platform. It was first developed by IBM but is now open source and can be downloaded from the Eclipse Foundation at www.eclipse.org.$$$﻿Before a computer can complete useful tasks for us (e.g. check the spelling in our documents) software needs to be written and implemented on the machine it will run on. Software implementation involves the writing of program source code and preparation for execution on a particular machine. Of course before the software is written it needs to be designed and at some point it needs to be tested. There are many iterative lifecycles to support the process of design, implementation and testing that involve multiple implementation phases. Of particular concern here are the three long established approaches to getting source code to execute on a particular machine… compilation into machine-language object code direct execution of source code by ‘interpreter’ program compilation into intermediate object code which is then interpreted by run-time system Implementing Java programs involves compiling the source code (Java) into intermediate object code which is then interpreted by a run-time system called the JRE. This approach has some advantages and disadvantages and it is worth comparing these three options in order to appreciate the implications for the Java developer. The compiler translates the source code into machine code for the relevant hardware/OS combination. Strictly speaking there are two stages: compilation of program units (usually files), followed by ‘linking’ when the complete executable program is put together including the separate program units and relevant library code etc. The compiled program then runs as a ‘native’ application for that platform. This is the oldest model, used by early languages like Fortran and Cobol, and many modern ones like C++. It allows fast execution speeds but requires re-compilation of the program each time the code is changed. Here the source code is not translated into machine code. Instead an interpreter reads the source code and performs the actions it specifies. We can say that the interpreter is like a ‘virtual machine’ whose machine language is the source code language. No re-compilation is required after changing the code, but the interpretation process inflicts a significant impact on execution speed. Scripting languages tend to work in this way. This model is a hybrid between the previous two. Compilation takes place to convert the source code into a more efficient intermediate representation which can be executed by a ‘run-time system’ (again a sort of ‘virtual machine’) more quickly that direct interpretation of the source code. However, the use of an intermediate code which is then executed by run-time system software allows the compilation process to be independent of the OS/hardware platform, i.e. the same intermediate code should run on different platforms so long as an appropriate run-time system is available for each platform. This approach is long-established (e.g. in Pascal from the early 1970s) and is how Java works. To run Java programs we must first generate intermediate code (called bytecode) using a compiler available as part of the Java Development Kit (JDK) – see section 8.4 below. A version of the Java Runtime Environment (JRE), which incorporates a Java Virtual machine (VM), is required to execute the bytecode and the Java library packages. Thus a JRE must be present on any machine which is to run Java programs. The Java bytecode is standard and platform independent and as JRE’s have been created for most computing devices (including PC’s, laptops, mobile devices, mobile phones, internet devices etc) this makes Java programs highly portable. Whatever mode of execution is employed, programmers can work with a variety of tools to create source code. Options include the use of simple discrete tools (e.g. editor, compiler, interpreter) invoked manually as required or alternatively the use of an Integrated Development Environment (IDE) which incorporates these implementation tools behind a seamless interface. Still more sophisticated CASE (Computer Aided Software Engineering) tools which integrate the implementation process with other phases of the development cycle – such software could take UML class diagrams and generate classes and method stubs automatically saving some of the effort required to write the Java code. When writing java programs each class (or interface) in a Java program has its own name.java file containing the source code. These are processed by the compiler to produce name.class files containing the corresponding bytecode. To actually run as an application, one of the classes must contain a main() method with the signature shown above. To develop Java programs you must first install the Java Development Kit (JDK). This was developed by Sun and is available freely from the internet via http://java.sun.com/. Prior to version 5.0 (or 1.5) this was known as the Java Software Development Kit (SDK). A Java IDE’s, e.g. Eclipse or NetBeans, sits on top of’ the JDK to add the IDE features - these may include interface development tools, code debugging tools, testing tools and refactoring tools (more on these later). When using an IDE it is easy to forget that much of the functionality is in fact part of the JDK and when the IDE is asked to compile a program it is infact just passing on this request to the JDK sitting underneath. We can use the JDK directly from the command line to compile and run Java programs though mostly it is easier to use the additional facilities offered by an IDE. Somewhat confusingly the current version of Java is known both as 6.0, 1.6 and even 1.6.0. These supposedly have subtly different meanings – don’t worry about it! There are many tools in the JDK. A description of each of these is available from http://java.sun.com/javase/downloads/ and following the links for Documentation, APIs and JDK Programmer guides. The two most important basic tools are: javac – the java compiler java – the java program launcher (that runs the VM) To compile MyProg.java we type javac MyProg.java If successful this will create MyProg.class To run Myprog (assuming it has a main() method) we type java MyProg Another, extremely useful tool, is javadoc - this uses comments in Java source code to generate automatic documentation for programs. Moving to an ‘industrial strength’ IDE is an important stepping stone in your progress as a software developer, like riding a bicycle without stabilisers for the first time. With some practice you will soon find it offers lots of helpful and time-saving facilities that you will not want to work without again… Eclipse is a flexible and extensible IDE platform. It was first developed by IBM but is now open source and can be downloaded from the Eclipse Foundation at www.eclipse.org.
EN15	0	﻿The risk of computer crime has become a global issue affecting almost all countries. Salifu (2008) argues that the Internet is a "double-edged sword" providing many opportunities for individuals and organizations to develop and prosper, but at the same time has brought with it new opportunities to commit crime. For example, Nigeria-related financial crime is extensive and 122 out of 138 countries at an Interpol meeting complained about Nigerian involvement in financial fraud in their countries. The most notorious type attempted daily on office workers all over the world, is the so-called advance fee fraud. The sender will seek to involve the recipient in a scheme to earn millions of dollars if the recipient pays an advance fee (Ampratwum, 2009). Computer crime is an overwhelming problem worldwide. It has brought an array of new crime activities and actors and, consequently, a series of new challenges in the fight against this new threat (Picard, 2009). Policing computer crime is a knowledge-intensive challenge indeed because of the innovative aspect of many kinds of computer crime. Cyberspace presents a challenging new frontier for criminology, police science, law enforcement and policing. Virtual reality and computer-mediated communications challenge the traditional discourse of criminology and police work, introducing new forms of deviance, crime, and social control. Since the 1990s, academics and practitioners have observed how cyberspace has emerged as a new field of criminal activity. Cyberspace is changing the nature and scope of offending and victimization. A new discipline named cyber criminology is emerging. Jaishankar (2007) defines cyber criminology as the study of causation of crimes that occur in the cyberspace and its impact in the physical space.  Employees of the organization commit most computer crime, and the crime occurs inside company walls (Hagen et al., 2008: Nykodym et al, 2005). However, in our perspective of financial crime introduced in this chapter, we will define computer crime as a profit-oriented crime rather than a damage-oriented crime, thereby excluding the traditional focus of dissatisfied and frustrated employees wanting to harm their own employers.  Computer crime is defined as any violations of criminal law that involve knowledge of computer technology for their perpetration, investigation, or prosecution (Laudon and Laudon, 2010). The initial role of information and communication technology was to improve the efficiency and effectiveness of organizations. However, the quest of efficiency and effectiveness serves more obscure goals as fraudsters exploit the electronic dimension for personal profits. Computer crime is an overwhelming problem that has brought an array of new crime types (Picard, 2009). Examples of computer-related crimes include sabotage, software piracy, and stealing personal data (Pickett and Pickett, 2002). In computer crime terminology, the term cracker is typically used to denote a hacker with a criminal intent. No one knows the magnitude of the computer crime problem – how many systems are invaded, how many people engage in the practice, or the total economic damage. According to Laudon and Laudon (2010), the most economically damaging kinds of computer crime are denial-of-service attacks, where customer orders might be rerouted to another supplier. Eleven men in five countries carried out one of the worst data thefts for credit card fraud ever (Laudon and Laudon, 2010: 326): In early August 2008, U.S. federal prosecutors charged 11 men in five countries, including the United States, Ukraine, and China, with stealing more than 41 million credit and debit card numbers. This is now the biggest known theft of credit card numbers in history. The thieves focused on major retail chains such as OfficeMax, Barnes & Noble, BJ’s Wholesale Club, the Sports Authority, and T.J. Marxx. The thieves drove around and scanned the wireless networks of these retailers to identify network vulnerabilities and then installed sniffer programs obtained from overseas collaborators. The sniffer programs tapped into the retailers’ networks for processing credit cards, intercepting customers’ debit and credit card numbers and PINs (personal identification numbers). The thieves then sent that information to computers in the Ukraine, Latvia, and the United States. They sold the credit card numbers online and imprinted other stolen numbers on the magnetic stripes of blank cards so they could withdraw thousands of dollars from ATM machines. Albert Gonzales of Miami was identified as a principal organizer of the ring.  The conspirators began their largest theft in July 2005, when they identified a vulnerable network at a Marshall’s department store in Miami and used it to install a sniffer program on the computers of the chain’s parent company, TJX. They were able to access the central TJX database, which stored customer transactions for T.J. Marxx, Marshalls, HomeGoods, and A.J. Wright stores in the United States and Puerto Rico, and for Winners and HomeSense stores in Canada. Fifteen months later, TJX reported that the intruders had stolen records with up to 45 million credit and debit card numbers. TJX was still using the old Wired Equivalent Privacy (WEP) encryption system, which is relatively easy for hackers to crack. Other companies had switched to the more secure Wi-Fi Protected Access (WPA) standard with more complex encryption, but TJX did not make the change. An auditor later found that TJX had also neglected to install firewalls and data encryption on many of the computers using the wireless network, and did not properly install another layer of security software it had purchased. TJX acknowledged in a Securities and Exchange Commission filing that it transmitted credit card data to banks without encryption, violating credit card company guidelines. Computer crime, often used synonymous with cyber crime, refers to any crime that involves a computer and a network, where the computer has played a part in the commission of a crime. Internet crime, as the third crime label, refers to criminal exploitation of the Internet. In our perspective of profit-oriented crime, crime is facilitated by computer networks or devices, where the primary target is not computer networks and devices, but rather independent of the computer network or device.  Cyber crime is a term used for attacks on the cyber security infrastructure of business organizations that can have several goals. One goal pursued by criminals is to gain unauthorized access to the target’s sensitive information. Most businesses are vitally dependent on their proprietary information, including new product information, employment records, price lists and sales figures. According to Gallaher et al. (2008), an attacker may derive direct economic benefits from gaining access to and/or selling such information, or may inflict damage on an organization by impacting upon it. ﻿Fake websites have become increasingly pervasive and trustworthy in their appearance, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. Abbasi et al. (2010) found that the growth in profitable fake websites is attributable to several factors, including their authentic appearance, a lack of user awareness regarding them, and the ability of fraudsters to undermine many existing mechanisms for protecting against them. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Distinctions can be made between spoof sites and concocted sites. A spoof site is an imitation of an existing commercial website such as eBay or PayPal. A concocted site is a deceptive website attempting to create the impression of a legitimate, unique and trustworthy entity. Detecting fake websites is difficult. There is a need for both fraud cues as well as problem-specific knowledge. Fraud cues are important design elements of fake websites that may serve as indicators of their lack of authenticity. First, fake websites often use automatic content generation techniques to mass-produce fake web pages. Next, fraud cues include information, navigation, and visual design. Information in terms of web page text often contains fraud cues stemming from information design elements. Navigation in terms of linkage information and URL names for a website can provide relevant fraud cues relating to navigation design characteristics. For example, it is argued that 70 percent of ".biz" domain pages are fake sites. Fake websites frequently use images from existing legitimate or prior fake websites. For example spoof sites copy company logos from the websites they are mimicking. The fact that it is copied can be detected in the system (Abbasi et al., 2010). In addition to fraud cues, there is a need for problem-specific knowledge. Problem-specific knowledge regarding the unique properties of fake websites includes stylistic similarities and content duplication (Abbasi et al., 2010). Abbasi et al. (2010) developed a prototype system for fake website detection. The system is based on statistical learning theory. Statistical learning theory is a computational learning theory that attempts to explain the learning process from a statistical point of view. The researchers conducted a series of experiments, comparing the prototype system against several existing fake website detection systems on a test sample encompassing 900 websites. The results indicate that systems grounded in statistical learning theory can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. A variation of fake websites is fraudulent email solicitation where the sender of an email claims an association with known and reputable corporations or organizational entities. For example, one email from the "Microsoft/AOL Award Team" notified its winners of a sweepstake by stating, "The prestigious Microsoft and AOL has set out and successfully organized a Sweepstakes marking the end of year anniversary we rolled out over 100,000.000.00 for our new year Anniversary Draw" (Nhan et al., 2009). The email proceeded to ask for the potential victim's personal information.  Nhan et al. (2009) examined 476 fraudulent email solicitations, and found that the three most frequently alleged organizational associations were Microsoft, America Online, and PayPal. Fraudsters also attempt to establish trust through associating with credit-issuing financial corporations and authoritative organizations and groups.  Money laundering is an important activity for most criminal activity (Abramova, 2007; Council of Europe, 2007; Elvins, 2003). Money laundering means the securing of the proceeds of a criminal act. The proceeds must be integrated into the legal economy before the perpetrators can use it. The purpose of laundering is to make it appear as if the proceeds were acquired legally, as well as disguises its illegal origins (Financial Intelligence Unit, 2008). Money laundering takes place within all types of profit-motivated crime, such as embezzlement, fraud, misappropriation, corruption, robbery, distribution of narcotic drugs and trafficking in human beings (Økokrim, 2008). Money laundering has often been characterized as a three-stage process that requires (1) moving the funds from direct association with the crime, (2) disguising the trail to foil pursuit, and (3) making them available to the criminal once again with their occupational and geographic origins hidden from view. The first stage is the most risky one for the criminals, since money from crime is introduced into the financial system. Stage 1 is often called the placement stage. Stage 2 is often called the layering stage, in which money is moved in order to disguise or remove direct links to the offence committed. The money may be channeled through several transactions, which could involve a number of accounts, financial institutions, companies and funs as well as the use of professionals such as lawyers, brokers and consultants as intermediaries. Stage 3 is often called the integration stage, where a legitimate basis for asset origin has been created. The money is made available to the criminal and can be used freely for private consumption, luxury purchases, real estate investment or investment in legal businesses. Money laundering has also been described as a five-stage process: placement, layering, integration, justification, and embedding (Stedje, 2004). It has also been suggested that money laundering falls outside of the category of financial crime. Since money-laundering activities may use the same financial system that is used for the perpetration of core financial crime, its overlap with the latter is apparent (Stedje, 2004). According to Joyce (2005), criminal money is frequently removed from the country in which the crime occurred to be cycled through the international payment system to obscure any audit trail. The third stage of money laundering is done in different ways. For example, a credit card might be issued by offshore banks, casino 'winning' can be cashed out, capital gains on option and stock trading might occur, and real estate sale might cause profit.  The proceeds of criminal acts could be generated from organized crime such as drug trafficking, people smuggling, people trafficking, proceeds from robberies or money acquired by embezzlement, tax evasion, fraud, abuse of company structures, insider trading or corruption. The Financial Intelligence Unit (2008) in Norway argues that most criminal acts are motivated by profit. When crime generates significant proceeds, the perpetrators need to find a way to control the assets without attracting attention to them selves or the offence committed. Thus, the money laundering process is decisive in order to enjoy the proceeds without arousing suspicion. ﻿While we focus on white-collar financial crime in this book on computer crime, we must not forget that there are a number of other types of crime that are typical for cyber crime and Internet crime as well. Typical examples are hacking, child pornography and online child grooming. In this chapter, we present the case of child grooming as computer crime. Internet use has grown considerably in the last decade. Information technology now forms a core part of the formal education system in many countries, ensuring that each new generation of Internet users is more adept than the last. Research studies in the UK suggest that the majority of young people aged 9-19 accessed the Internet at least once a day. The Internet provides the opportunity to interact with friends on social networking sites such as Myspace and Bebo and enables young people to access information in a way that previous generations would not have thought possible. The medium also allows users to post detailed personal information, which may be accessed by any site visitor and provides a platform for peer communication hitherto unknown (Davidson and Martellozzo, 2008). There is, however, increasing evidence that the Internet is used by some adults to access children and young people in order to groom them for the purposes of sexual abuse. Myspace have recently expelled 29,000 suspected sex offenders and is being sued in the United States by parents who claim that their children were contacted by sex offenders on the site and consequently abused (BBC, 2007). The Internet also plays a role in facilitating the production and distribution of indecent illegal images of children, which may encourage and complement online grooming.  Recent advances in computer technology have been aiding sexual sex offenders, stalkers, child pornographers, child traffickers, and others with the intent of exploiting children (Kierkegaard, 2008: 41): Internet bulletin boards, chat rooms, private websites, and peer-to-peer networks are being used daily by pedophiles to meet unsuspecting children. Compounding the problem is the lack of direct governance by an international body, which will curb the illegal content and activity. Most countries already have laws protecting children, but what is needed is a concerted law enforcement and international legislation to combat child sex abuse. Men who target young people online for sex are pedophiles (Kierkegaard, 2008; Wolak et al., 2008). According to Dunaigre (2001), the pedophile is an emblematic figure, made into a caricature and imbued with all the fears, anxieties and apprehensions rocking our society today. Pedophile acts are - according to the World Health Organization (WHO) - sexual behavior that an adult major (16 years or over), overwhelmingly of the male sex, acts out towards prepubescent children (13 years or under). According to the WHO, there must normally be a five-year age difference between the two, except in the case of pedophilic practices at the end of adolescence where what counts is more the difference in sexual maturity. However, the definition of criminal behavior varies among countries. As will become evident from reading this article, pedophile acts in Norway are sexual behavior that a person acts out towards children of 16 years or under. There is no minimum age definition for the grooming person in Norwegian criminal law, but age difference and difference in sexual maturity is included as criteria for criminal liability.  Wolak et al. (2009: 4) present two case examples of crimes by online sex offenders in the United States: • Police in West Coast state found child pornography in the possession of the 22-year-old offender. The offender, who was from a North-eastern state, confessed to befriending a 13-year-old local boy online, traveling to the West Coast, and meeting him for sex. Prior to the meeting, the offender and victim had corresponded online for about six months. The offender had sent the victim nude images via web cam and e-mail and they had called and texted each other hundreds of times. When they met for sex, the offender took graphic pictures of the encounter. The victim believed he was in love with the offender. He lived alone with his father and was struggling to fit in and come to terms with being gay. The offender possessed large quantities of child pornography that he had downloaded from the Internet. He was sentenced to 10 years in prison. • A 24-year-old man met a 14-year-old girl at a social networking site. He claimed to be 19. Their online conversation became romantic and sexual and the victim believed she was in love. They met several times for sex over a period of weeks. The offender took nude pictures of the victim and gave her alcohol and drugs. Her mother and stepfather found out and reported the crime to the police. The victim was lonely, had issues with drugs and alcohol, and problems at school and with her parents. She had posted provocative pictures of herself on her social networking site. She had met other men online and had sex with them. The offender was a suspect in another online enticement case. He was found guilty but had not been sentenced at time of the interview.  According to Davidson and Martellozzo (2008: 277), Internet sex offender behavior can include: "the construction of sites to be used for the exchange of information, experiences, and indecent images of children; the organization of criminal activities that seek to use children for prostitution purposes and that produce indecent images of children at a professional level; the organization of criminal activities that promote sexual tourism". Child grooming is a process that commences with sexual sex offenders choosing a target area that is likely to attract children. In the physical world, this could be venues visited by children such as schools, shopping malls or playgrounds. A process of grooming then commences when offenders take a particular interest in the child and make them feel special with the intention of forming a bond. The Internet has greatly facilitated this process in the virtual world. Offenders now seek out their victims by visiting Internet relay chat (IRC) rooms from their home or Internet cafés at any time. Once a child victim is identified, the offender can invite it into a private area of the IRC to engage in private conversations on intimate personal details including the predator's sex life (Australian, 2008). ﻿When a business enterprise is the potential victim of computer crime, there are a number of measures that can be implemented to protect the business. In the survey by Hagen et al. (2008), they addressed both breath and depth in defense strategies. Depth is concerned with technological as well as organizational measures, while depth is concerned with dimensions of prevention, emergency preparedness and detection. The survey addressed the use of a broad range of technical security measures relating to access control and protection of data. Technical security measures include prevention (password, physical zones, biometric authentication, and software update), emergency (backup), and detection (intrusion detection and antivirus software). Organizational security measures include prevention (access rights and user guidelines), emergency (management plans), detection (log reviews), and incident response (management reports). The survey showed that the use of personal passwords is widespread among all enterprises, even the smallest ones (Hagen et al., 2008: 364): The trend is that the use of a variety of access control mechanisms increases with enterprise size. There is also a clear tendency that large enterprises implement more and a wider range of emergency preparedness and detection measures. The findings show that small enterprises should strengthen their access control and data protection measures, in addition to security routines. Hagen et al. (2008) found it surprising that large enterprises did not perform better than small enterprises when it comes to awareness raising and education of users as organizational security measures.  Profiling of criminals is based on the idea that an individual committing crime in cyberspace using a computer can fit a certain outline or profile. A profile consists of offender characteristics that represent assumptions of the offender’s personality and behavioral appearance. Characteristics can include physical build, offender sex, work ethic, mode of transportation, criminal history, skill level, race, marital status, passiveness/aggressiveness, medical history, and offender residence in relation to the crime (Nykodym et al., 2005). Nykodym et al. (2005: 413) make distinctions between four main categories of cyber crime: espionage, theft, sabotage, and personal abuse of the organizational network: Unlike saboteurs and spies, the thief is guided only by mercantile motives for his own gain. The only goal in front of the cyber thief is to steal valuable information from an organization and use it or sell it afterwards for money. In terms of criminal profiling, Nykodym et al. (2005) found that there is a strong pattern in the age of these cyber robbers. If the crime is for less than one hundred thousand dollars, then most likely the attacker is young 20-25 years old, male or female, still in the low hierarchy of the organization. If the crime involves more money, then the committer is probably an older male from a management level in the organization. His crime is not driven by hate or revenge but by greed and hunger for money.  Computer crime is defined as financial crime in this book. White-collar criminals commit financial crime. Characteristics of white-collar criminals include: • Wealthy yet greedy person • Highly educated yet practical person • Socially connected yet anti-social person • Talks ethics yet acts immoral • Employed by and in a legitimate organization • A person of respectability with high social status • Member of the privileged socioeconomic class • Commit crime within the occupation based on competence • On the slippery slope from legitimate to illegitimate behavior • Often charismatic, convincing and socially skilled • So desperate to succeed that they are willing to use criminal means • Sometimes excited about the thrill of not being uncovered • Often in a position where the police is reluctant to start investigation • Applies resources to hide tracks and crime outcome • Behaves in court in a manner creating sympathy and understanding  These kinds of characteristics are organized according to criteria in criminal profiling. For example, some of them are individual factors that are grounded in psychology, while others are environmental factors grounded in sociology. In terms of psychological factors, criminal profiling may ask question such as: • What kind of personality types become more easily white-collar criminals? • What are their typical background, life style and development? • What are their values, ideas and ambitions? In terms of sociological factors, criminal profiling may ask questions such as: • How do white-collar criminals look at society and their own role in society? • How do they perceive laws, and what do they consider to be crime and criminals? • How do they participate in networks, and what is associated with status and power? Not all computer criminals are white-collar criminals, but most of them are committing crime for financial gain. Cyber offenders are likely to share a broader range of social characteristics, and the cases of hacking and other Internet-related offences that have been reported in the media would suggest they are likely to be young, clever and fairly lonely individuals who are of middle-class origin, often without prior criminal records, often processing expert knowledge and often motivated by a variety of financial and non-financial goals. Some degree of technical competence is required to commit many computer-related types of crime (Salifu, 2008).  Some theorists believe that crime can be reduced through the use of deterrents. The goal of deterrence, crime prevention, is based on the assumption that criminals or potential criminals will think carefully before committing a crime if the likelihood of getting caught and/or the fear of swift and severe punishment are present. Based on such belief, general deterrence theory holds that crime can be thwarted by the threat of punishment, while special deterrence theory holds that penalties for criminal acts should be sufficiently severe that convicted criminals will never repeat their acts (Lyman and Potter, 2007). Threat is an external stimulus that exists whether or not an individual perceives it (Johnson and Warkentin, 2010). If an individual perceives the threat, then is has deterrent potential. Deterrence theory postulates that people commit such crimes on the basis of rational calculations about perceived personal benefits, and that the threat of legal sanctions will deter people for fear of punishment (Yusuf and Babalola, 2009). In more recent years when executives have been seen arrested and handcuffed for the purposes of public humiliation, it sets in motion a deterrence model of crime prevention or at the very least, a shaming policy. The purpose of these public arrests are often symbolic and say more about the regulatory agencies need to appear to be legitimately prosecuting corporate wrongdoers. As such, with regulation so closely tied to the political climate, there has been no consistency in the prosecution of corporate criminals, as compared with drug war policies of the past couple of decades (Hansen, 2009). ﻿Another kind of “compliance” is compliance with the rules of court procedure. In the early stages of a civil case, each side is required to supply the other with copies of any documentation potentially relevant to the issues under dispute, so that the lawsuit can be settled by reference to the relative merits of either side’s case rather than by who happens to have the most telling pieces of evidence in their hands. The traditional term for this process was discovery. In Britain this was officially changed in 1999 to disclosure, but “discovery” is still current in the rest of the English-speaking world. Because the new, electronic version of this process has developed much further to date in the USA than in Britain, the term e-discovery is commonly used on both sides of the Atlantic, and I shall use it here (though e-disclosure is sometimes used in Britain). Before the IT revolution, discovery involved legal complexities, relating for instance to classes of document (such as letters between an organization and its lawyers) which were exempt from discovery, or privileged; but it posed no great practical problems. Correspondence on paper was filed in ways that made it fairly straightforward to locate relevant material. Phone calls were not normally recorded, so the question of discovery did not arise. This changed with the arrival of e-mail. An e-mail can be saved, in which case in principle it is as subject to the discovery process as a letter or inter-office memo on paper. But e-mails are far more numerous, and they tend to be dealt with directly by the people they are addressed to rather than by secretaries who are skilled at organizing filing systems. Many people file e-mails chaotically, or at least idiosyncratically. An e-mail may not be saved by the person it was sent to but may still be retrievable from backup tapes, held at department or organization level – in which case the messages that matter will probably be mixed up with a great deal of irrelevant material. So “e-discovery” is challenging in a practical way, apart from any legal niceties involved. The main reason why e-discovery is a hot topic is that American courts have begun awarding large sums in damages against organizations that fail to produce comprehensive collections of electronic documentation. The first significant example was the 2005 case Laura Zubulake v. UBS (Union Bank of Switzerland, then Europe’s largest bank). Laura Zubulake was an equities trader earning about $650,000 a year at the New York branch of UBS; she was sacked, and sued her employer for sex discrimination. She was awarded about $29 million, part of which was compensation for loss of earnings but $20 million of which was “punitive damages” connected with the fact that UBS had failed to produce all the e-mails demanded by her lawyers – backup tapes from years past were restored to retrieve the material, but some relevant material had gone missing despite instructions given that it should be preserved. Then in Coleman (Parent) Holdings Inc. v. Morgan Stanley (2005) the plaintiff was awarded $1.45 billion, including $850 million in punitive damages for similar reasons – this was reversed on appeal, but the huge initial award shows the risk that firms now face. In both of these cases there were claims that adverse electronic evidence had deliberately been destroyed. But UBS seems to have been punished in Zubulake less for actively destroying evidence than for failing to put in place adequate mechanisms to ensure preservation of relevant material – something which is technically not at all easy to achieve, when items are scattered across directories on different servers (together with portable PDAs, memory sticks, laptops, etc.) in a complex computing environment, and when the items may be of very diverse kinds (not just e-mails but, for instance, voicemails, blogs, spreadsheets, videoconferences). Zubulake and Coleman were at least concerned with very large sums of money. But e-discovery in the USA is becoming a large problem in lesser cases. In a linked pair of cases reported as ongoing in New Jersey in 2008, Beye v. Horizon and Foley v. Horizon, where a health-insurance company was resisting paying for two teenagers’ treatments for anorexia on the ground that it might be psychological in origin, the company demanded to see practically everything the teenagers had said on their Facebook and MySpace profiles, in instant-messaging threads, text messages, e-mails, blog posts and whatever else the girls might have done online … [The court supported this demand, so] hard disks and web pages are being scoured in order for the case to proceed.67 Rebecca Love Kourlis, formerly a judge and now director of the academic Institute for the Advancement of the American Legal System, sees cases being settled out of court rather than fought to a conclusion purely because one side cannot afford the costs of e-discovery. What is more, the difficulties of e-discovery do not fall solely on the side giving the material. The receiving side then has the problem of winnowing nuggets of evidence that can actually be used to strengthen its case out of a sea of irrelevancies, peripheral material, duplicate copies, near-duplicates, messages about other people with the same surname, and so forth. Malcolm Wheeler describes e-discovery as “the single most significant change to the legal system” in his forty years as an American business lawyer.68 American companies are having to take radical steps to impose discipline on their internal communication practices, so that they will be equal to the e-discovery challenge if it arises – waiting until they are hit by a lawsuit is seen as unworkable. One suggestion, for instance, is to prohibit any use of company servers for personal e-mail – surely a draconian rule, considering how much of people’s waking lives is spent at work. A legal organization, the Sedona Conference, has been developing “Best Practice Guidelines … for Managing Information and Records in the Electronic Age” (over a hundred pages in the 2005 version), and American courts are treating compliance with the Sedona guidelines as a test of whether an organization is meeting its discovery obligations. The court system of England and Wales revised its rules on discovery (or “disclosure”) in 2005 in line with the Sedona principles for electronic documents. The English rules do differ from the American rules, in ways that mean that e-discovery in England will not lead either to such vast quantities of electronic material being handed over, or to eye-catching punitive damages awards. An English court would not require the level of discovery we saw in Beye and Foley v. Horizon.$$$﻿Another kind of “compliance” is compliance with the rules of court procedure. In the early stages of a civil case, each side is required to supply the other with copies of any documentation potentially relevant to the issues under dispute, so that the lawsuit can be settled by reference to the relative merits of either side’s case rather than by who happens to have the most telling pieces of evidence in their hands. The traditional term for this process was discovery. In Britain this was officially changed in 1999 to disclosure, but “discovery” is still current in the rest of the English-speaking world. Because the new, electronic version of this process has developed much further to date in the USA than in Britain, the term e-discovery is commonly used on both sides of the Atlantic, and I shall use it here (though e-disclosure is sometimes used in Britain). Before the IT revolution, discovery involved legal complexities, relating for instance to classes of document (such as letters between an organization and its lawyers) which were exempt from discovery, or privileged; but it posed no great practical problems. Correspondence on paper was filed in ways that made it fairly straightforward to locate relevant material. Phone calls were not normally recorded, so the question of discovery did not arise. This changed with the arrival of e-mail. An e-mail can be saved, in which case in principle it is as subject to the discovery process as a letter or inter-office memo on paper. But e-mails are far more numerous, and they tend to be dealt with directly by the people they are addressed to rather than by secretaries who are skilled at organizing filing systems. Many people file e-mails chaotically, or at least idiosyncratically. An e-mail may not be saved by the person it was sent to but may still be retrievable from backup tapes, held at department or organization level – in which case the messages that matter will probably be mixed up with a great deal of irrelevant material. So “e-discovery” is challenging in a practical way, apart from any legal niceties involved. The main reason why e-discovery is a hot topic is that American courts have begun awarding large sums in damages against organizations that fail to produce comprehensive collections of electronic documentation. The first significant example was the 2005 case Laura Zubulake v. UBS (Union Bank of Switzerland, then Europe’s largest bank). Laura Zubulake was an equities trader earning about $650,000 a year at the New York branch of UBS; she was sacked, and sued her employer for sex discrimination. She was awarded about $29 million, part of which was compensation for loss of earnings but $20 million of which was “punitive damages” connected with the fact that UBS had failed to produce all the e-mails demanded by her lawyers – backup tapes from years past were restored to retrieve the material, but some relevant material had gone missing despite instructions given that it should be preserved. Then in Coleman (Parent) Holdings Inc. v. Morgan Stanley (2005) the plaintiff was awarded $1.45 billion, including $850 million in punitive damages for similar reasons – this was reversed on appeal, but the huge initial award shows the risk that firms now face. In both of these cases there were claims that adverse electronic evidence had deliberately been destroyed. But UBS seems to have been punished in Zubulake less for actively destroying evidence than for failing to put in place adequate mechanisms to ensure preservation of relevant material – something which is technically not at all easy to achieve, when items are scattered across directories on different servers (together with portable PDAs, memory sticks, laptops, etc.) in a complex computing environment, and when the items may be of very diverse kinds (not just e-mails but, for instance, voicemails, blogs, spreadsheets, videoconferences). Zubulake and Coleman were at least concerned with very large sums of money. But e-discovery in the USA is becoming a large problem in lesser cases. In a linked pair of cases reported as ongoing in New Jersey in 2008, Beye v. Horizon and Foley v. Horizon, where a health-insurance company was resisting paying for two teenagers’ treatments for anorexia on the ground that it might be psychological in origin, the company demanded to see practically everything the teenagers had said on their Facebook and MySpace profiles, in instant-messaging threads, text messages, e-mails, blog posts and whatever else the girls might have done online … [The court supported this demand, so] hard disks and web pages are being scoured in order for the case to proceed.67 Rebecca Love Kourlis, formerly a judge and now director of the academic Institute for the Advancement of the American Legal System, sees cases being settled out of court rather than fought to a conclusion purely because one side cannot afford the costs of e-discovery. What is more, the difficulties of e-discovery do not fall solely on the side giving the material. The receiving side then has the problem of winnowing nuggets of evidence that can actually be used to strengthen its case out of a sea of irrelevancies, peripheral material, duplicate copies, near-duplicates, messages about other people with the same surname, and so forth. Malcolm Wheeler describes e-discovery as “the single most significant change to the legal system” in his forty years as an American business lawyer.68 American companies are having to take radical steps to impose discipline on their internal communication practices, so that they will be equal to the e-discovery challenge if it arises – waiting until they are hit by a lawsuit is seen as unworkable. One suggestion, for instance, is to prohibit any use of company servers for personal e-mail – surely a draconian rule, considering how much of people’s waking lives is spent at work. A legal organization, the Sedona Conference, has been developing “Best Practice Guidelines … for Managing Information and Records in the Electronic Age” (over a hundred pages in the 2005 version), and American courts are treating compliance with the Sedona guidelines as a test of whether an organization is meeting its discovery obligations. The court system of England and Wales revised its rules on discovery (or “disclosure”) in 2005 in line with the Sedona principles for electronic documents. The English rules do differ from the American rules, in ways that mean that e-discovery in England will not lead either to such vast quantities of electronic material being handed over, or to eye-catching punitive damages awards. An English court would not require the level of discovery we saw in Beye and Foley v. Horizon.
EN38	0	﻿Digital signal processing (DSP) has become a common tool for many disciplines. The topic includes the methods of dealing with digital signals and digital systems. The techniques are useful for all the branches of natural and social sciences which involve data acquisition, analysis and management, such as engineering, physics, chemistry, meteorology, information systems, financial and social services. Before the digital era, signal processing devices were dominated by analogue type. The major reason for DSP advancement and shift from analogue is the extraordinary growth and popularization of digital microelectronics and computing technology. The reason that digital becomes a trend to replace analogue systems, apart from it is a format that microprocessors can be easily used to carry out functions, high quality data storage, transmission and sophisticated data management are the other advantages. In addition, only 0s and 1s are used to represent a digital signal, noise can easily be suppressed or removed. The quality of reproduction is high and independent of the medium used or the number of reproduction. Digital images are two dimensional digital signals, which represent another wide application of digital signals. Digital machine vision, photographing and videoing are already widely used in various areas. In the field of signal processing, a signal is defined as a quantity which carries information. An analogue signal is a signal represented by a continuous varying quantity. A digital signal is a signal represented by a sequence of discrete values of a quantity. The digital signal is the only form for which the modern microprocessor can take and exercise its powerful functions. Examples of digital signals which are in common use include digital sound and imaging, digital television, digital communications, audio and video devices. To process a signal is to make numerical manipulation for signal samples. The objective of processing a signal can be to detect the trend, to extract a wanted signal from a mixture of various signal components including unwanted noise, to look at the patterns present in a signal for understanding underlying physical processes in the real world. To analyse a digital system is to find out the relationship between input and output, or to design a processor with pre-defined functions, such as filtering and amplifying under applied certain frequency range requirements. A digital signal or a digital system can be analysed in time domain, frequency domain or complex domain, etc. Representation of digital signals can be specific or generic. A digital signal is refereed to a series of numerical numbers, such as: where 2, 4, 6 are samples and the whole set of samples is called a signal. In a generic form, a digital signal can be represented as time-equally spaced data where -1, 0, 1, 2 etc are the sample numbers, x[0], x[1], x[2], etc are samples. The square brackets represent the digital form. The signal can be represented as a compact form In the signal, x[-1], x[1], x[100], etc, are the samples, n is the sample number. The values of a digital signal are only being defined at the sample number variable n , which indicates the occurrence order of samples and may be given a specific unit of time, such as second, hour, year or even century, in specific applications. We can have many digital signal examples: -- Midday temperature at Brighton city, measured on successive days, -- Daily share price, -- Monthly cost in telephone bills, -- Student number enrolled on a course, -- Numbers of vehicles passing a bridge, etc. Examples of digital signal processing can be given in the following: Example 1.1 To obtain a past 7 day’s average temperature sequence. The averaged temperature sequence for past 7 days is For example, if n=0 represents today, the past 7 days average is where x[0], x[−1], x[−2], ... represent the temperatures of today, yesterday, the day before yesterday, …; y[0] represents the average of past 7 days temperature from today and including today. On the other hand, represents the average of past 7 days temperature observed from tomorrow and including tomorrow, and so on. In a shorter form, the new sequence of averaged temperature can be written as where x[n] is the temperature sequence signal and y[n] is the new averaged temperature sequence. The purpose of average can be used to indicate the trend. The averaging acts as a low-pass filter, in which fast fluctuations have been removed as a result. Therefore, the sequence y[n] will be smoother than x[n]. Example 1.2. To obtain the past M day simple moving averages of share prices, let x[n] denotes the close price, y [n] M the averaged close price over past M days. For example, M=20 day simple moving average is used to indicate 20 day trend of a share price. M=5, 120, 250 (trading days) are usually used for indicating 1 week, half year and one year trends, respectively. Figure 1.1 shows a share’s prices with moving averages of different trading days. Although some signals are originally digital, such as population data, number of vehicles and share prices, many practical signals start off in analogue form. They are continuous signals, such as human’s blood pressure, temperature and heart pulses. A continuous signal can be first converted to a proportional voltage waveform by a suitable transducer, i.e. the analogue signal is generated. Then, for adapting digital processor, the signal has to be converted into digital form by taking samples. Those samples are usually equally spaced in time for easy processing and interpretation. Figure 1.2 shows a analogue signal and its digital signal by sampling with equal time intervals. The upper is the analogue signal x(t) and the lower is the digital signal sampled at time t = nT, where n is the sample number and T is the sampling interval. Therefore, For ease of storage or digital processing, an analogue signal must be sampled into a digital signal. The continuous signal is being taken sample at equal time interval and represented by a set of members. First of all, a major question about it is how often should an analogue signal be sampled, or how frequent the sampling can be enough to represent the details of the original signal. ﻿A digital system is also refereed as a digital processor, which is capable of carrying out a DSP function or operation. The digital system takes variety of forms, such as a microprocessor, a programmed general-purpose computer, a part of digital device or a piece of computing software. Among digital systems, linear time-invariant (LTI) systems are basic and common. For those reasons, it will be restricted to address about only the LTI systems in this whole book. The linearity is an important and realistic assumption in dealing with a large number of digital systems, which satisfies the following relationships between input and output described by Figure 3.1. i.e. a single input [ ] 1 x n produces a single output [ ] 1 y n , Applying sum of inputs [ ] [ ] 1 2 x n + x n produces [ ] [ ] 1 2 y n + y n , and applying input [ ] [ ] 1 2 ax n bx n generates [ ] [ ] 1 2 ay n by n . The linearity can be described as the combination of a scaling rule and a superposition rule. The time-invariance requires the function of the system does not vary with the time. e.g. a cash register at a supermarket adds all costs of purchased items x[n], x[n −1],… at check-out during the period of interest, and the total cost y[n] is given by where y[n] is the total cost, and if x[0] is an item registered at this moment, x[−1] then is the item at the last moment, etc. The calculation method as a simple sum of all those item’s costs is assumed to remain invariant at the supermarket, at least, for the period of interest. Like a differential equation is used to describe the relationship between its input and output of a continuous system, a difference equation can be used to characterise the relationship between the input and output of a digital system. Many systems in real life can be described by a continuous form of differential equations. When a differential equation takes a discrete form, it generates a difference equation. For example, a first order differential equation is commonly a mathematical model for describing a heater’s rising temperature, water level drop of a leaking tank, etc: where x[n] is the input and y[n] is the output. For digital case, the derivative can be described as i.e. the ratio of the difference between the current sample and one backward sample to the time interval of the two samples. Therefore, the differential equation can be approximately represented by a difference equation: yielding a standard form difference equation: For input’s derivative, we have similar digital form as Further, the second order derivative in a differential equation contains can be discretised as When the output can be expressed only by the input and shifted input, the difference equation is called non-recursive equation, such as On the other hand, if the output is expressed by the shifted output, the difference equation is a recursive equation, such as where the output y[n] is expressed by it shifted signals y[n −1] , y[n − 2], etc. In general, an LTI processor can be represented as or a short form A difference equation is not necessarily from the digitization of differential equation. It can originally take digital form, such as the difference equation in Eq.(3.1). Alternatively, equivalent to the difference equation, an LTI system can also be represented by a block diagram, which also characterises the input and output relationship for the system. For example, to draw a block diagram for the digital system described by the difference equation: The output can be rewrite as The block diagram for the system is shown in Figure 3.2. In the bock diagram, T is the sampling interval, which acts as a delay or right-shift by one sample in time. For general cases, instead of Eq.(3.9), Eq. (3.8) is used for drawing a block diagram. It can easily begin with the input, output flows and the summation operator, then add input and output branches. Both the difference equation and block diagram can be used to describe a digital system. Furthermore, the impulse response can also be used to represent the relationship between input and output of a digital system. As the terms suggest, impulse response is the response to the simplest input – unit impulse. Figure 3.2 illustrates a digital LTI system, in which the input is the unit impulse and the output is the impulse response. Once the impulse response of a system is known, it can be expected that the response to other types of input can be derived. An LTI system can be classified as causal or non-causal. A causal system is refereeing to those in which the response is no earlier than input, or h[n] =0 before n=0. This is the case for most of practical systems or the systems in the natural world. However, non-causal system can exist if the response is arranged, such as programmed, to be earlier than the excitation. The impulse response of a system can be evaluated from its difference equation. Following are the examples of finding the values of impulse responses from difference equations Example 3.1 Evaluating the impulse response for the following systems We know that when the input is the simplest unit impulse d[n], the output response will be the impulse response. Therefore, replacing input x[n] by d[n] and response y[n] by h[n], the equation is still holding and has become special: It is easy to evaluate the impulse response by letting n=-1, 0,1,2,3,… Generally for the difference equation: The impulse response can evaluated by the special equation with the simple unit impulse input: The step response is also commonly used to characterize the relationship between the input and output of a system. To find the step response using the impulse response, we know that the unit step can be expressed by unit impulses as The linear system satisfies the superposition rule. Therefore, the step response is a sum of a series of impulse responses excited by a series of shifted unit impulses. i.e., the step response is a sum of impulse responses ﻿The order of the filter can be increased to obtain a smaller passband width and to obtain a frequency response closer to the ideal ‘square’ filter. The sum filter in (4.3) had an order of 1; if we cascaded another first order sum filter, we will have the block diagram shown in Figure 4.18 (we’ll drop the constants for simplicity of discussion): Solving for z[n] in Figure 4.18 will give Eq. (4.5) could be easily verified by replacing values for x[n]. For example, using x[1]=3, x[2]=2 and x[3]=5 and computing z[3] for the single cascaded second order filter will give 12. Computation using two first order filters (i.e. y[2] and y[3]) will give the same result. It should be noted that z[n] in the example above will be defined only for n=3 onwards if x[1] is the starting point of the signal, i.e. for every order M, M initial data points will be lost in filtering. Likewise y[n] in (4.3) is defined only from y[2] onwards. For order M, we have As an example, for order M=3, we will have The magnitude response is given in Figure 4.19. The passband is about 0.302 rad or Fs/6. It could be seen that with increasing order, the passband is becoming smaller without any change in stopband. Also, the response is becoming closer to the ideal ‘square’. So, we can increase/decrease M depending on the requirements. The 3-dB cut-off frequency is given by Similarly for the HPF with order N, we will have and the magnitude response as Similarly, a BPF can be designed using a combination of LPF and HPF. This BPF is known as sum and difference (SD) filter. Different orders, M and N can be chosen to obtain the required frequency response [1]: where Gaincf is the gain at centre frequency given by For example, with filter orders of M=28 and N=8 gives the centre frequency of 40 Hz when Fs=256 Hz. The approximate 3 dB bandwidth is from 32 to 48 Hz (rounded to the nearest integer) and the gain amplification at 40 Hz is approximately 47.21. Figure 4.20 shows this example using different filter orders but with similar centre frequency (which is dependent on ratio of M/N). As another example, let us obtain the band pass FIR filter expression for orders, LPF, M=4 and HPF, N=1, i.e. obtain the band pass FIR equation that expresses z[n] in terms of x[n] and delays of x[n]. Using (4.6), obtain y[n] in term of x[n] and using (4.10), obtain z[n] in terms y[n]. Next, replace y[n] in the latter expression to arrive at The SD filter that we studied in the previous section is simple to design but for practical purposes, we often need filters that can be tailored to suit our required specifications. Consider doing an inverse DFT of the ideal LPF shown in Figure 4.1 (a) to obtain what is known as the impulse response, which are basically the filter coefficients20. The impulse response is actually the sinc function given by It would be obvious that we will not be able to use hLPF as the filter coefficients as the length is infinite. So we could use a rectangular window, w[n] to truncate the impulse response. However, by using a finite set of coefficients (i.e. impulse response), the shape of the magnitude response is changed with ripples showing up as in Figure 4.20. This is known as the Gibbs phenomenon - oscillatory behaviour in the magnitude responses caused by truncating the ideal impulse response function (i.e. the rectangular window has an abrupt transition to zero). Gibbs phenomenon can be reduced by using a window that tapers smoothly at each end such as Hamming, Hanning, triangular etc (refer to Section 3.5 in the previous chapter); providing a smooth transition from passband to stopband in the magnitude specifications. But a question which one may now raise is on the appropriate length of the filter. The filter length (i.e. order) affects the magnitude response. With a length increase, the number of ripples in both passband and stopband increases21 but with a corresponding decrease in the ripple widths, i.e. as N increases, the magnitude response approaches closer to the ideal LPF (see Figure 4.21). Similar oscillatory behaviour could be observed in the magnitude responses of the truncated versions of other types of ideal filters. There are several methods such as Kaiser and Bellanger formulas to select the ‘suitable’ order i.e. the smallest length that can meet the requirements. Kaiser’s formula [2] is given by: where p and s are the ripples in the passband and stopband, respectively while fp and fs are passband and stopband edge frequencies. It should be obvious that the actual location of the transition band is immaterial, only the transition width matters. The next issue is on the choice of window, which could be decided using the areas under main lobe and side lobes. Figure 4.22 shows two windows, Hanning and Blackman designed using order 101; the main lobe is the first ripple while other ripples are known as side lobes. To ensure a fast transition from passband to stopband, the window should have a very small main lobe width (i.e. area under the main lobe should be small). For a reduction in the ripples, the area under the sidelobes should be small (i.e. to increase the stopband attenuation, we need to decrease the sidelobe amplitudes). Most of the time, a compromise has to be met with regards to these two requirements (smaller main lobe with smaller side lobes). Consider Figure 4.23, which shows LPF design with cut-off at Fs/2, i.e. 0.5 in normalised frequency using the two windows shown in Figure 4.22. From Figure 4.23, we can see that Blackman window, which has smaller area under side lobes but bigger main lobe area, has a higher attenuation in the stopband but with a higher width transition band. The case for Hanning window is the opposite with smaller transition band but with lower stopband attenuation as it has smaller main lobe area but bigger side lobes area.$$$﻿The order of the filter can be increased to obtain a smaller passband width and to obtain a frequency response closer to the ideal ‘square’ filter. The sum filter in (4.3) had an order of 1; if we cascaded another first order sum filter, we will have the block diagram shown in Figure 4.18 (we’ll drop the constants for simplicity of discussion): Solving for z[n] in Figure 4.18 will give Eq. (4.5) could be easily verified by replacing values for x[n]. For example, using x[1]=3, x[2]=2 and x[3]=5 and computing z[3] for the single cascaded second order filter will give 12. Computation using two first order filters (i.e. y[2] and y[3]) will give the same result. It should be noted that z[n] in the example above will be defined only for n=3 onwards if x[1] is the starting point of the signal, i.e. for every order M, M initial data points will be lost in filtering. Likewise y[n] in (4.3) is defined only from y[2] onwards. For order M, we have As an example, for order M=3, we will have The magnitude response is given in Figure 4.19. The passband is about 0.302 rad or Fs/6. It could be seen that with increasing order, the passband is becoming smaller without any change in stopband. Also, the response is becoming closer to the ideal ‘square’. So, we can increase/decrease M depending on the requirements. The 3-dB cut-off frequency is given by Similarly for the HPF with order N, we will have and the magnitude response as Similarly, a BPF can be designed using a combination of LPF and HPF. This BPF is known as sum and difference (SD) filter. Different orders, M and N can be chosen to obtain the required frequency response [1]: where Gaincf is the gain at centre frequency given by For example, with filter orders of M=28 and N=8 gives the centre frequency of 40 Hz when Fs=256 Hz. The approximate 3 dB bandwidth is from 32 to 48 Hz (rounded to the nearest integer) and the gain amplification at 40 Hz is approximately 47.21. Figure 4.20 shows this example using different filter orders but with similar centre frequency (which is dependent on ratio of M/N). As another example, let us obtain the band pass FIR filter expression for orders, LPF, M=4 and HPF, N=1, i.e. obtain the band pass FIR equation that expresses z[n] in terms of x[n] and delays of x[n]. Using (4.6), obtain y[n] in term of x[n] and using (4.10), obtain z[n] in terms y[n]. Next, replace y[n] in the latter expression to arrive at The SD filter that we studied in the previous section is simple to design but for practical purposes, we often need filters that can be tailored to suit our required specifications. Consider doing an inverse DFT of the ideal LPF shown in Figure 4.1 (a) to obtain what is known as the impulse response, which are basically the filter coefficients20. The impulse response is actually the sinc function given by It would be obvious that we will not be able to use hLPF as the filter coefficients as the length is infinite. So we could use a rectangular window, w[n] to truncate the impulse response. However, by using a finite set of coefficients (i.e. impulse response), the shape of the magnitude response is changed with ripples showing up as in Figure 4.20. This is known as the Gibbs phenomenon - oscillatory behaviour in the magnitude responses caused by truncating the ideal impulse response function (i.e. the rectangular window has an abrupt transition to zero). Gibbs phenomenon can be reduced by using a window that tapers smoothly at each end such as Hamming, Hanning, triangular etc (refer to Section 3.5 in the previous chapter); providing a smooth transition from passband to stopband in the magnitude specifications. But a question which one may now raise is on the appropriate length of the filter. The filter length (i.e. order) affects the magnitude response. With a length increase, the number of ripples in both passband and stopband increases21 but with a corresponding decrease in the ripple widths, i.e. as N increases, the magnitude response approaches closer to the ideal LPF (see Figure 4.21). Similar oscillatory behaviour could be observed in the magnitude responses of the truncated versions of other types of ideal filters. There are several methods such as Kaiser and Bellanger formulas to select the ‘suitable’ order i.e. the smallest length that can meet the requirements. Kaiser’s formula [2] is given by: where p and s are the ripples in the passband and stopband, respectively while fp and fs are passband and stopband edge frequencies. It should be obvious that the actual location of the transition band is immaterial, only the transition width matters. The next issue is on the choice of window, which could be decided using the areas under main lobe and side lobes. Figure 4.22 shows two windows, Hanning and Blackman designed using order 101; the main lobe is the first ripple while other ripples are known as side lobes. To ensure a fast transition from passband to stopband, the window should have a very small main lobe width (i.e. area under the main lobe should be small). For a reduction in the ripples, the area under the sidelobes should be small (i.e. to increase the stopband attenuation, we need to decrease the sidelobe amplitudes). Most of the time, a compromise has to be met with regards to these two requirements (smaller main lobe with smaller side lobes). Consider Figure 4.23, which shows LPF design with cut-off at Fs/2, i.e. 0.5 in normalised frequency using the two windows shown in Figure 4.22. From Figure 4.23, we can see that Blackman window, which has smaller area under side lobes but bigger main lobe area, has a higher attenuation in the stopband but with a higher width transition band. The case for Hanning window is the opposite with smaller transition band but with lower stopband attenuation as it has smaller main lobe area but bigger side lobes area.
EN39	0	﻿Using the File Manager (in KDE, Konqueror or in Gnome, Nautilus) create a new directory somewhere in your home directory called something appropriate for all the examples in this book, perhaps “Programming_In_Linux” without any spaces in the name. Open an editor (in KDE, kate, or in Gnome, gedit) and type in (or copy from the supplied source code zip bundle) the following: Save the text as chapter1_1.c in the new folder you created in your home directory. Open a terminal window and type: gcc -o hello chapter1_1.c to compile the program into a form that can be executed. Now type “ls -l” to list the details of all the files in this directory. You should see that chapter1_2.c is there and a file called “hello” which is the compiled C program you have just written. Now type: ./hello to execute, or run the program and it should return the text: "Hello you are learning C!!". If this worked, congratulations, you are now a programmer! The part inside /*** ***/ is a comment and is not compiled but just for information and reference. The “#include...” part tells the compiler which system libraries are needed and which header files are being referenced by this program. In our case “printf” is used and this is defined in the stdio.h header. The “int main(int argc, char *argv[])” part is the start of the actual program. This is an entrypoint and most C programs have a main function. The “int argc” is an argument to the function “main” which is an integer count of the number of character string arguments passed in “char *argv[]” (a list of pointers to character strings) that might be passed at the command line when we run it. A pointer to some thing is a name given to a memory address for this kind of data type. We can have a pointer to an integer: int *iptr, or a floating point number: float *fPtr. Any list of things is described by [], and if we know exactly how big this list is we might declare it as [200]. In this case we know that the second argument is a list of pointers to character strings. Everything else in the curly brackets is the main function and in this case the entire program expressed as lines. Each line or statement end with a semi-colon “;”. We have function calls like “printf(...)” which is a call to the standard input / output library defined in the header file stdio.h. At the end of the program “return 0” ends the program by returning a zero to the system. Return values are often used to indicate the success or status should the program not run correctly. Taking this example a stage further, examine the start of the program at the declaration of the entry point function: int main(int argc, char *argv[]) In plain English this means: The function called “main”, which returns an integer, takes two arguments, an integer called “argc” which is a count of the number of command arguments then *argv[] which is a list or array of pointers to strings which are the actual arguments typed in when you run the program from the command line. Let's rewrite the program to see what all this means before we start to panic. Save the text as chapter1_2.c in the same folder. Open a terminal window and type: gcc -o hello2 chapter1_2.c to compile the program into a form that can be executed. Now type ls -l to list the details of all the files in this directory. You should see that chapter1_2.c is there and a file called hello2 which is the compiled C program you have just written. Now type ./hello2 to execute, or run the program and it should return the text: We can see that the name of the program itself is counted as a command line argument and that the counting of things in the list or array of arguments starts at zero not at one. Now type ./hello2 my name is David to execute the program and it should return the text: So, what is happening here? It seems we are reading back each of the character strings (words) that were typed in to run the program. Lets get real and run this in a web page. Make the extra change adding the first output printf statement “Content-type:text/plain\n\n” which tells our server what kind of MIME type is going to be transmitted. Compile using gcc -o hello3 chapter1_3.c and copy the compiled file hello3 to your public_html/cgi-bin directory (or on your own machine as superuser copy the program to /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu)). Open a web browser and type in the URL http://localhost/cgi-bin/hello3?david+haskins and you should see that web content can be generated by a C program. A seldom documented feature of the function signature for “main” is that it can take three arguments and the last one we will now look at is char *env[ ] which is also a list of pointers to strings, but in this case these are the system environment variables available to the program at the time it is run Compile with gcc -o hello4 chapter1_4.c and as superuser copy the program to /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu). You can run this from the terminal where you compiled it with ./hello4 and you will see a long list of environment variables. In the browser when you enter http://localhost/cgi-bin/hello4 you will a different set altogether. We will soon find out that QUERY_STRING is an important environment variable for us in communicating with our program and in this case we see it has a value of “david+haskins” or everything after the “?” in the URL we typed. It is a valid way to send information to a common gateway interface (CGI) program like hello4 but we should restrict this to just one string. In our case we have used a “+” to join up two strings. If we typed: “david haskins” the browser would translate this so we would see: QUERY_STRING=david%20haskins We will learn later how complex sets of input values can be transmitted to our programs. ﻿When we write programs we have to make decisions or assertions about the nature of the world as we declare and describe variables to represent the kinds of things we want to include in our information processing. This process is deeply philosophical; we make ontological assertions that this or that thing exists and we make epistemological assertions when we select particular data types or collections of data types to use to describe the attributes of these things. Heavy stuff with a great responsibility and not to be lightly undertaken. As a practical example we might declare something that looks like the beginnings of a database record for geography. Here we are doing the following: - asserting that all the character strings we will ever encounter in this application will be 255 limited to characters so we define this with a preprocessor statement – these start with #. - assert that towns are associated with counties, and counties are associated with countries some hierarchical manner. - assert that the population is counted in whole numbers – no half-people. - assert the location is to be recorded in a particular variant (WGS84) of the convention of describing spots on the surface of the world in latitude and longitude that uses a decimal fraction for degrees, minutes, and seconds. Each of these statements allocates memory within the scope of the function in which it is declared. Each data declaration will occupy an amount of memory in bytes and give that bit of memory a label which is the variable name. Each data type has a specified size and the sizeof() library function will return this as an integer. In this case 3 x 256 characters, one integer, and two floats. The exact size is machine dependent but probably it is 780 bytes. Outside the function in which the data has been declared this data is inaccessible – this is the scope of declaration. If we had declared outside the main() function it would be global in scope and other functions could access it. C lets you do this kind of dangerous stuff if you want to, so be careful. Generally we keep a close eye on the scope of data, and pass either read-only copies, or labelled memory addresses to our data to parts of the programs that might need to do work on it and even change it. These labelled memory addresses are called pointers. We are using for output the printf family of library functions (sprintf for creating strings, fprintf for writing to files etc) which all use a common format string argument to specify how the data is to be represented. - %c character - %s string - %d integer - %f floating point number etc. The remaining series of variables in the arguments are placed in sequence into the format string as specified. In C it is a good idea to intialise any data you declare as the contents of the memory allocated for them is not cleared but may contain any old rubbish. Compile with: gcc -o data1 chapter2_1.c -lc Output of the program when called with : ./data1 Some programming languages like Java and C++ have a string data type that hides some of the complexity underneath what might seem a simple thing. An essential attribute of a character string is that it is a series of individual character elements of indeterminate length. Most of the individual characters we can type into a keyboard are represented by simple numerical ASCII codes and the C data type char is used to store character data. Strings are stored as arrays of characters ending with a NULL so an array must be large enough to hold the sequence of characters plus one. Remember array members are always counted from zero. In this example we can see 5 individual characters declared and initialised with values, and an empty character array set to “”. Take care to notice the difference between single quote marks ' used around characters and double quote marks “ used around character strings. Compile with: gcc -o data2 chapter2_2.c -lc Output of the program when called with : ./data2 Anything at all – name given to a variable and its meaning or its use is entirely in the mind of the beholder. Try this Download free ebooks at bookboon.com C Programming in Linux 29 Data and Memory Compile with: gcc -o data3 chapter2_3.c -lc As superuser copy the program to your public_html/cgi-bin directory (or /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu)). In the browser enter: http://localhost/cgi-bin/data3?red what you should see is this: Or if send a parameter of anything at all you will get surprising results: What we are doing here is using the string parameter argv[1] as a background colour code inside an HTML body tag. We change the Content-type specification to text/html and miraculously now our program is generating HTML content. A language being expressed inside another language. Web browsers understand a limited set of colour terms and colours can be also defined hexadecimal codes such as #FFFFFF (white) #FF0000 (red) #00FF00 (green) #0000FF (blue). This fun exercise is not just a lightweight trick, the idea that one program can generate another in another language is very powerful and behind the whole power of the internet. When we generate HTML (or XML or anything else) from a common gateway interface program like this we are creating dynamic content that can be linked to live, changing data rather than static pre-edited web pages. In practice most web sites have a mix of dynamic and static content, but here we see just how this is done at a very simple level. Throughout this book we will use the browser as the preferred interface to our programs hence we will be generating HTML and binary image stream web content purely as a means to make immediate the power of our programs. Writing code that you peer at in a terminal screen is not too impressive, and writing window-type applications is not nearly so straightforward. In practice most of the software you may be asked to write will be running on the web so we might as well start with this idea straight away. Most web applications involve multiple languages too such as CSS, (X)HTML, XML, JavaScript, PHP, JAVA, JSP, ASP, .NET, SQL. If this sounds frightening, don't panic. A knowledge of C will show you that many of these languages, which all perform different functions, have a basis of C in their syntax. ﻿The entry point into all our programs is called main() and this is a function, or a piece of code that does something, usually returning some value. We structure programs into functions to stop them become long unreadable blocks of code than cannot be seen in one screen or page and also to ensure that we do not have repeated identical chunks of code all over the place. We can call library functions like printf or strtok which are part of the C language and we can call our own or other peoples functions and libraries of functions. We have to ensure that the appropriate header file exists and can be read by the preprocessor and that the source code or compiled library exists too and is accessible. As we learned before, the scope of data is restricted to the function in which is was declared, so we use pointers to data and blocks of data to pass to functions that we wish to do some work on our data. We have seen already that strings are handled as pointers to arrays of single characters terminated with a NULL character. In this example we can repeatedly call the function “doit” that takes two integer arguments and reurns the result of some mathematical calculation. (by now you should be maintaining a Makefile as you progress, adding targets to compile examples as you go.) The result in a browser looks like this called with “func1?5:5”. In this case the arguments to our function are sent as copies and are not modified in the function but used. If we want to actual modify a variable we would have to send its pointer to a function. We send the address of the variable 'result' with &result, and in the function doit we de-reference the pointer with *result to get at the float and change its value, outside its scope inside main . This gives identical output to chapter3_1.c. C contains a number of built-in functions for doing commonly used tasks. So far we have used atoi, printf, sizeof, strtok, and sqrt. To get full details of any built-in library function all we have to do is type for example: and we will see all this: Which pretty-well tells you everything you need to know about this function and how to use it and variants of it. Most importantly it tells you which header file to include. There is no point in learning about library functions until you find you need to do something which then leads you to look for a function or a library of functions that has been written for this purpose. You will need to understand the function signature – or what the argument list means and how to use it and what will be returned by the function or done to variables passed as pointers to functions. Sometimes we wish to manage a set of variable as a group, perhaps taking all the values from a database record and passing the whole record around our program to process it. To do this we can group data into structures. This program uses a struct to define a set of properties for something called a player. The main function contains a declaration and instantiation of an array of 5 players. We pass a pointer to each array member in turn to a function to rank each one. This uses a switch statement to examine the first letter of each player name to make an arbitrary ranking. Then we pass a pointer to each array member in turn to a function that prints out the details. The results are shown here, as usual in a browser: This is a very powerful technique that is quite advanced but you will need to be aware of it. The idea of structures leads directly to the idea of classes and objects. We can see that using a struct greatly simplifies the business task of passing the data elements around the program to have different work done. If we make a change to the definition of the struct it will still work and we simply have to add code to handle new properties rather than having to change the argument lists or signatures of the functions doing the work. The definition of the structure does not actually create any data, but just sets out the formal shape of what we can instantiate. In the main function we can express this instantiation in the form shown creating a list of sequences of data elements that conform to the definition we have made. You can probably see that a struct with additional functions or methods is essentially what a class is in Java, and this is also the case in C++. Object Oriented languages start here and in fact many early systems described as “object oriented” were in fact just built using C language structs. If you take a look for example, at the Apache server development header files you will see a lot of structs for example in this fragment of httpd.h : Dont worry about what this all means – just notice that this is a very common and very powerful technique, and the design of data structures, just like the design of database tables to which it is closely related are the core, key, vital task for you to understand as a programmer. You make the philosophical decisions that the world is like this and can be modelled in this way. A heavy responsibility - in philosophy this work is called ontology (what exists?) and epistemology (how we can know about it?). I bet you never thought that this was what you were doing! We have used some simple data types to represent some information and transmit input to a program and to organise and display some visual output. We have used HTML embedded in output strings to make output visible in a web browser. We have learned about creating libraries of functions for reuse. We have learning about data structures and the use of pointers to pass them around a program. ﻿NetBeans has a visual tool to help with the development of graphical user interfaces. This tools allows a window to be created and a range of objects to be dropped onto it including panels, tabbed panes, scrolled panes, buttons, radio buttons, check boxes, combo boxes, password fields, progress bars, and trees. When designing a graphical user interface it is ‘normal’ for objects to be placed on a window in precise positions. Interfaces are thus displayed exactly how they are designed. For this to work well an interface designer usually has some idea of the size of the display they are designing an interface for. Most PC games are designed to run on window of 14” – 21”. Most mobile games are designed to work on much smaller screens. However Java programs are expected to be platform independent – one of the strengths of Java is that code written in Java will work on a PC, laptop or small mobile device irrespective of the hardware or operating systems these devices use (as long as they all have a JRE). Thus the same interface should work on a large visual display unit or small mobile screen. For this reason when developing interfaces in Java it is usual to give Java some control over deciding exactly how / where to display objects. Thus, at run time, Java can reconfigure a window to fit whatever device is being used to run the program. This has benefits as the display will be reconfigured automatically to fit whatever device is being used to run the program but this flexibility comes at a cost. As some control is given to Java the interface designer cannot be certain exactly how the interface will look though they can give Java some ‘instructions’ on how an interface should be displayed. Layout Managers are ‘objects’ that define how an interface should be displayed and it is normal to create a layout manager and assign it to a ‘container’ object such as a window frame. There are a range of common layout managers - two of the most common being flow layout and grid layout. Using flow layout Java will arrange objects in a row and objects will automatically flow onto another row if the window is not big enough to hold each object on one row. Two windows are shown below with ten buttons on each – in the second figure Java places some of the buttons on a second row as the window has been resized and the object will not fit on one line. A flow manager can be created with various properties to define the justification of the object (centre, left or right) and to define the horizontal and vertical space between objects. Another very common layout manager is gird layout. When applying objects in a grid a number of rows can be specified. Java will work out how many columns it needs to use in order to place the required number of objects in that number of rows. The objects will be resized to fit the window. Java will shrink or enlarge the objects to fit the window – even if this means that not all of the text can be shown. The two figures below show the same 10 buttons being displayed on a grid with three rows. Note in the second of these the buttons have been resized to fit a different window – in doing so there is not enough space to display all of the text for button ten. Layout managers can be used in layers and applied to any container object Thus we can create a grid where one element contains multiple objects arranged in a flow. By using layout managers we can create displays that are flexible and can be displayed on a range of display equipment thus enabling our programs to be platform independent - however this does mean we lose some of the fine control in the design. Creating a layout managers and setting their properties using NetBeans is very easy. Right clicking on a container object will bring up a context sensitive menu one option of which will be to apply a layout manager. Selecting this option will display the list of layout managers possible. Once a layout manager has been selected a window will appear to allow its properties to be specified. The final step in creating a working interface is to create Action Listeners and apply them to object on the interface. Action listeners are objects that listen for user actions and respond accordingly – thus when a ‘calculate’ button is pushed a program will calculate and display some results. Action listeners can be created for all objects on an interface and for a range of actions for each object. Though many objects, such as labels, will be not be required to respond to a users interactions. Creating an action listener in NetBeans is very easy. When any object is selected, in design mode, a properties window is displayed. Selecting the events tab will allow a range of actions to be created for this object. Once one has been selected an ‘Action Listener’ stub will be written. Code will automatically be written to create an object of this class and assign this to the object selected in the design. All that remains is for the programmer to write the code to define the response to the users action. In the example below an event has been created to listen for button1 being pushed. One particularly useful tool within the JDK is ‘javadoc’. Programmers for many years have been burdened with the tedious and time consuming task of writing documentation. Some of this documentation is intended for users and explain what a program does and how to use it. Other documentation is intended for future programmers who will need to amend and adapt the program so that its functionality will change as the needs of an organisation change. These programmers need to know what the program does and how it is structured. They need to know :- what packages it contains what classes exist in each of these packages and what these classes do, what methods exist in each class and for each of these methods o what does the method do o what parameters does it require o what, if any, value is returned. The javadoc tool won’t produce a user guide but it will provide a technical description of the program. The tool analyses *.java source files and produces documentation as a set of web pages (HTML files) in the same format as API documentation on the Sun website.$$$﻿NetBeans has a visual tool to help with the development of graphical user interfaces. This tools allows a window to be created and a range of objects to be dropped onto it including panels, tabbed panes, scrolled panes, buttons, radio buttons, check boxes, combo boxes, password fields, progress bars, and trees. When designing a graphical user interface it is ‘normal’ for objects to be placed on a window in precise positions. Interfaces are thus displayed exactly how they are designed. For this to work well an interface designer usually has some idea of the size of the display they are designing an interface for. Most PC games are designed to run on window of 14” – 21”. Most mobile games are designed to work on much smaller screens. However Java programs are expected to be platform independent – one of the strengths of Java is that code written in Java will work on a PC, laptop or small mobile device irrespective of the hardware or operating systems these devices use (as long as they all have a JRE). Thus the same interface should work on a large visual display unit or small mobile screen. For this reason when developing interfaces in Java it is usual to give Java some control over deciding exactly how / where to display objects. Thus, at run time, Java can reconfigure a window to fit whatever device is being used to run the program. This has benefits as the display will be reconfigured automatically to fit whatever device is being used to run the program but this flexibility comes at a cost. As some control is given to Java the interface designer cannot be certain exactly how the interface will look though they can give Java some ‘instructions’ on how an interface should be displayed. Layout Managers are ‘objects’ that define how an interface should be displayed and it is normal to create a layout manager and assign it to a ‘container’ object such as a window frame. There are a range of common layout managers - two of the most common being flow layout and grid layout. Using flow layout Java will arrange objects in a row and objects will automatically flow onto another row if the window is not big enough to hold each object on one row. Two windows are shown below with ten buttons on each – in the second figure Java places some of the buttons on a second row as the window has been resized and the object will not fit on one line. A flow manager can be created with various properties to define the justification of the object (centre, left or right) and to define the horizontal and vertical space between objects. Another very common layout manager is gird layout. When applying objects in a grid a number of rows can be specified. Java will work out how many columns it needs to use in order to place the required number of objects in that number of rows. The objects will be resized to fit the window. Java will shrink or enlarge the objects to fit the window – even if this means that not all of the text can be shown. The two figures below show the same 10 buttons being displayed on a grid with three rows. Note in the second of these the buttons have been resized to fit a different window – in doing so there is not enough space to display all of the text for button ten. Layout managers can be used in layers and applied to any container object Thus we can create a grid where one element contains multiple objects arranged in a flow. By using layout managers we can create displays that are flexible and can be displayed on a range of display equipment thus enabling our programs to be platform independent - however this does mean we lose some of the fine control in the design. Creating a layout managers and setting their properties using NetBeans is very easy. Right clicking on a container object will bring up a context sensitive menu one option of which will be to apply a layout manager. Selecting this option will display the list of layout managers possible. Once a layout manager has been selected a window will appear to allow its properties to be specified. The final step in creating a working interface is to create Action Listeners and apply them to object on the interface. Action listeners are objects that listen for user actions and respond accordingly – thus when a ‘calculate’ button is pushed a program will calculate and display some results. Action listeners can be created for all objects on an interface and for a range of actions for each object. Though many objects, such as labels, will be not be required to respond to a users interactions. Creating an action listener in NetBeans is very easy. When any object is selected, in design mode, a properties window is displayed. Selecting the events tab will allow a range of actions to be created for this object. Once one has been selected an ‘Action Listener’ stub will be written. Code will automatically be written to create an object of this class and assign this to the object selected in the design. All that remains is for the programmer to write the code to define the response to the users action. In the example below an event has been created to listen for button1 being pushed. One particularly useful tool within the JDK is ‘javadoc’. Programmers for many years have been burdened with the tedious and time consuming task of writing documentation. Some of this documentation is intended for users and explain what a program does and how to use it. Other documentation is intended for future programmers who will need to amend and adapt the program so that its functionality will change as the needs of an organisation change. These programmers need to know what the program does and how it is structured. They need to know :- what packages it contains what classes exist in each of these packages and what these classes do, what methods exist in each class and for each of these methods o what does the method do o what parameters does it require o what, if any, value is returned. The javadoc tool won’t produce a user guide but it will provide a technical description of the program. The tool analyses *.java source files and produces documentation as a set of web pages (HTML files) in the same format as API documentation on the Sun website.
EN17	0	﻿Using the File Manager (in KDE, Konqueror or in Gnome, Nautilus) create a new directory somewhere in your home directory called something appropriate for all the examples in this book, perhaps “Programming_In_Linux” without any spaces in the name. Open an editor (in KDE, kate, or in Gnome, gedit) and type in (or copy from the supplied source code zip bundle) the following: Save the text as chapter1_1.c in the new folder you created in your home directory. Open a terminal window and type: gcc -o hello chapter1_1.c to compile the program into a form that can be executed. Now type “ls -l” to list the details of all the files in this directory. You should see that chapter1_2.c is there and a file called “hello” which is the compiled C program you have just written. Now type: ./hello to execute, or run the program and it should return the text: "Hello you are learning C!!". If this worked, congratulations, you are now a programmer! The part inside /*** ***/ is a comment and is not compiled but just for information and reference. The “#include...” part tells the compiler which system libraries are needed and which header files are being referenced by this program. In our case “printf” is used and this is defined in the stdio.h header. The “int main(int argc, char *argv[])” part is the start of the actual program. This is an entrypoint and most C programs have a main function. The “int argc” is an argument to the function “main” which is an integer count of the number of character string arguments passed in “char *argv[]” (a list of pointers to character strings) that might be passed at the command line when we run it. A pointer to some thing is a name given to a memory address for this kind of data type. We can have a pointer to an integer: int *iptr, or a floating point number: float *fPtr. Any list of things is described by [], and if we know exactly how big this list is we might declare it as [200]. In this case we know that the second argument is a list of pointers to character strings. Everything else in the curly brackets is the main function and in this case the entire program expressed as lines. Each line or statement end with a semi-colon “;”. We have function calls like “printf(...)” which is a call to the standard input / output library defined in the header file stdio.h. At the end of the program “return 0” ends the program by returning a zero to the system. Return values are often used to indicate the success or status should the program not run correctly. Taking this example a stage further, examine the start of the program at the declaration of the entry point function: int main(int argc, char *argv[]) In plain English this means: The function called “main”, which returns an integer, takes two arguments, an integer called “argc” which is a count of the number of command arguments then *argv[] which is a list or array of pointers to strings which are the actual arguments typed in when you run the program from the command line. Let's rewrite the program to see what all this means before we start to panic. Save the text as chapter1_2.c in the same folder. Open a terminal window and type: gcc -o hello2 chapter1_2.c to compile the program into a form that can be executed. Now type ls -l to list the details of all the files in this directory. You should see that chapter1_2.c is there and a file called hello2 which is the compiled C program you have just written. Now type ./hello2 to execute, or run the program and it should return the text: We can see that the name of the program itself is counted as a command line argument and that the counting of things in the list or array of arguments starts at zero not at one. Now type ./hello2 my name is David to execute the program and it should return the text: So, what is happening here? It seems we are reading back each of the character strings (words) that were typed in to run the program. Lets get real and run this in a web page. Make the extra change adding the first output printf statement “Content-type:text/plain\n\n” which tells our server what kind of MIME type is going to be transmitted. Compile using gcc -o hello3 chapter1_3.c and copy the compiled file hello3 to your public_html/cgi-bin directory (or on your own machine as superuser copy the program to /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu)). Open a web browser and type in the URL http://localhost/cgi-bin/hello3?david+haskins and you should see that web content can be generated by a C program. A seldom documented feature of the function signature for “main” is that it can take three arguments and the last one we will now look at is char *env[ ] which is also a list of pointers to strings, but in this case these are the system environment variables available to the program at the time it is run Compile with gcc -o hello4 chapter1_4.c and as superuser copy the program to /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu). You can run this from the terminal where you compiled it with ./hello4 and you will see a long list of environment variables. In the browser when you enter http://localhost/cgi-bin/hello4 you will a different set altogether. We will soon find out that QUERY_STRING is an important environment variable for us in communicating with our program and in this case we see it has a value of “david+haskins” or everything after the “?” in the URL we typed. It is a valid way to send information to a common gateway interface (CGI) program like hello4 but we should restrict this to just one string. In our case we have used a “+” to join up two strings. If we typed: “david haskins” the browser would translate this so we would see: QUERY_STRING=david%20haskins We will learn later how complex sets of input values can be transmitted to our programs. ﻿When we write programs we have to make decisions or assertions about the nature of the world as we declare and describe variables to represent the kinds of things we want to include in our information processing. This process is deeply philosophical; we make ontological assertions that this or that thing exists and we make epistemological assertions when we select particular data types or collections of data types to use to describe the attributes of these things. Heavy stuff with a great responsibility and not to be lightly undertaken. As a practical example we might declare something that looks like the beginnings of a database record for geography. Here we are doing the following: - asserting that all the character strings we will ever encounter in this application will be 255 limited to characters so we define this with a preprocessor statement – these start with #. - assert that towns are associated with counties, and counties are associated with countries some hierarchical manner. - assert that the population is counted in whole numbers – no half-people. - assert the location is to be recorded in a particular variant (WGS84) of the convention of describing spots on the surface of the world in latitude and longitude that uses a decimal fraction for degrees, minutes, and seconds. Each of these statements allocates memory within the scope of the function in which it is declared. Each data declaration will occupy an amount of memory in bytes and give that bit of memory a label which is the variable name. Each data type has a specified size and the sizeof() library function will return this as an integer. In this case 3 x 256 characters, one integer, and two floats. The exact size is machine dependent but probably it is 780 bytes. Outside the function in which the data has been declared this data is inaccessible – this is the scope of declaration. If we had declared outside the main() function it would be global in scope and other functions could access it. C lets you do this kind of dangerous stuff if you want to, so be careful. Generally we keep a close eye on the scope of data, and pass either read-only copies, or labelled memory addresses to our data to parts of the programs that might need to do work on it and even change it. These labelled memory addresses are called pointers. We are using for output the printf family of library functions (sprintf for creating strings, fprintf for writing to files etc) which all use a common format string argument to specify how the data is to be represented. - %c character - %s string - %d integer - %f floating point number etc. The remaining series of variables in the arguments are placed in sequence into the format string as specified. In C it is a good idea to intialise any data you declare as the contents of the memory allocated for them is not cleared but may contain any old rubbish. Compile with: gcc -o data1 chapter2_1.c -lc Output of the program when called with : ./data1 Some programming languages like Java and C++ have a string data type that hides some of the complexity underneath what might seem a simple thing. An essential attribute of a character string is that it is a series of individual character elements of indeterminate length. Most of the individual characters we can type into a keyboard are represented by simple numerical ASCII codes and the C data type char is used to store character data. Strings are stored as arrays of characters ending with a NULL so an array must be large enough to hold the sequence of characters plus one. Remember array members are always counted from zero. In this example we can see 5 individual characters declared and initialised with values, and an empty character array set to “”. Take care to notice the difference between single quote marks ' used around characters and double quote marks “ used around character strings. Compile with: gcc -o data2 chapter2_2.c -lc Output of the program when called with : ./data2 Anything at all – name given to a variable and its meaning or its use is entirely in the mind of the beholder. Try this Download free ebooks at bookboon.com C Programming in Linux 29 Data and Memory Compile with: gcc -o data3 chapter2_3.c -lc As superuser copy the program to your public_html/cgi-bin directory (or /srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu)). In the browser enter: http://localhost/cgi-bin/data3?red what you should see is this: Or if send a parameter of anything at all you will get surprising results: What we are doing here is using the string parameter argv[1] as a background colour code inside an HTML body tag. We change the Content-type specification to text/html and miraculously now our program is generating HTML content. A language being expressed inside another language. Web browsers understand a limited set of colour terms and colours can be also defined hexadecimal codes such as #FFFFFF (white) #FF0000 (red) #00FF00 (green) #0000FF (blue). This fun exercise is not just a lightweight trick, the idea that one program can generate another in another language is very powerful and behind the whole power of the internet. When we generate HTML (or XML or anything else) from a common gateway interface program like this we are creating dynamic content that can be linked to live, changing data rather than static pre-edited web pages. In practice most web sites have a mix of dynamic and static content, but here we see just how this is done at a very simple level. Throughout this book we will use the browser as the preferred interface to our programs hence we will be generating HTML and binary image stream web content purely as a means to make immediate the power of our programs. Writing code that you peer at in a terminal screen is not too impressive, and writing window-type applications is not nearly so straightforward. In practice most of the software you may be asked to write will be running on the web so we might as well start with this idea straight away. Most web applications involve multiple languages too such as CSS, (X)HTML, XML, JavaScript, PHP, JAVA, JSP, ASP, .NET, SQL. If this sounds frightening, don't panic. A knowledge of C will show you that many of these languages, which all perform different functions, have a basis of C in their syntax. ﻿The entry point into all our programs is called main() and this is a function, or a piece of code that does something, usually returning some value. We structure programs into functions to stop them become long unreadable blocks of code than cannot be seen in one screen or page and also to ensure that we do not have repeated identical chunks of code all over the place. We can call library functions like printf or strtok which are part of the C language and we can call our own or other peoples functions and libraries of functions. We have to ensure that the appropriate header file exists and can be read by the preprocessor and that the source code or compiled library exists too and is accessible. As we learned before, the scope of data is restricted to the function in which is was declared, so we use pointers to data and blocks of data to pass to functions that we wish to do some work on our data. We have seen already that strings are handled as pointers to arrays of single characters terminated with a NULL character. In this example we can repeatedly call the function “doit” that takes two integer arguments and reurns the result of some mathematical calculation. (by now you should be maintaining a Makefile as you progress, adding targets to compile examples as you go.) The result in a browser looks like this called with “func1?5:5”. In this case the arguments to our function are sent as copies and are not modified in the function but used. If we want to actual modify a variable we would have to send its pointer to a function. We send the address of the variable 'result' with &result, and in the function doit we de-reference the pointer with *result to get at the float and change its value, outside its scope inside main . This gives identical output to chapter3_1.c. C contains a number of built-in functions for doing commonly used tasks. So far we have used atoi, printf, sizeof, strtok, and sqrt. To get full details of any built-in library function all we have to do is type for example: and we will see all this: Which pretty-well tells you everything you need to know about this function and how to use it and variants of it. Most importantly it tells you which header file to include. There is no point in learning about library functions until you find you need to do something which then leads you to look for a function or a library of functions that has been written for this purpose. You will need to understand the function signature – or what the argument list means and how to use it and what will be returned by the function or done to variables passed as pointers to functions. Sometimes we wish to manage a set of variable as a group, perhaps taking all the values from a database record and passing the whole record around our program to process it. To do this we can group data into structures. This program uses a struct to define a set of properties for something called a player. The main function contains a declaration and instantiation of an array of 5 players. We pass a pointer to each array member in turn to a function to rank each one. This uses a switch statement to examine the first letter of each player name to make an arbitrary ranking. Then we pass a pointer to each array member in turn to a function that prints out the details. The results are shown here, as usual in a browser: This is a very powerful technique that is quite advanced but you will need to be aware of it. The idea of structures leads directly to the idea of classes and objects. We can see that using a struct greatly simplifies the business task of passing the data elements around the program to have different work done. If we make a change to the definition of the struct it will still work and we simply have to add code to handle new properties rather than having to change the argument lists or signatures of the functions doing the work. The definition of the structure does not actually create any data, but just sets out the formal shape of what we can instantiate. In the main function we can express this instantiation in the form shown creating a list of sequences of data elements that conform to the definition we have made. You can probably see that a struct with additional functions or methods is essentially what a class is in Java, and this is also the case in C++. Object Oriented languages start here and in fact many early systems described as “object oriented” were in fact just built using C language structs. If you take a look for example, at the Apache server development header files you will see a lot of structs for example in this fragment of httpd.h : Dont worry about what this all means – just notice that this is a very common and very powerful technique, and the design of data structures, just like the design of database tables to which it is closely related are the core, key, vital task for you to understand as a programmer. You make the philosophical decisions that the world is like this and can be modelled in this way. A heavy responsibility - in philosophy this work is called ontology (what exists?) and epistemology (how we can know about it?). I bet you never thought that this was what you were doing! We have used some simple data types to represent some information and transmit input to a program and to organise and display some visual output. We have used HTML embedded in output strings to make output visible in a web browser. We have learned about creating libraries of functions for reuse. We have learning about data structures and the use of pointers to pass them around a program. ﻿A pointer is a special kind of variable which contains a memory address to for instance a number instead of directly refer to the number. That implies a detour to the value via the memory address. This might sound unnecessarily complicated, but implies a number of advantages like for instance more efficient program code, faster execution and memory saving. Especially in object oriented programming you get these advantages when copying objects or sending objects to functions. Object oriented programming is however beyond the scope of this course. The pointer concept is unique for C++. It is for instance not present in the programming languages Visual Basic or Java. As a consequence C++ might be felt more complicated than other languages. In this chapter we will aqcuire basic knowledge about pointers. We will learn how to use pointers to different data types, how to declare pointers and assign values. We will examine the anology between pointers and arrays and how to use pointers as parameters to functions. Finally we will touch the subject dynamic memory allocation, which actually does not closely relate to pointers, but still often is used in connection with pointers. A pointer is a variable of a special kind which only can contain a memory address of the primary memory. This memory location in turn contains a value of some kind. Le tus first study the situation for a common variable In the figure above we have the variable iNumber which contains the actual value of the variable, in our example 34. Let us now focus on the corresponding pointer: In the figure above we have a pointer named pNumber. It contains an address in the primary memory. If we go to that address, there is a number, 23 in our example. The asterisc (*) indicates that it is a pointer. int* means that it is a pointer to an integer value. You must always specify the data type pointed to by the pointer variable. Below we declare a pointer to a double value: Below we declare a pointer to a char value: You can as well place the space in front of the asterisc. The declarations above could be written: You can use both variants. To get the address to the variable iNo, we use the & operator. The expression &iNo gives the address to the variable iNo. In the declaration you can specify the memory location to be pointed at by a pointer variable: Here we create a pointer variable named pNumber and assign the address of the variable iNo to it. The variable iNo and the pointer variable pNumber now points to the same memory location, which means the value 23. Note that in a pointer declaration you can’t directly assign a fixed value: since currently there is no specific memory location pointed to by pNumber. However, when pNumber has got its memory address, we can change the value in the location indicated by pNumber: Here we must remember to use the asterisc together with the name of the pointer variable. The program then understands that it is the value that is to be changed. Compare this to this erroneous statement: This would mean that we updated the address pointed to by pNumber. The address 25 would be pointed to, which of course is erroneous. We have introduced two operators in connection with pointers: * means ‘the content of ’ & means ‘the address to’ In the same way we can write: When printing a value pointed to by a pointer variable, you use: This means ‘print the content of pNumber’. To print the address pointed to by a pointer variable you write: The printed address is in hexadecimal format. Normally we don’t have to bother about the exact address. The only thing to remember is whether we mean ‘the address to’ or ‘the content of ’. We will now take a look at how pointers work in connection with string variables, i.e. arrays of char type. We declare a string array named cName: We then declare a char pointer named pName which points to the same text as the content of cName. Why didn’t we use the & operator in front of cName like in the previous example? The explanation is that an array actually is a pointer. When using the name of the array, cName, it is interpreted as a pointer to the first item of the array. So when writing the statement: it means that we let the pointer pName get the same address as the pointer (array) cName. The print function cout has some peculiarities you ought to know when printing strings. The statement: should actually print the address in hexadecimal format of pName. But cout performs a reinterpretation. It takes the content in the memory location pointed to by pName, i.e. the character ‘J’, and prints character by character until the null character is found. This means that the entire name ‘John Smith’ is printed. Compare the statement: which gives the same result, which we discussed in the Strings chapter. The statement: correctly prints the content of the memory location pointed to by pName, but it only takes that character. This means that only ‘J’ is printed. The statement: prints the address of the memory location in which pName is stored. The statement prints the address of the memory location where the name ’John Smith’ is stored. We will now create a program which reads quantity and unit price of a product from the user, and the name of the user. The program will then calculate the total price of the product and print a personal price note on the screen. We will use pointer variables. The logical process is given by the following JSP graph: Let us say that we enter ‘John Smith’, quantity 5 and unit price 12. Then the printout will be: By pointer arithmetics we mean how to increment and decrement a pointer, i.e. how to make a pointer to an array move stepwise from item to item. Let’s say that we have an array of integers:$$$﻿A pointer is a special kind of variable which contains a memory address to for instance a number instead of directly refer to the number. That implies a detour to the value via the memory address. This might sound unnecessarily complicated, but implies a number of advantages like for instance more efficient program code, faster execution and memory saving. Especially in object oriented programming you get these advantages when copying objects or sending objects to functions. Object oriented programming is however beyond the scope of this course. The pointer concept is unique for C++. It is for instance not present in the programming languages Visual Basic or Java. As a consequence C++ might be felt more complicated than other languages. In this chapter we will aqcuire basic knowledge about pointers. We will learn how to use pointers to different data types, how to declare pointers and assign values. We will examine the anology between pointers and arrays and how to use pointers as parameters to functions. Finally we will touch the subject dynamic memory allocation, which actually does not closely relate to pointers, but still often is used in connection with pointers. A pointer is a variable of a special kind which only can contain a memory address of the primary memory. This memory location in turn contains a value of some kind. Le tus first study the situation for a common variable In the figure above we have the variable iNumber which contains the actual value of the variable, in our example 34. Let us now focus on the corresponding pointer: In the figure above we have a pointer named pNumber. It contains an address in the primary memory. If we go to that address, there is a number, 23 in our example. The asterisc (*) indicates that it is a pointer. int* means that it is a pointer to an integer value. You must always specify the data type pointed to by the pointer variable. Below we declare a pointer to a double value: Below we declare a pointer to a char value: You can as well place the space in front of the asterisc. The declarations above could be written: You can use both variants. To get the address to the variable iNo, we use the & operator. The expression &iNo gives the address to the variable iNo. In the declaration you can specify the memory location to be pointed at by a pointer variable: Here we create a pointer variable named pNumber and assign the address of the variable iNo to it. The variable iNo and the pointer variable pNumber now points to the same memory location, which means the value 23. Note that in a pointer declaration you can’t directly assign a fixed value: since currently there is no specific memory location pointed to by pNumber. However, when pNumber has got its memory address, we can change the value in the location indicated by pNumber: Here we must remember to use the asterisc together with the name of the pointer variable. The program then understands that it is the value that is to be changed. Compare this to this erroneous statement: This would mean that we updated the address pointed to by pNumber. The address 25 would be pointed to, which of course is erroneous. We have introduced two operators in connection with pointers: * means ‘the content of ’ & means ‘the address to’ In the same way we can write: When printing a value pointed to by a pointer variable, you use: This means ‘print the content of pNumber’. To print the address pointed to by a pointer variable you write: The printed address is in hexadecimal format. Normally we don’t have to bother about the exact address. The only thing to remember is whether we mean ‘the address to’ or ‘the content of ’. We will now take a look at how pointers work in connection with string variables, i.e. arrays of char type. We declare a string array named cName: We then declare a char pointer named pName which points to the same text as the content of cName. Why didn’t we use the & operator in front of cName like in the previous example? The explanation is that an array actually is a pointer. When using the name of the array, cName, it is interpreted as a pointer to the first item of the array. So when writing the statement: it means that we let the pointer pName get the same address as the pointer (array) cName. The print function cout has some peculiarities you ought to know when printing strings. The statement: should actually print the address in hexadecimal format of pName. But cout performs a reinterpretation. It takes the content in the memory location pointed to by pName, i.e. the character ‘J’, and prints character by character until the null character is found. This means that the entire name ‘John Smith’ is printed. Compare the statement: which gives the same result, which we discussed in the Strings chapter. The statement: correctly prints the content of the memory location pointed to by pName, but it only takes that character. This means that only ‘J’ is printed. The statement: prints the address of the memory location in which pName is stored. The statement prints the address of the memory location where the name ’John Smith’ is stored. We will now create a program which reads quantity and unit price of a product from the user, and the name of the user. The program will then calculate the total price of the product and print a personal price note on the screen. We will use pointer variables. The logical process is given by the following JSP graph: Let us say that we enter ‘John Smith’, quantity 5 and unit price 12. Then the printout will be: By pointer arithmetics we mean how to increment and decrement a pointer, i.e. how to make a pointer to an array move stepwise from item to item. Let’s say that we have an array of integers:
EN34	1	﻿While there is a study guide (available from Ventus) that focuses largely on objects and their characteristics, it will be instructive to the learner (of the Java programming language) to understand how the concept of an object is applied to their construction and use in Java applications. Therefore, Chapter One (of this guide) introduces the concept of an object from a language-independent point of view and examines the essential concepts associated with object-oriented programming (OOP) by briefly comparing how OOP and non-OOP approach the representation of data and information in an application. The chapter goes on to explain classes, objects and messages and concludes with an explanation of how a class is described with a special diagram known as a class diagram.  Despite the wide use of OOP languages such as Java, C++ and C#, non-OOP languages continue to be used in specific domains such as for some categories of embedded applications. In a conventional, procedural language such as C, data is sent to a procedure for processing; this paradigm of information processing is illustrated in Figure 1.1 below.  The figure shows that the number 4 is passed to the function (SQRT) which is ‘programmed’ to calculate the result and output it (to the user of the procedure). In general, we can think of each procedure in an application as ready and waiting for data items to be sent to them so that they can do whatever they are programmed to do on behalf of the user of the application. Thus an application written in C will typically comprise a number of procedures along with ways and means to pass data items to them.  The way in which OOP languages process data, on the other hand, can be thought of as the inverse of the procedural paradigm. Consider Figure 1.2 below.  In the figure, the data item – the number 4 – is represented by the box (with the label ‘4’ on its front face). This representation of the number 4 can be referred to as the object of the number 4. This simple object doesn’t merely represent the number 4, it includes a button labeled sqrt which, when pressed, produces the result that emerges from the slot labeled return. Whilst it is obvious that the object-oriented example is expected to produce the same result as that for the procedural example, it is apparent that the way in which the result is produced is entirely different when the object-oriented paradigm considered. In short, the latter approach to producing the result 2 can be expressed as follows.  A message is sent to the object to tell it what to do. Other messages might press other buttons associated with the object. However for the present purposes, the object that represents the number 4 is a very simple one in that it has only one button associated with it. The result of sending a message to the object to press its one and only button ‘returns’ another object. Hence in Figure 1.2, the result that emerges from the ‘return’ slot - the number 2 – is an object in its own right with its own set of buttons. Despite the apparent simplicity of the way in which the object works, the question remains: how does it calculate the square root of itself? The answer to this question enshrines the fundamental concept associated with objects, which is to say that objects carry their programming code around with them. Applying this concept to the object shown in Figure 1.2, it has a button which gives access to the programming code which calculates the square root (of the number represented by the object). This amalgam of data and code is further illustrated by an enhanced version of the object shown in Figure 1.3 below.  The enhanced object (representing the number 4) has two buttons: one to calculate the square root of itself – as before - and a second button that adds a number to the object. In the figure, a message is sent to the object to press the second button – the button labeled ‘+’ – to add the object that represents the number 3 to the object that represents the number 4. For the ‘+’ button to work, it requires a data item to be sent to it as part of the message to the object. This is the reason why the ‘+’ button is provided with a slot into which the object representing the number 3 is passed. The format of the message shown in the figure can be expressed as follows.  When this message is received and processed by the object, it returns an object that represents the number 7. In this case, the message has accessed the code associated with the ‘+’ button. The enhanced object can be thought of as having two buttons, each of which is associated with its own programming code that is available to users of the object.  Extrapolating the principal of sending messages to the object depicted in Figure 1.3 gives rise to the notion that an object can be thought of as comprising a set of buttons that provide access to operations which are carried out depending on the details in the messages sent to that object.  In summary: in procedural programming languages, data is sent to a procedure; in an object-oriented programming language, messages are sent to an object; an object can be thought of as an amalgam of data and programming code: this is known as encapsulation.  Whilst the concept of encapsulation is likely to appear rather strange to learners who are new to OOP, working with objects is a much more natural way of designing applications compared to designing them with procedures. Objects can be constructed to represent anything in the world around us and, as such, they can be easily re-used or modified. Given that we are surrounded by things or objects in the world around us, it seems natural and logical that we express this in our programming paradigm.  The next section takes the fundamental concepts explored in this section and applies them to a simple object.  ﻿The aim of Chapter Two is to take the simple class diagram shown at the end of Chapter One and explain how it is translated into Java source code. The code is explained in terms of its attributes, constructor and behaviour and a test class is used to explain how its constructor and behaviour elements are used. Before we embark on our first Java programme, let us recall the class diagram with which we concluded Chapter One. The class diagram is reproduced in Figure 2.1 below, with the omission of the constructor: this is to keep the code simple to begin with. We will replace the constructor in the class diagram and provide code for it later in this chapter. In Figure 2.1, let us be reminded that the qualifier ‘-‘ means private and the qualifier ‘+’ means public. The purpose of these qualifiers will be revealed when we write the code for the class. The next section explains how the information in the class diagram shown in Figure 2.1 is translated into Java source code. Remember that, in general, a class definition declares attributes and defines constructors and behaviour. The Java developer concentrates on writing types called classes, as a result of interpreting class diagrams and other elements of the OOA & D of an application’s domain. The Java developer also makes extensive use of the thousands of classes provided by the originators of the Java language (Sun Microsystems Inc.) that are documented in the Java Applications Programming Interface (API). We have established that classes typically comprise attributes and the behaviour that is used to manipulate these data. Attributes are implemented, in Java, as variables, whose value determines the condition or state of an object of that class and behaviour elements are implemented using a construct known as a method. When a method is executed, it is said to be called or invoked. As has been mentioned earlier, an instance of a class is also called an object, such that, perhaps somewhat confusingly, the terms instance and object are interchangeable in Java. The requirement to create an instance of a class from the definition of the class gives rise to a fundamental question: how do we actually create an instance of a class so that its methods can be executed? We will address this question in this section. One of the components of a class, which we haven’t explained fully so far in the discussion of the Member class, is its constructor. A constructor is used to create or construct an instance of that class. Object construction is required so that the Java run-time environment (JRE) can respond to a call to an object’s constructor to create an actual object and store it in memory. An instance does not exist in memory until its constructor is called; only its class definition is loaded by the (JRE). We will meet the constructor for the Member class later. Broadly, then, we can think of the Java developer as writing Java classes, from which objects can be constructed (by calling their constructors). Classes are to objects as an architect’s plan is to a house, i.e. we can produce many houses from a single plan and we can construct or instantiate many instances from a single template known as a class. Given that objects can communicate with other objects, this gives the developer the means to re-use classes from one application in another application. Therefore, with Java object technology, we can build software applications by combining re-useable and interchangeable objects, some of which can be standardised in terms of their interface. This is probably the single-most important advantage of object-oriented programming (OOP) compared with non-OOP in application development. We are now at the stage when we can translate the class diagram for the Member class into Java source code, often shortened to ‘code’. The code that follows is the class definition of the class named Member but includes only some of the attributes and methods that do not involve object types: this is to keep the example straightforward. The reason for this restriction is that if we were to declare attributes or parameters of the MembershipCard class type in the class Member, as required by the class diagram, the Java compiler would look for the class definition of the class MembershipCard. In order to keep the example straightforward, we will only write the class definition for the class Member for the time being; we will refer to the class definition of the class MembershipCard in a later chapter. Thus, in this section, we will work with a single class that includes only primitive data types; there are no class types included in the simplified class diagram. In order to make the example code even more straightforward, the class diagram is further simplified as shown in the next diagram. The class diagram that we will translate into Java code declares two variables and their corresponding ‘setter’ (or mutator) and ‘getter’ (or accessor) methods, as follows. The reason for the simplification (of the full class diagram) is so that the class definition can be more easily understood, compared to its full definition. In short, we well keep our first Java programme as simple as possible. In the class definition that follows below, ‘ // ‘ is a single-line comment and ‘ /** … */ ‘ is a block comment and, as such, are ignored by the Java compiler. For the purposes of the example, Java statements are written in bold and comments in normal typeface. // Class definition for the class diagram shown in Figure 2.2. Note that the name of // the class starts, by convention, with a capital letter and that it is declared as public. // The first Java statement is the class declaration. Note that the words public and // class must begin with a lower case letter. public class Member { // The class declaration. // Declare instance variables first. Things to note: // String types in Java are objects and are declared as ‘String’, not ‘string’. // The qualifier 'private' is used for variables. // 'String' is a type and 'userName' and ‘password’ are variable names, also // known as identifiers. Thus, we write the following: ﻿In Chapter Two, we see that class attributes are implemented in Java programmes as variables, whose values determine the state of an object. To some extent Chapter Two addresses the question of how we name variables; this question is explored further in this chapter. Chapter Three explores some of the basic elements of the Java language. Given the nature of this guide, it is not the intention to make this chapter exhaustive with respect to all of the basic elements of the Java language. Further details can be found in the on-line Java tutorial. We see in Chapter Two that the two broad categories of Java types are primitives and classes. There are eight of the former and a vast number of classes, including several thousand classes provided with the Java language development environment and an infinitude of classes written by the worldwide community of Java developers. This chapter examines aspects of both categories of types. An identifier is a meaningful name given to a component in a Java programme. Identifiers are used to name the class itself – where the name of the class starts with an upper case letter – and to name its instances, its methods and their parameters. While class identifiers always – by convention – start with an upper case letter, everything else is identified with a word (or compound word) that starts with a lower case letter. Identifiers should be made as meaningful as possible, in the context of the application concerned. Thus compound words or phrases are used in practice. Referring to elements of the themed application, we can use the following identifiers for variables in the Member class: because we wouldn’t name a class membershipCard and spaces are not permitted in identifiers. We could have declared other variables in the class definition as follows: We cannot use what are known as keywords for identifiers. These words are reserved and cannot be used solely as an identifier, but can be used as part of an identifier. Thus we cannot identify a variable as follows: // not permitted because int is a keyword but we could write The table below lists the keywords in the Java language. Java is case-sensitive: this means that we cannot expect the following statement to compile: if we have not previously declared the identifier newint. On the other hand, if we write as the last statement of the getNewInt method, it will compile because the identifier named newInt has been declared previously. Similarly we cannot expect the compiler to recognise identifiers such as the following if they have not been declared before we refer to them later in our code. In one of the declarations in Section 3.2, we declared a variable with the identifier newInt to be of the int type, in the following statement: Let us deconstruct this simple statement from right to left: we declare that we are going to use an identifier named newInt to refer to integer values and ensure that access to this variable is private. This kind of declaration gives rise to an obvious question: what primitive data types are there in the Java language? The list on the next page summarises the primitive data types supported in Java. Before we move on to discuss assignment of actual values to variables, it will be instructive to find out if Java can convert between types automatically or whether this is left to the developer and if compile-time and run-time rules for conversion between types are different. In some situations, the JRE implicitly changes the type without the need for the developer to do this. All conversion of primitive data types is checked at compile-time in order to establish whether or not the conversion is permissible. Consider, for example, the following code snippet: A value of 10.0 is displayed when d is output. Evidently the implicit conversion from an int to a double is permissible. Consider this code snippet: The first statement compiles; this means that the implicit conversion from an int to a double is permissible when we assign a literal integer value to a double. However the second statement does not compile: the compiler tells us that there is a possible loss of precision. This is because we are trying to squeeze, as it were, an eight byte value into a four byte value (see Table 3.2); the compiler won’t let us carry out such a narrowing conversion. On the other hand, if we write: // the cast ( int ) forces d to be an int; we will examine the concept of casting // or explicit conversion later in this section Both statements compile and a value of 10 is displayed when i is output. The general rules for implicit assignment conversion are as follows: a boolean cannot be converted to any other type; a non-boolean type can be converted to another non-boolean type provided that the conversion is a widening conversion; a non-boolean type cannot be converted to another non-boolean type if the conversion is a narrowing conversion. Another kind of conversion occurs when a value is passed as an argument to a method when the method defines a parameter of some other type. For example, consider the following method declaration: The method is expecting a value of a double to be passed to it when it is invoked. If we pass a float to the method when it is invoked, the float will be automatically converted to a double. Fortunately the rules that govern this kind of conversion are the same as those for implicit assignment conversion listed above. The previous sub-section shows that Java is willing to carry out widening conversions implicitly. On the other hand, a narrowing conversion generates a compiler error. Should we actually intend to run the risk of the possible loss of precision when carrying out a narrowing conversion, we must make what is known as an explicit cast. Let us recall the following code snippet from the previous sub-section: Casting means explicitly telling Java to force a conversion that the compiler would otherwise not carry out implicitly. To make a cast, the desired type is placed between brackets, as in the second statement above, where the type of d – a double - is said to be cast (i.e. flagged by the compiler to be converted at run-time) into an int type.  ﻿By now the learner will be familiar, to some extent, with method invocation from earlier chapters, when objects of the Member class in the themed application are used to give some examples of passing arguments to methods. Chapter Four goes into more detail about methods and gives a further explanation about how methods are defined and used. Examples from the themed application are used to illustrate the principal concepts associated with an object’s methods. Chapter Three examines an object’s variables, i.e. its state or what it knows what its values are. An object’s methods represent the behaviour of an object, or what is knows what it can do, and surround, or encapsulate, an object’s variables. This section answers the question about how we get computable values into methods. As we know from previous chapters, a method is invoked by selecting the object reference for the instance required. The general syntax of a method invocation can be summarised as follows. Referring, again, to the Member class of the themed application, we could instantiate a number of Member objects (in a main method) and call their methods as in the following code snippet. // Instantiate three members; call the no-arguments constructor for the Member class. // Call one of the set methods of these objects. // Call one of the get methods of these objects in a print statement. The screen output from executing this fragment of main is: In short, we must ensure that we know which method we are calling on which object and in which order. In the code snippet above, it is evident that setUserName expects a String argument to be passed to it; this is because its definition is written as: The single parameter is replaced by a computable value, i.e. an argument, when the method is invoked. The general syntax of a method’s declaration is modifier return_type method_name( parameter_list ) exception_list The method’s definition is its declaration, together with the body of the method’s implementation between braces, as follows: The method’s signature is its name and parameter list. It is in the body of a method where application logic is executed, using statements such as: invocations: calls to other methods; assignments: changes to the values of fields or local variables; selection: cause a branch; repetition: cause a loop; detect exceptions, i.e. error conditions. If the identifier of a parameter is the same as that of an instance variable, the former is said to hide the latter. The compiler is able to distinguish between the two identifiers by the use of the keyword ‘this’, as in the following method definition that we met in Chapter One: If, on the other hand, we wish to avoid hiding, we could write the method definition as follows: where the identifier of the parameter is deliberately chosen to be different from that of the instance variable. In this case, the keyword ‘this’ can be included but it is not necessary to do so. In both versions of the method setUserName, the value of the parameter’s argument has scope only within the body of the method. Thus, in general, arguments cease to exist when a method completes its execution. A final point to make concerning arguments is that a method cannot be passed as an argument to another method or a constructor. Instead, an object reference is passed to the method or constructor so that the object reference is made available to that method or constructor or to other members of the class that invoke that method. For example, consider the following code snippet from the graphical version of the themed application shown on the next page. The examples and discussion in this section are meant to raise a question in the mind of the learner: are arguments passed by value or by reference? This question is addressed in the next sub-section. All arguments to methods (and constructors) are, in Java, passed by value. This means that a copy of the argument is passed in to a method (or a constructor) call. The example that follows aims to illustrate what pass by value semantics means in practice: detailed code documentation is omitted for the sake of clarity. The method changeValue changes the value of the argument passed to it – a copy of x – but it does not change the original value of x, as shown by the output. Thus the integer values 1235 and 1234 are output according to the semantics of pass by value as they apply to arguments. When a parameter is an object reference, it is a copy of the object reference that is passed to the method. You can change which object the argument refers to inside the method, without affecting the original object reference that was passed. However if the body of the method calls methods of the original object – via the copy of its reference - that change the state of the object, the object’s state is changed for the duration of its scope in a programme. Thus, in the example above, the strings “Bonjour” and “Hello there!” are output according to the semantics of pass by value as they apply to object references. A common misconception about passing object references to methods or constructors is that Java uses pass by reference semantics. This is incorrect: pass by reference would mean that if used by Java, the original reference to the object would be passed to the method or constructor, rather than a copy of the reference, as is the case in Java. The Java language passes object references by value, in that a copy of the object reference is passed to the method or constructor. The statement in the box isn’t true when objects are passed amongst objects in a distributed application. However, such applications are beyond the scope of this guide. For the purposes of the present guide, the learner should use the examples above to understand the consequences of Java’s use of pass by value semantics. In previous chapters, we have encountered a number of references to a method’s return type. In the definition of a method, the return type is declared as part of the method’s declaration and its value is returned by the final statement of the method.  ﻿There are several examples in previous chapters that illustrate how constructors are used to instantiate objects of a class. Let us recall the overall technique before we bring together a number of features of constructors in this chapter. One of the constructors for Member objects in the themed application is as follows: An object’s constructors have the same name as the class they instantiate. To access an object of the class Member in an application, we first declare a variable of the Member type in a main method in a test class as follows: The statement above does not create a Member object; it merely declares a variable of the required type that can subsequently be initialised to refer to an instance of the Member type. The variable that refers to an object is known as its object reference. The object that an object reference refers to must be created explicitly, in a statement that instantiates a Member object as follows. The two statements above can be combined as follows. When the Member object is created by using ‘new’, the type of object required to be constructed is specified and the required arguments are passed to the constructor. The JRE allocates sufficient memory to store the fields of the object and initialises its state. When initialisation is complete, the JRE returns a reference to the new object. Thus, we can regard a constructor as returning an object reference to the object stored in memory. While objects are explicitly instantiated using ‘new’, as shown above for a Member object, there is no need to explicitly destroy them (as is required in some OO run-time systems). The Java Virtual Machine (JVM) manages memory on behalf of the developer so that memory for objects that is no longer used in an application is automatically reclaimed without the intervention of the developer. In general, an object’s fields can be initialised when they are declared or they can be declared without being initialised. For example, the code snippet on the next page shows part of the class declaration for a version of the Member class: The code snippet illustrates an example where some of the instance variables are initialised and some are only declared. In the case of the latter type of declaration, the instance variable is initialised to its default value when the constructor returns an object reference to the newly-created object. For example, the instance variable noOfCards is initialised to 0 when the object is created. Declaring and initialising none, some or all instance variables in this way if often sufficient to establish the initial state of an object. On the other hand, where more than simple initialisation to literals or default values is required and where other tasks are required to be performed, the body of a constructor can be used to do the work of establishing the initial state of an object. Consider the following part of the constructor for the Member class. This constructor is used when simple initialisation of Member objects is insufficient. Thus, in the code block of the constructor above, the arguments passed to the constructor are associated with four of the fields of the Member class. The effect of the four statements inside the constructor’s code block is to initialise the four fields before the constructor returns a reference to the object. Constructors can, like methods, generate or throw special objects that represent error conditions. These special objects are instances of Java’s in-built Exception class. We will explore how to throw and detect Exception objects in Chapter Four in An Introduction to Java Programming 2: Classes in Java Applications. It is worthwhile being reminded at this point in the discussion about constructors that the compiler inserts a default constructor if the developer has not defined any constructors for a class. The default constructor takes no arguments and contains no code. It is provided automatically only if the developer has not provided any constructors in a class definition. We saw in the previous chapter that methods can be overloaded. Constructors can be similarly overloaded to provide flexibility in initialising the state of objects of a class. For example, the following class definition includes more than one constructor. The example class – SetTheTime – is a simple illustration of a class which provides more than one constructor. The example also shows that a constructor can be called from the body of another constructor by using the ‘this’ invocation as the first executable statement in the constructor. Thus, in the example above, the two argument constructor is called in the first statement of the three argument constructor. Complex initialisation of fields can be achieved by using what is known as an initialisation block. An initialisation block is a block of statements, delimited by braces, that appears near the beginning of a class definition outside of any constructor definitions. The position of such a block can be generalised in the following simple template for a typical class definition: An initialisation block is executed as if it were placed at the beginning of every constructor of a class. In other words, it represents a common block of code that every constructor executes. Thus far, in this study guide, we have only been able to work with single values of primitive data types and object references. In the next chapter, we will find out how we can associate multiple values of types with a single variable so that we can work with multiple values of primitives or object references in an application. ﻿The thread object is instantiated by calling the start method of the Thread class; this invocation places the thread in a runnable state, which means that it becomes available for scheduling by the JVM. The start method automatically calls the thread’s run method. The thread is typically instantiated in a separate, lightweight class that includes a main method. A template for this class follows. The body of the main method instantiates an instance of the thread by calling its no-arguments constructor; the start method of the thread is called on this instance. A template for the class definition of a class that implements the Runnable interface is as follows. The next template instantiates objects of the class that implements the Runnable interface. In this case, the body of the main method instantiates an instance of the class that implements the Runnable interface and it passes this object reference to the constructor of the Thread class that takes a Runnable object as its only parameter. The start method of the thread is called, as before. The outcome of using either of the two methods that can be used to create threads is that the developer provides the body of the thread’s run method; it is this method that does the work that the thread is required to do in a Java application. There are a number of tasks that could be designed to execute in a dedicated thread; these include: • I/O tasks that require substantial resources or are large in scale; • large-scale printing tasks where the print driver executes in its own thread; • synchronised multiple read/write tasks; • server applications that provide an application service to multiple clients. The fourth item in the list above identifies applications that require shared access to resources from multiple client applications and implies a high degree of synchronisation in order to maintain the integrity and security of data. The next sub-section explains, with an example, how synchronisation can be achieved. It seems reasonable to assert that data that is required to be accessed by multiple client applications must be synchronised to protect the state of the data so that it is consistent from the point of view of client applications that need to use it. Synchronisation logic is required for any server application that provides simultaneous services to multiple clients that require read/write access to shared data. This can be achieved in Java applications by controlling the thread that accesses an object’s data values by identifying the critical sections of code that require exclusive access to shared data and ‘flagging’ such code by using the keyword synchronized. Synchronisation of critical sections of code relies on an entity known as the intrinsic lock of an object. A thread ‘owns’ an object’s lock between the time it acquires it and releases it. The Java language provides two synchronising idioms: synchronised methods and synchronised statements. When a thread invokes a synchronised method, it acquires the lock for that method’s object and releases it when the method returns. Synchronising a method allows exclusive access to code that accesses shared data and ensures that it is not possible for two invocations of the method to interleave and interfere with one another. When a thread invokes a synchronised method of an object, all other threads that invoke synchronised methods of that object are blocked until the first thread has finished with the object and releases its lock. Thus, sharing an object amongst threads is made safe by declaring methods to be synchronised using the keyword synchronized as follows: Synchronising statements, on the other hand, provides a finer-grained approach than with synchronising methods. When synchronising a block of statements, the block must specify the object that provides the lock, as shown next. When the thread reaches the synchronised block, it examines the object passed as an argument and obtains the object’s lock before continuing with the execution of the statements in the block. The lock is released when the thread passes the end of the block. The example that follows on the next few pages illustrates how methods and statements are synchronised in the thread that runs a banking application. The author (of this guide) uses the example to teach some of the principles of distributed, client/server applications where the client and server run in separate JVMs on a computer network. The outcome of distributing the client and server components of the application means that the synchronised code in the server ensures that only one client at a time can access a customer’s account. Some of the code of the bank’s server class is omitted in order to allow the reader to identify and study the purpose of the code that is synchronised. The definitions of the Account and BankingException classes do not need to be shown here. An object of the class that follows is instantiated in the run method of a thread so that its methods can be called from multiple clients. The reader is not expected to understand fully how the bank application works. Rather, the aim of presenting the substantive code for the BankServer class is so that the reader can gain an understanding how synchronisation is used to synchronise methods and blocks of code in order to meet the requirements of the application in a way that ensures that a named account can be accessed by only one client application at a time. Chapter Four explains how the Thread class and the Runnable interface are used in a Java programme to create threads of execution. An example is used to illustrate how synchronised access to code is achieved in situations that require exclusive access to shared data resources. The chapter omits any discussion of thread scheduling. It is sufficient to say, for the purposes of this guide, that Java threads are pre-emptive. This means that the pre-emptive scheduler knows that a number of threads are runnable because their run method has been invoked implicitly by the JVM or explicitly by the developer’s code. However, only one thread is actually running at a time. A running thread continues to run until it ceases to be runnable or another thread of higher priority becomes runnable. In the latter case, the lower priority thread is pre-empted by the higher priority thread. A thread might cease to be runnable (i.e. it become blocked) for a variety of reasons, such as it might have to wait to access a resource. This gives other threads a chance to execute.$$$﻿The thread object is instantiated by calling the start method of the Thread class; this invocation places the thread in a runnable state, which means that it becomes available for scheduling by the JVM. The start method automatically calls the thread’s run method. The thread is typically instantiated in a separate, lightweight class that includes a main method. A template for this class follows. The body of the main method instantiates an instance of the thread by calling its no-arguments constructor; the start method of the thread is called on this instance. A template for the class definition of a class that implements the Runnable interface is as follows. The next template instantiates objects of the class that implements the Runnable interface. In this case, the body of the main method instantiates an instance of the class that implements the Runnable interface and it passes this object reference to the constructor of the Thread class that takes a Runnable object as its only parameter. The start method of the thread is called, as before. The outcome of using either of the two methods that can be used to create threads is that the developer provides the body of the thread’s run method; it is this method that does the work that the thread is required to do in a Java application. There are a number of tasks that could be designed to execute in a dedicated thread; these include: • I/O tasks that require substantial resources or are large in scale; • large-scale printing tasks where the print driver executes in its own thread; • synchronised multiple read/write tasks; • server applications that provide an application service to multiple clients. The fourth item in the list above identifies applications that require shared access to resources from multiple client applications and implies a high degree of synchronisation in order to maintain the integrity and security of data. The next sub-section explains, with an example, how synchronisation can be achieved. It seems reasonable to assert that data that is required to be accessed by multiple client applications must be synchronised to protect the state of the data so that it is consistent from the point of view of client applications that need to use it. Synchronisation logic is required for any server application that provides simultaneous services to multiple clients that require read/write access to shared data. This can be achieved in Java applications by controlling the thread that accesses an object’s data values by identifying the critical sections of code that require exclusive access to shared data and ‘flagging’ such code by using the keyword synchronized. Synchronisation of critical sections of code relies on an entity known as the intrinsic lock of an object. A thread ‘owns’ an object’s lock between the time it acquires it and releases it. The Java language provides two synchronising idioms: synchronised methods and synchronised statements. When a thread invokes a synchronised method, it acquires the lock for that method’s object and releases it when the method returns. Synchronising a method allows exclusive access to code that accesses shared data and ensures that it is not possible for two invocations of the method to interleave and interfere with one another. When a thread invokes a synchronised method of an object, all other threads that invoke synchronised methods of that object are blocked until the first thread has finished with the object and releases its lock. Thus, sharing an object amongst threads is made safe by declaring methods to be synchronised using the keyword synchronized as follows: Synchronising statements, on the other hand, provides a finer-grained approach than with synchronising methods. When synchronising a block of statements, the block must specify the object that provides the lock, as shown next. When the thread reaches the synchronised block, it examines the object passed as an argument and obtains the object’s lock before continuing with the execution of the statements in the block. The lock is released when the thread passes the end of the block. The example that follows on the next few pages illustrates how methods and statements are synchronised in the thread that runs a banking application. The author (of this guide) uses the example to teach some of the principles of distributed, client/server applications where the client and server run in separate JVMs on a computer network. The outcome of distributing the client and server components of the application means that the synchronised code in the server ensures that only one client at a time can access a customer’s account. Some of the code of the bank’s server class is omitted in order to allow the reader to identify and study the purpose of the code that is synchronised. The definitions of the Account and BankingException classes do not need to be shown here. An object of the class that follows is instantiated in the run method of a thread so that its methods can be called from multiple clients. The reader is not expected to understand fully how the bank application works. Rather, the aim of presenting the substantive code for the BankServer class is so that the reader can gain an understanding how synchronisation is used to synchronise methods and blocks of code in order to meet the requirements of the application in a way that ensures that a named account can be accessed by only one client application at a time. Chapter Four explains how the Thread class and the Runnable interface are used in a Java programme to create threads of execution. An example is used to illustrate how synchronised access to code is achieved in situations that require exclusive access to shared data resources. The chapter omits any discussion of thread scheduling. It is sufficient to say, for the purposes of this guide, that Java threads are pre-emptive. This means that the pre-emptive scheduler knows that a number of threads are runnable because their run method has been invoked implicitly by the JVM or explicitly by the developer’s code. However, only one thread is actually running at a time. A running thread continues to run until it ceases to be runnable or another thread of higher priority becomes runnable. In the latter case, the lower priority thread is pre-empted by the higher priority thread. A thread might cease to be runnable (i.e. it become blocked) for a variety of reasons, such as it might have to wait to access a resource. This gives other threads a chance to execute.
EN36	0	﻿Computer simulation is used to reduce the risk associated with creating new systems or with making changes to existing ones. More than ever, modern organizations want assurance that investments will produce the expected results. For instance, an assembly line may be required to produce a particular number of autos during an eight hour shift. Complex, interacting factors influence operation and so powerful tools are needed to develop an accurate analysis. Over the past few decades, computer simulation software, together with statistical analysis techniques have evolved to give decision makers tools equal to the task. As the world grows more technical and the need for precision becomes more important, the margin for error will continue to shrink. Business, industry, and governments cannot afford to make educated guesses during systems development. For that reason, computer simulation is more important than ever. Simulation uses a model to develop conclusions providing insight on the behavior of real-world elements being studied. Computer simulation uses the same concept but requires the model be created through computer programming. While this field has grown and flourished with availability of powerful software and hardware, its origins in the desire to forecast future behaviors, run quite deep. Men and women have attempted to foretell the future since ancient times. Kings employed wizards and soothsayers. Various religions used prophets. Seers such as French apothecary Michel de Nostredame (better known as Nostradamus) became famous with their visions of the future. Others attempted to make predictions based on birth dates and the stars. Crystal balls, bones, and tarot cards were all used as tools to probe the future. Although this book does not advocate those methods, in the same way modern chemists bear a relationship to the ancient alchemist, the modern simulation practitioner has a relationship with the ancient prophet. Of course, methodologies used by modern simulation analysts bear virtually no similarity with the prediction methods used in ancient times. However, there are common elements. For instance, each sought to remove the risk of a future event or behavior and reduce uncertainty. The prophet tried to accomplish this with the magic available at the time. Today, the simulation analyst uses the modern magic of mathematical principles, experimentation, computer science and statistics. Computer simulation can be classified as a branch applied mathematics. The use of computer simulation increased due to availability of computing power and improvements in programming languages. Added to this are inherent difficulties or even impossibilities to accurately describe complex real world systems using analytical or purely mathematical models. For these reasons, a tool that can represent these complexities accurately is required. Computer simulation can be broadly defined as: “Using a computer to imitate the operations of a real world process or facility according to appropriately developed assumptions taking the form of logical, statistical, or mathematical relationships which are developed and shaped into a model.” The result can be manipulated by varying a set of input parameters to help an analyst understand the underlying system’s dynamics. The model typically is evaluated numerically over a simulated period of time and data is gathered to estimate real world system characteristics. Generally, the collected data is interpreted with statistics like any experiment. Computer simulation can be an expensive, time consuming, and complicated problem solving technique. Therefore, certain circumstances warrant its use. Situations well suited to its application include the following (Table 1.1): Real system does not yet exist and building a prototype is cost prohibitive, time-consuming or hazardous. Aircraft, Production System, Nuclear Reactor System is impossible to build. National Economy, Biological System Real system exists but experimentation is too expensive, hazardous or disruptive to conduct. Proposed Changes to a Materials Handling System, Military Unit, Transportation System, Airport Baggage Handling System Forecasting is required to analyze long time periods in a compressed format. Population Growth, Forest Fire Spread, Urbanization Studies, Pandemic Flu Spread Mathematical modeling has no practical analytical or numeric solution. Stochastic Problems, Nonlinear Differential Equations Using computer simulation for analysis has many advantages over other decision making techniques. Among these advantages are: 1. Allows Experimentation without Disruptions to Existing Systems - In systems that already exist, testing new ideas may be difficult, costly, or impossible. Simulation allows a model to be developed and compared to the system to ensure it accurately reflects current operation. Any desired modifications can be made to the model first, the impact on the system examined, and then a decision to implement the changes in the real world system can be made. Example: Changing the Line An automotive assembly line may run 24 hours a day, seven days a week. Shutting the line down, putting in a temporary modification (which may or may not speed up a process), and resuming production would be very expensive. Example: Adding Equipment Unless the machinery was already purchased, the process of adding it to the line and trying it firsthand would be impossible. 2. Concept can be Tested Prior to Installation - A computer simulation will allow concepts to be tested prior to the installation of new systems. This testing may reveal unforeseen design flaws and give designers a tool for improvement. If the same flaws were discovered after installation, changes to the system might end up being very costly or even impossible to implement. Example: Purchase of an Automatic Storage and Retrieval system (ASRS) Engineers in a medium sized company decide to replace an existing warehouse with an ASRS. After stretching their tight budget to its outer limits, the equipment was procured and installed. Several months of usage revealed the new system was unable to keep up with the demands for pallets being entering and removed from the racks. Their assembly line process began to bog down because material wasn't arriving from storage in a timely fashion. The budget was gone and upper management told manufacturing to live with their decision. The entire situation could have been avoided by simulating the ASRS system prior to purchase. 3. Detection of Unforeseen Problems or Bugs - When a system is simulated prior to installation and found to work in concept, the model is often refined to include finer details. The detailed simulation may reveal unforeseen problems or bugs that may exist in the system's design. By discovering these problems prior to installation, debug time and rework costs can be avoided. In addition, improvements to system operation may be discovered. ﻿Simulation languages are versatile, general purpose classes of simulation software that can be used to create a multitude of modeling applications. In a sense, these languages are comparable to FORTRAN, C#, Visual Basic.net or Java but also include specific features to facilitate the modeling process. Some examples of modern simulation languages are GPSS/H, GPSS/PC, SLX, and SIMSCRIPT III. Other simulation languages such as SIMAN have been integrated into broader development frameworks. In the case of SIMAN, this framework is ARENA. Simulation languages exist for discrete, continuous and agent-based modeling paradigms. The remainder of this book will focus on the discrete event family of languages. Specialized features usually differentiate simulation languages from general programming languages. These features are intended to free the analyst from recreating software tools and procedures used by virtually all modeling applications. Not only would the development of these features be time consuming and difficult, but without them, the consistency of a model could vary and additional debugging, validation and verification would be required. Most simulation languages provide the features shown in Table 2.1. 1) Simulation clock or a mechanism for advancing simulated time. 2) Methods to schedule the occurrence of events. 3) Tools to collect and analyze statistics concerning the usage of various resources and entities. 4) Methods for representing constrained resources and the entities using these resources. 5) Tools for reporting results. 6) Debugging and error detection facilities. 7) Random number generators and related sets of tools. 8) General frameworks for model creation. Although many models are written using simulation languages, some analysts still prefer to rely on traditional programming languages for model development. In other cases, extensions to a language are developed to add capabilities to a traditional language. For instance, Repast Simphony is a free and open source, agent-based modeling toolkit that adds features to Java in order to simplify model creation and use. This blended approach provides the advantages of both the traditional language and the simulation modeling extensions. The motivations behind using a general purpose language include: Programmer familiarity: Developers already know the general purpose programming language. They may not have the time or inclination to learn a simulation language. Flexibility: Programming languages inherently are flexible, giving the analyst freedom to create the model using his or her preferred methodology. Cost: Programming language software is usually more accessible and far less expensive than specific simulation software. This may not always be true since several leading simulation languages can be downloaded for no cost. However, other leading simulation language packages can be very expensive. Hardware Concern: General purpose software may be available on any hardware platform while some simulation languages may require special machines and memory configurations. Lack of Analyst Knowledge: The analyst may not understand simulation languages and may lack knowledge on the advantages of using a simulation language package. Training: Available classes in the use of traditional languages are more likely to be available than specialty simulation training. Although traditional languages do offer some advantages, most of these are outweighed by features standard to many simulation languages. In a typical modeling application, the programmer or analyst will find the initial investment in a simulation language more than pays off. A simulation language will provide a savings in coding, debugging, analysis of results, and in making changes. A variety of simulation languages exist and are used by businesses, researchers, manufacturing and service companies, and consultants. The next sections briefly discuss two common simulation languages: GPSS and SIMSCRIPT. GPSS: General Purpose Simulation System (GPSS) was originally developed by Geoffrey Gordon of IBM and released in October of 1961. Following IBM’s release of GPSS to the public domain, it became a multivendor simulation language and has been in continuous use since. In general, GPSS enjoys widespread popularity due to its sensible world view and overall power. Its basic functions can be easily learned while powerful features make it ideal for modeling complex systems. In general, GPSS is used to simulate queuing systems that consist of customer entities interacting and completing in a system of constrained resources. The resources are structured as networks of blocks that entities (also called transactions) enter and use to perform various tasks in certain amounts of simulated time. As entities move through these networks, which have been organized to represent a real world system, statistics are collected and used to determine if the system contains bottlenecks, is over or under utilization, or exhibits other characteristics. Output data is made available for analysis at the end of a production run. Presently, several vendors offer versions of GPSS. Included are: Wolverine Software which produces GPSS/H, a powerful, state-of-the-art version of GPSS engineered to allow creation of large, complex models (http://www.wolverinesoftware.com). Minuteman Software which produces a user friendly GPSS simulation environment called GPSS World that features special model development tools (http://minutemansoftware.com). ngolf Ståhl and Beliber AB which produce WebGPSS, a stream-lined version of GPSS, with a focus simulation and modeling concept education (http://www.webgpss.com). SIMSCRIPT III: This language is a direct descendant of the original SIMSCRIPT language produced at Rand Corporation in the 1960's. SIMSCRIPT III has constructs that allow a modeler to approach a problem from either a process or an event oriented world view. SIMSCRIPT III offers unique features which add to its appeal. Among these are: • Object-Oriented Programming • Modularity • SIMSCRIPT III Development Studio (SimStudio) • Object-Oriented Simscript III graphics • Data Base Connectivity SDBC In general, SIMSCRIPT III is a free form language with English-like syntax. This syntax allows the code in the system to become self-documenting. Model components can be programmed clearly enough to provide an excellent representation of the organization and logic of the system being simulated. SIMSCRIPT III is maintained and distributed at http:// www.simscript.com by CACI Products Company. Most discrete event simulation languages model a system by updating the simulation clock to the time that the next event is scheduled to occur. Events and their scheduled times of occurrence are maintained automatically on one of two ordered lists: the current events chain or the future events chain. The current events chain keeps a list of all events that will (or may) occur at the present clock time. The future events chain is a record of all events that can occur at some point in the future. A simulation clock moves to the next event on the future events chain and changes the system state of the model based on that event’s characteristics. ﻿There has been a long running debate that concentrates on trying to decide whether simulation should be defined as an art or a science. Those who believe it to be a science feel that statistics, mathematics, and computer science comprise its foundation and are the basis for this classification. Others feel that the skill of the modeling team, the creativity involved in developing the model, and the interpretation of the results all add up to an individualized art. In this author's opinion, there is no real way to scientifically structure a simulation to guarantee that its results are valid. Instead, after the model has been coded and run, the outputs can be studied and compared with corresponding known values to determine suitability. If no known values exist then the modeler must rely on his instincts and the judgment of experts to make this determination. The creativity and instincts used are akin to an art. Much of the methodology involved in model creation and analysis are based on computer science and mathematical principles. Therefore, elements of art and science exist in modeling. Simulation best may be defined as a “soft-science” containing both. Figure 3.1 depicts the spectrum extending from art to science and the simulation's place. In recent years simulation has been moving along the spectrum more toward the science end. Improvements in simulation languages and the advent of simulators have removed some of the need to be as creative and innovative. Much of the uncertainty in model creation has also been eliminated through the development of application specific languages. Numerous explanations exist for reasons behind the phenomenal growth computer simulation has experienced both in terms of application areas and in the number of available software products. Among these reasons are: 1. Improvements in computers: The first simulations were done on large, room-sized mainframe computers. These computers relied on card decks and operated most often in the batch mode. Not many people had access to these mammoth devices. In the last thirty years, computers have been reduced in size and cost considerably. Equivalents of the mainframes that used to occupy large rooms are now carried around in briefcase sized packages. Computers have moved from research laboratories and can now be found on practically every desk in all industries. The computing power of a single chip is fast becoming all that is necessary to run the most sophisticated commercial simulation software. This widespread availability and reduction in cost of computers has enabled simulation to prosper. 2. Improvements in simulation products: Thirty years ago, most simulation work was done using assembly language, FORTRAN, or other high level languages. Development time was much greater, as was debugging, statistic tabulation, and the reliability of results. This situation began to change with the advent of GPSS in 1961. Since that time, a multitude of simulation languages, analysis programs, animators, and pre-programmed simulators have become available. Much of the development time required to create a model has been eliminated through standard features found in these products. In addition to performing the necessary simulation functions, varying degrees of user friendliness are available. Simulation languages such as Simscript and GPSS/H appeal to practitioners with programming skills while non-programmers can enjoy the mouse driven menus found in many application specific simulators. 3. New opportunities for simulation education: Three decades ago, very few universities offered discrete event simulation classes. Today many universities offer simulation classes in engineering and business curriculums. In addition, private seminars and training sessions are available. These sessions are sponsored by simulation software vendors, consultants, and corporations with an interest in simulation and modeling. Other sources of simulation education are trade magazines, academic publications, conferences, societies, and books. 4. Increasingly complex and technical work environments: During the previous few decades the average work environment has changed from simple manual assembly lines to complex automated systems. Many repetitive human tasks have been replaced with robots and factory automation equipment. Conveyors, forklift trucks, storage and retrieval racks, as well as many other factory floor items are now routinely controlled by programmable logic controllers or computers. This new complexity has made factory output rates very difficult to predict and manage. Thus, the need for better analysis tools arose. New simulation techniques evolved to satisfy this demand and were able to remove much of the guess work and uncertainty in the work place. 5. Computer literacy among analysts and engineers: Computer literacy and use is nearly ubiquitous among professionals. Everyone has the ability to receive computer training. 6. Competition and tightening budgets - Another factor adding to growth in simulation use is an emphasis on lowering overhead costs, reducing labor requirements, and streamlining operations. Much of this has come about as globalization has increased competition and businesses compete on an international basis. 7. Realization of the benefits offered by simulation: As more industries recognize the benefits of simulation, investing in capabilities to use these tools becomes more important. Apparent economic advantages have prompted companies to invest time and resources into this area. 8. Industrial peer pressure: Organizations without simulation capabilities have found themselves at a competitive disadvantage. This was most apparent in industries, such as materials handling, where a simulation study accompanying a quotation would lend credibility to a proposed system and often become a determining factor when the contract was awarded. A situation of industrial peer pressure was created. Purchasers would inquire why simulation was not being used by their vendors and demand that some type of through-put guarantee be given. Presently, most materials handling system request-for-quotations require modeling be performed prior to purchase. Similar requirements exist in other industries. These forces have been a key factor in popularizing simulation. 9. Warm fuzzy feeling: Many companies have developed internal simulation groups to model inhouse problems, proposed systems, and existing manufacturing processes. Presentation materials such as simulation animation packages have helped to sell internal proposals to management and create a corporate warm fuzzy feeling. “Seeing is believing” and many simulation packages available today place an emphasis on graphics and output. 10. Part of an effort to increase quality: Another force related to simulation adoption is a commitment to quality improvement processes. The success of these philosophies has been demonstrated in many industries with large productivity gains and improved profitability. A major precept of quality is prevention of problems. By creating a model of systems to be designed, purchased, or installed, costly mistakes can be avoided and installations can be done right the first time. ﻿A computer simulation project is more than purchasing modeling software, plugging in a few values and expecting an answer to a question to pop out. This chapter considers the broader picture that simulation encompasses by developing a methodology for starting the process correctly. However, before that is discussed, a failed project is examined. Simulations can go terribly wrong, particularly if the appropriate infrastructure is not carefully developed to support the modeling effort. Few simulation analysts can report that all of their models are flawless. The previous example analyzed a real simulation project that became disastrous and the steps taken to revive it. Although the model had been accurate, the simulation received criticism and blame for the problems. Several pitfalls can be identified by looking at the process in retrospect. First: The simulation analyst was inexperienced. She lacked practical knowledge of the system's operation and this slowed initial model development. Second: The simulation 'team' members didn't participate adequately. Although the model demonstrated problems did exist, time was not taken to solve these problems as a preventative measure. Rather, the system was installed and then problems were tackled at a much higher cost. Third: Goals changed mid-simulation effort. The original simulation was meant to discover if the would work. When production fell short, a new goal of 'making it work' became the directive. The problems pointed out by this case study could have been avoided if the simulation effort had been taken more seriously. If preventative action had addressed system inadequacies early, costly corrections and time delays could have been avoided. A recent study of discrete event computer simulation projects revealed unsuccessful efforts are characterized by high costs, model size constraints, and slow software. In contrast, successful projects are characterized by teamwork, cooperation, mentoring, effective communication of outputs, high-quality vendor documentation, easily understood software syntax, higher levels of analyst experience, and structured approaches to model development. While these findings should not come as a surprise, the importance of having a structured methodology for approaching a simulation is apparent. The next sections of this textbook provide a detailed view of an approach to help enable simulation project success. The process used in this textbook is representative of many possible computer simulation project life cycles that have been used in practice and documented in simulation textbooks and journal articles. However, remember each simulation project is unique and specific requirements may be better served through customizing the steps discussed. In general, the simulation project life cycle described in this book consists of six major steps, each having multiple components. The major steps being investigated are: 1. Intelligence Phase. The precursor to any solution is full development and an understanding of the underlying dilemma or problem. 2. Managerial Phase. In general, this step requires interaction with management and other nontechnical staff of the organization to acquire necessary resources and support along with the formation of a simulation project team. 3. Developmental Phase. During this phase, the simulation model or models are created. System design, detail design, and coding all take place and rely on the interaction of the analysts and other members of the simulation team. 4. Quality Assurance Phase. While the model is being coded in complete or at least in prototype form, the analyst must ensure proper validation and verification. The ideas of testing and completion or integration are also important here. It may be necessary take testing one step further and begin development of face validity through interaction with end-users and management. Generally, quality assurance accompanies the entire modeling lifecycle. 5. Implementation Phase. Model use begins and decision support activities take place. 6. Operations, Maintenance, and Archival Phase. Development is essentially complete and only maintenance tasks remain. This phase may be substantial if the project results in a simulation maintained for repeated use A simulation project can be a complex organizational undertaking. In general, to believe a simulation project starts and ends with coding a model would be a mistake. Much more is involved. In many instances, modeling is one small component in a much larger simulation project life cycle. As stated previously, a simulation project is inclusive of a wide spectrum of activities ranging from system and problem definition to ensuring logic developed in the model can bridge the gap between model and real world and be used in actual system implementation. The following sections provide a closer look at the mechanics of conducting a professional simulation study. 4.1 Intelligence The intelligence phase of the simulation life cycle involves understanding the environment and determining problems to be solved. In general, this phase consists of the simulation analyst or potential simulation customer discovering situations that require modeling. The simulation analyst should emerge from this phase with knowledge that a problem exists and should have at least a preliminary understanding of the problem’s nature. Often, problem definition and feasibility are assessed at this point in time. 4.1.1 Problem Definition In order to ensure simulation project success, several events need to occur before any other work begins. First, the customer needs to define objectives for the study. The objectives for the simulation describe what questions the simulation needs to answer. In other words, the precursor to any solution is a full understanding of the underlying dilemma or problem. Parameters for the problem as well as broad scale constraints need to be defined. A simulation project may be driven by a broad statement such as ‘The Denver Chamber of Commerce has projected that incoming passenger flights will increase by 50% over the next 18 months.” Definition of the problem operationalizes the statement and looks at the system environment to clarify goals for analysis. For example the problem definition may become: Can the existing baggage handling system handle the project increase in passenger arrivals and departures? Of course, this question could rapidly be replaced with a related: What needs to be changed in order to accommodate the expected increase in passenger arrivals and departures? These sorts of clarifications need to be made early in the simulation life cycle. ﻿Every transaction is somehow dated and the relevant date of no transaction can precede the date on which the relevant account was opened. In addition to the amount, the information associated with each transaction number varies according to the kind of transaction, as follows. Payments in: For a payment into an account, the account number, date, time, and source. BR9 Every payment in has exactly one account number, exactly one date, exactly one time of day, and exactly one source. Payment by cheque: A cheque on a particular account is uniquely identified within that account by its cheque number. For a payment by cheque, the account number, cheque number, date written (as shown on the cheque), date processed (by the bank), payee, and amount are recorded. BR10 Every payment by cheque has exactly one account number, exactly one cheque number, exactly one date written (as shown on the cheque), exactly one date processed (by the bank), and exactly one payee. It is possible for more than one payment to be made by cheque to the same payee with the same date written and date processed. It is assumed that cheque books are not issued to a customer until the relevant account has been opened. A cheque cannot be processed before the date written as shown on the cheque. BR11 The date processed of a cheque cannot precede its date written. Payment by direct debit: For a payment by direct debit, the account number, date, time, payee, and amount are recorded. Note that is theoretically possible for more than one payment to be made by direct debit to the same payee at exactly the same time on the same day. BR12 Every payment by direct debit has exactly one account number, exactly one date, exactly one time, and exactly one payee. Payment by debit card: This includes cash withdrawals from ATMs. A customer can be issued with any number of debit cards. Each debit card is for a particular account and several debit cards can be issued for the same account, perhaps for use by various family members. Each debit card is identified by a card number. For every debit card, the relevant account number, cardholder’s name, and expiry date are recorded. For a payment by debit card, the card number, date, time, payee (which might refer to an ATM), and amount are recorded. It is not possible for the same debit card to be used more than once at exactly the same time on the same day. It is not possible to use a debit card for any payment on a date after the expiry date. BR13 Every debit card is uniquely identified by a card number and has exactly one account number (identifying an existing account), exactly one cardholder’s name, exactly one expiry date, and exactly one payee. BR14 Every payment by debit card is uniquely identified by the combination of its transaction number and card number of an existing card, also by the combination of its card number, date, and time, and has exactly one payee. BR15 The date of a payment by debit card cannot be later than the expiry date of the relevant card. BR16 Every transaction is either a payment in, or a payment by cheque, or a payment by direct debit, or a payment by debit card. BR17 No transaction is of more than one of the types mentioned in BR16. BR18 A transaction other than a payment by cheque cannot be dated earlier than the date on which the relevant account was opened. 7. Based on your experiences with Exercise 7, suggest enhancements to Tutorial D to make it easier to express any constraints you declared that struck you as being of a common enough kind to warrant an additional shorthand. 8. (For students familiar with SQL). Consider the following SQL definitions: a. What problem was the designer solving here? b. What possible problem remains in this solution? c. Describe and comment on the particular features of SQL that make this solution possible. 1. Explore Rel’s catalogue. It consists of a relvar named sys.Catalog. Use the following trick to see sys.Catalog’s heading only: From their names, you might be able to guess which attributes are of most interest (possibly Name, Owner, and isVirtual?). Create a virtual relvar named myvars giving the Name, Owner, and isVirtual of every relvar not owned by 'Rel'. Virtual relvars are created like this: Test your virtual relvar definition by entering the queries 2. If you haven’t already done so, load the file OperatorsChar.d, provided in the Scripts subdirectory of the Rel program directory, and execute it. One of the relvars mentioned in sys.Catalog is named sys.Operators. Display the contents of that relvar. How many attributes does it have? What is the declared type of the attribute named Implementations? 3. Evaluate the expression What are the “ReturnsTypes” of LENGTH, IS_DIGITS, and SUBSTRING? 4. Note that if s is a value of type CHAR, then LENGTH(s) gives the number of characters in s, IS_DIGITS(s) gives TRUE if and only if every character of s is a decimal digit. SUBSTRING(s,0,l) gives the string consisting of the first l characters of s (note that strings are considered to start at position 0, not 1). SUBSTRING(s,f) gives the string consisting of all the characters of s from position f to the end. What is the result of IS_DIGITS('')? Is it what you expected? Is it consistent with the definition given above? 5. Using operators defined by OperatorsChar.d, define types for supplier numbers and part numbers, following Example 2.4 from Chapter 2. Define relvars Srev, Prev, and SPrev as replacements for S, P and SP, using the types you have just defined as the declared types of attributes S# and P#. Write relvar assignments to copy the contents of S, P and SP to Srev, Prev, and SPrev, respectively. Note that if SNO is the type name for supplier numbers in S and Srev, then SNO(S#) “converts” an S# value in S to one for use in Srev. 6. Using the relvars defined in Exercise 5, repeat Exercise 6 from the set headed “Working with A Database in Rel” given with the exercises for Chapter 4. Which of your solutions need revisions beyond the obvious changes in relvar names?$$$﻿Every transaction is somehow dated and the relevant date of no transaction can precede the date on which the relevant account was opened. In addition to the amount, the information associated with each transaction number varies according to the kind of transaction, as follows. Payments in: For a payment into an account, the account number, date, time, and source. BR9 Every payment in has exactly one account number, exactly one date, exactly one time of day, and exactly one source. Payment by cheque: A cheque on a particular account is uniquely identified within that account by its cheque number. For a payment by cheque, the account number, cheque number, date written (as shown on the cheque), date processed (by the bank), payee, and amount are recorded. BR10 Every payment by cheque has exactly one account number, exactly one cheque number, exactly one date written (as shown on the cheque), exactly one date processed (by the bank), and exactly one payee. It is possible for more than one payment to be made by cheque to the same payee with the same date written and date processed. It is assumed that cheque books are not issued to a customer until the relevant account has been opened. A cheque cannot be processed before the date written as shown on the cheque. BR11 The date processed of a cheque cannot precede its date written. Payment by direct debit: For a payment by direct debit, the account number, date, time, payee, and amount are recorded. Note that is theoretically possible for more than one payment to be made by direct debit to the same payee at exactly the same time on the same day. BR12 Every payment by direct debit has exactly one account number, exactly one date, exactly one time, and exactly one payee. Payment by debit card: This includes cash withdrawals from ATMs. A customer can be issued with any number of debit cards. Each debit card is for a particular account and several debit cards can be issued for the same account, perhaps for use by various family members. Each debit card is identified by a card number. For every debit card, the relevant account number, cardholder’s name, and expiry date are recorded. For a payment by debit card, the card number, date, time, payee (which might refer to an ATM), and amount are recorded. It is not possible for the same debit card to be used more than once at exactly the same time on the same day. It is not possible to use a debit card for any payment on a date after the expiry date. BR13 Every debit card is uniquely identified by a card number and has exactly one account number (identifying an existing account), exactly one cardholder’s name, exactly one expiry date, and exactly one payee. BR14 Every payment by debit card is uniquely identified by the combination of its transaction number and card number of an existing card, also by the combination of its card number, date, and time, and has exactly one payee. BR15 The date of a payment by debit card cannot be later than the expiry date of the relevant card. BR16 Every transaction is either a payment in, or a payment by cheque, or a payment by direct debit, or a payment by debit card. BR17 No transaction is of more than one of the types mentioned in BR16. BR18 A transaction other than a payment by cheque cannot be dated earlier than the date on which the relevant account was opened. 7. Based on your experiences with Exercise 7, suggest enhancements to Tutorial D to make it easier to express any constraints you declared that struck you as being of a common enough kind to warrant an additional shorthand. 8. (For students familiar with SQL). Consider the following SQL definitions: a. What problem was the designer solving here? b. What possible problem remains in this solution? c. Describe and comment on the particular features of SQL that make this solution possible. 1. Explore Rel’s catalogue. It consists of a relvar named sys.Catalog. Use the following trick to see sys.Catalog’s heading only: From their names, you might be able to guess which attributes are of most interest (possibly Name, Owner, and isVirtual?). Create a virtual relvar named myvars giving the Name, Owner, and isVirtual of every relvar not owned by 'Rel'. Virtual relvars are created like this: Test your virtual relvar definition by entering the queries 2. If you haven’t already done so, load the file OperatorsChar.d, provided in the Scripts subdirectory of the Rel program directory, and execute it. One of the relvars mentioned in sys.Catalog is named sys.Operators. Display the contents of that relvar. How many attributes does it have? What is the declared type of the attribute named Implementations? 3. Evaluate the expression What are the “ReturnsTypes” of LENGTH, IS_DIGITS, and SUBSTRING? 4. Note that if s is a value of type CHAR, then LENGTH(s) gives the number of characters in s, IS_DIGITS(s) gives TRUE if and only if every character of s is a decimal digit. SUBSTRING(s,0,l) gives the string consisting of the first l characters of s (note that strings are considered to start at position 0, not 1). SUBSTRING(s,f) gives the string consisting of all the characters of s from position f to the end. What is the result of IS_DIGITS('')? Is it what you expected? Is it consistent with the definition given above? 5. Using operators defined by OperatorsChar.d, define types for supplier numbers and part numbers, following Example 2.4 from Chapter 2. Define relvars Srev, Prev, and SPrev as replacements for S, P and SP, using the types you have just defined as the declared types of attributes S# and P#. Write relvar assignments to copy the contents of S, P and SP to Srev, Prev, and SPrev, respectively. Note that if SNO is the type name for supplier numbers in S and Srev, then SNO(S#) “converts” an S# value in S to one for use in Srev. 6. Using the relvars defined in Exercise 5, repeat Exercise 6 from the set headed “Working with A Database in Rel” given with the exercises for Chapter 4. Which of your solutions need revisions beyond the obvious changes in relvar names?
EN40	0	﻿High-content screening can easily generate more than one Terabyte in primary images and metadata per run, that have to be stored and organized, which means an appropriate laboratory information management system (LIMS) has to be established. The LIMS must be able to collect, collate and integrate the data stream to allow at least searching and rapid evaluation of the data. After image acquisition and data transfer, image analysis will be run to extract the metadata. Further evaluation includes testing for process errors. Heat maps along with pattern recognition algorithms help to identify artefacts such as edge-effects, uneven pipetting, or simply to exclude images that are not in focus. All plates should be checked so that the selected positive and negative controls exhibit values in a pre-defined range. Further, data may be normalized against controls before further statistical analysis is run to identify putative hits. Known proteins of the pathway being screened should score, and are a good internal control for the accuracy of the assay and workflow. Hits have to be verified by going back to the original images. Further, results have to be compared between independent runs. After this, an appropriate hit verification strategy has to be applied as discussed above. Target gene expression should be confirmed, for example, by running a microarray analysis of gene expression for the given cell line. Finally, data will be compared to other internal and external data sources. Cluster analysis will assist in identifying networks and correlations. A critical aspect of high content screening is the informatics and data management solution that the user needs to implement to process and store the images. Typically multiple images are collected per microplate well at different magnifications and processed with pre-optimised algorithms (these are the software routines that analyse images, recognize patterns and extract measurements relevant to the biological application, enabling the automated quantitative comparison and ranking of compound effects) to derive numerical data on multiple parameters. This allows for the quantification of detailed cellular measurements that underlie the phenotype observed. From an image analysis perspective the following should not be overlooked when reviewing vendor offerings: the breadth of biology covered; how the software is delivered, does it run quickly, or open a script; is analysis done on-the-fly or offline; have the algorithms been fully validated with biology; the ease of exporting image files to other software packages; and access to new algorithms, is the user dependent on the supplier or is it relatively easy to develop your own or adapt existing algorithms? The key theme and piece of information repeated throughout this chapter is “partnering”. Scientific research and informatics must work together for the mutual benefit of screening like the drug discovery process. To really be part of the winning team in any organization, all areas must bring their collective expertise together and make the extra effort to understand one another and defer where there is lack of knowledge to those on the team with the experience and expertise or to seek external advises. It is necessary to start off by setting the stage concerning where laboratory computing, which includes the data management (we will discuss a bit later in the chapter), has progressed in order to gain the necessary understanding of where it currently is and where we anticipate it will be going in the HCS area in the future. A goal of this chapter is to provide an overview of the key aspects of informatics tools and technologies needed for HCS, including characteristics of HCS data; data models/structures for storing HCS data; HCS informatics system architectures, data management approaches, hardware and network considerations, visualization, data mining technologies, and integrating HCS data with other data and systems. HCS systems scan a multiwell plate with cells or cellular components in each well, acquire multiple images of cells, and extract multiple features (or measurements) relevant to the biological application, resulting in a large quantity of data and images. The amount of data and images generated from a single microtiter plate can range from hundreds of megabytes (MB) to multiple gigabytes (GB). One large-scale HCS experiment, often resulting in billions of features and millions of images that needs multiple terabytes (TB) of storage space. High content informatics tools and infrastructure is needed to manage the large volume of HCS data and images. There are many rules that are common for the image based HCS informatics infrastructure in academic or non academic organization. Answering the following questions analyzed by entire organization tells one exactly which strategy and organization setup has to be taken and what type of work has to assign to experts and researchers. In choosing the strategy and organization setup one needs to answer the following questions: • Is the required analysis software available off-the-shelf or must it be written in-house? This decision has to be taken in collaboration between IT and scientists, based on the defined requirements. • What kind of data will be acquired (how many screens in year)? • How is the data stored, managed, and protected for short-, medium-, and long-term use? • What type of desktop clusters and servers are required for HCS computing? (brand, type, speed, and memory) • How do the computer systems interface with the necessary data collection instrumentation and connect to the network and servers at the same time? • Can allowances and accommodations be made for external collaborations and programs shared among scientists? • Are we interested in setup a safety buffered zone outside of our firewalls to allow this external data exchange? After analysis of those questions one would think to have dedicated IT person from IT department working together with the scientists to allow IT professionals to take over responsibility for informatics tasks. The side-by-side person would allow the informatics organization to understand needs of HCS unit. For example the servers processes could be placed inside of HCS pipeline or infrastructure and not be placed as usual and forced to add extra steps to the workflow. It is also important to decide what will be operated by informatics department and what by HCS unit within organization. It makes better sense for informatics department to ﻿Within the last few years a large number of tools and softwares dealing with different computational problems related to HCS have been developed. Incorporating third party or new tools into existing frameworks needs a flexible, modular and customizable workflow framework. Workflow (Pipeline) systems could become crucial for enabling HCS researchers doing large scale experiments to deal with this data explosion. The workflow is termed abstract in that it is not yet fully functional but the actual components are in place and in the requisite order. In general, workflow systems concentrate on the creation of abstract process workflows to which data can be applied when the design process is complete. In contrast, workflow systems in the life sciences domain are often based on a data-flow model, due to the data-centric and data-driven nature of many scientific analyses. A comprehensive understanding of biological phenomena can be achieved only through the integration of all available biological information and different data analysis tools and applications. In general, an ideal workflow system in HCS can integrate nearly all standard tools and software. For example, for an HCS using small molecules, the workflow system must be able to integrate different image processing software and data mining toolkits with flexibility. The possibility that any single software covers all possible domains and data models is nearly zero. No one vendor or source can provide all the tools needed by HCS informatics. So it is suggested that one uses specialized tools from specialized sources. Also not all softwares components can be integrated with all workflow systems. Workflow environment helps also HCS researchers to perform the integration themselves without involving of any programming. A workflow system allows the construction of complex in silico experiments in the form of workflows and data pipelines. Data pipelining is a relatively simple concept. Visual representation of the workflow process logic is generally carried out using a Graphical User Interface where different types of nodes (data transformation point) or software components are available for connection through edges or pipes that define the workflow process. Graphical User Interfaces provide drag and drop utilities for creating an abstract workflow, also known as “visual programming”. The anatomy of a workflow node or component (Fig. 3) is basically defined by three parameters: input metadata, transformation rules, algorithms or user parameters and output metadata. Nodes can be plugged together only if the output of one, previous (set of) node(s) represents the mandatory input requirements of the following node. Thus, the essential description of a node actually comprises only in -and output that are described fully in terms of data types and their semantics. The user can create workflows using any combination of the available tools, readers, writers or database connections in workflow system by dragging/dropping and linking graphical icons. The component properties are best described by the input metadata, output metadata and user defined parameters or transformation rules. The input ports can be constrained to only accept data of a specific type such as those provided by another component. An HCS workflow design is best carried out in phases. In the first phase, a conceptual workflow is generated. A conceptual workflow, as the name suggests, is a sequential arrangement of different components that the user may require to accomplish the given task. It is possible that some of those steps may in turn be composed of several sub components. The next phase converts the conceptual workflow into an abstract workflow by performing a visual drag and drop of the individual components that were figured to be a part of the workflow in the first phase. The workflow is termed abstract in that it is not yet fully functional but the actual components are in place and in the requisite order. In general, workflow systems concentrate on the creation of abstract process workflows to which data can be applied when the design process is complete. HCS screening workflows are based on a dataflow which integrate most of the available, standard software tools (either commercial or public domain) along with different classes of programmable toolkits. As an example, Figure 3 shows a workflow designed to be run by the HCDCKNIME Workflow Management System ( http://hcdc.ethz.ch). This workflow is used by HCS facilities. It obtains RNAi from databases, annotates them, make dilutions steps, barcode handling, split volume. In this case, the tasks, also known as steps, nodes, activities, processors or components, represent either the invocation of a remote Web service (the databases), or the execution of a local recalculation. Data-flows along data links from the outputs of a task to the inputs of another, is prepared according to a pre-defined graph topology. The workflow defines how the output produced by one task is to be consumed by a subsequent task, a feature referred to as orchestration of a flow of data. Any computational component or node has data inputs and data outputs. Data pipelining views these nodes as being connected together by ‘pipes’ through which the data flows (Figure 4). Workflow technology is a generic mechanism to integrate diverse types of available resources (databases, microscopes, servers, software applications and different services) which facilitates data exchange within screening environment. Users without programming skill can easily incorporate and access diverse instruments, image processing tools and produced data to develop their own screening workflow for analysis. In this section, we will discuss the usage of existing workflow systems in HCS and the trends in applications of workflow based systems. Many free and commercial software packages are now available to analyse HCS data sets using statistical method or classification, although it is still difficult to find a single off-the-shelf software package that answers all the questions of HCS analysis. Statistical open source software packages such as BioConductor (www.bioconductor.org) provide large collections of methods suitable for HCS data analysis. However, their command-line usage can be too demanding for users without adequate computer knowledge. As an alternative, software packages where users can upload their data and receive their processed results are becoming increasingly common: Weka25, CellAnalyzer4, CellHTS3, TreeView21 have all been published within the last year. Unfortunately, these services often allow only limited freedom in the choice and arrangement of processing steps. Other, more flexible tools, such as Eclipse6, KNIME13, JOpera2, operate either stand-alone or require considerable computer knowledge and extra software to run through the web. In order to make use of the vast variety of data analysis methods around, it is essential that such an environment is easy and intuitive to use, allows for quick and interactive changes to the analysis process and enables the user to visually explore the results. ﻿The code for this model is shown in NetLogo Code 5.4. The model uses two types of agents – a wanderer breed for the big red agent that explores the environment with its vision cone sense as shown in the screenshots; and a stander breed that is used to draw 6000 randomly spread gray agents in the environment in the setup procedure.  In the main go method, as for the Line of Sight example, the turtle agents perform a small random right, then a left turn, followed by moving forward 1 step. Then setting the colour of all standers in the vision cone defined by the vision-radius and vision-angle variables shows their current field of vision. Sensing in agents need not be restricted to the traditional senses such as vision, or touch. For example, equilibrioception (see Table 5.2), or the sense of balance, combined with other senses such as the visual system and proprioception, allows humans and animals to gain information about their body position in relation to their immediate surroundings. Sensing can involve the detection of any aspect of the environment, such as the presence, absence or change in a particular attribute. This then provides the agent with an ability to follow a gradient in the environment relating to the attribute. For example, a useful sense for real-life agents is the ability to detect changes in elevation, in order to be able to head in an upwards or downwards direction when required. The Hill Climbing Example model in the NetLogo Models Library shows how such a sense can be implemented. A screenshot of the model is shown in Figure 5.6 and the code is shown in NetLogo Code 5.5. The screenshot shows turtle agents that start out at random locations then move steadily upwards to the highest local point in their immediate surrounding landscape (i.e. towards the patches with the lightest shading). Since a single high point serves as the highest point for many of its surrounding patches, this results in the agents’ paths coalescing into linear patterns of lines as shown in the figure. The vertical line to the left of the figure, for example, represents the top of a ridgeline in the 2D environment. As an analogy with real-life human agents, we can imagine a number of hill walkers climbing from different start positions up a ridge, and once they reach the ridge line, they all end up following a single path to the top.  The model uses the uphill command in NetLogo that directs an agent to move to a neighbouring patch that has the highest value for a particular patch variable. The variable in this case is the patch’s color. The go method asks each turtle to head in an upward direction, until that is no longer possible, in which case the turtle variable peak? is set to true to indicate the turtle has reached the top. When all turtles have reached the top, then no more processing is done.  Is it possible for an agent to perform a non-trivial task without knowing they are performing it? In other words, can an agent ‘solve’ a problem without cognitively being aware they are solving a problem? To answer this question, first we must ask what we mean by ‘cognitively being aware’. Cognition refers to the mental processes of an intelligent agent, either natural or artificial, such as comprehension, reasoning, decision-making, planning and learning. It can also be defined in a broader sense by linking it to the mental act of knowing or recognition of an agent in relation to a thought it is thinking or action it is performing. Thus, cognitive behaviour occurs when an agent knowingly processes its thoughts or actions. Under this definition, an autonomous agent exhibits cognitive behaviour if it knowingly processes sensory information, makes decisions, changes its preferences and applies any existing knowledge it may have while performing a task or mental thought process. Cognition is also related to perception. Perception for an agent is the mental process of attaining understanding or awareness of sensory information. Let us now return to the question posed at the beginning of this section. Consider an insect’s abilities to forage for food. Ants, for example, have the ability to quickly find food sources, but do this by sensing chemical scent laid down by other ants, and then react accordingly using a small set of rules. Despite not being cognitively aware of what they are doing, they achieve the goal of fully exploring the environment, efficiently locating nearby food sources and returning back to the nest. An Ants model provided in the NetLogo Models Library simulates one way how this might happen. A screenshot of the model is shown in Figure 5.7. It shows a colony of ants spread out throughout the environment, with its nest shown by the purple region at the centre of the image. Originally, there were three food sources, but at this stage in the simulation, the previous two have already been devoured and the remaining food source has been found and is in the process of being collected for return to the nest. The white and green shaded region represents how much chemical scent has been load down in the environment, with white representing the greatest amount.  The code for the part of the model that defines the behaviour of the ants is shown in NetLogo Code 5.6. A full listing of the code can be found by selecting the Ants model from the Models Library, and then by clicking on the Procedures button at the top of the NetLogo interface.  Each patch agent has a number of variables associated with it – for example, chemical stores the amount of chemical that ants have laid down on top of it, and nest-scent reflects how close the patch is to the nest. The recolor-patch procedure shows how the patch’s colour is reshaded according to how much chemical has been laid down on top it. The go procedure defines the behaviour of the ants. If the ant is not carrying food, it will look for it (by performing the look-for-food procedure) otherwise it will take the food back to the nest (by performing the return-to-nest procedure). As the agent is returning to the nest, it drops a chemical as it moves.$$$﻿The code for this model is shown in NetLogo Code 5.4. The model uses two types of agents – a wanderer breed for the big red agent that explores the environment with its vision cone sense as shown in the screenshots; and a stander breed that is used to draw 6000 randomly spread gray agents in the environment in the setup procedure.  In the main go method, as for the Line of Sight example, the turtle agents perform a small random right, then a left turn, followed by moving forward 1 step. Then setting the colour of all standers in the vision cone defined by the vision-radius and vision-angle variables shows their current field of vision. Sensing in agents need not be restricted to the traditional senses such as vision, or touch. For example, equilibrioception (see Table 5.2), or the sense of balance, combined with other senses such as the visual system and proprioception, allows humans and animals to gain information about their body position in relation to their immediate surroundings. Sensing can involve the detection of any aspect of the environment, such as the presence, absence or change in a particular attribute. This then provides the agent with an ability to follow a gradient in the environment relating to the attribute. For example, a useful sense for real-life agents is the ability to detect changes in elevation, in order to be able to head in an upwards or downwards direction when required. The Hill Climbing Example model in the NetLogo Models Library shows how such a sense can be implemented. A screenshot of the model is shown in Figure 5.6 and the code is shown in NetLogo Code 5.5. The screenshot shows turtle agents that start out at random locations then move steadily upwards to the highest local point in their immediate surrounding landscape (i.e. towards the patches with the lightest shading). Since a single high point serves as the highest point for many of its surrounding patches, this results in the agents’ paths coalescing into linear patterns of lines as shown in the figure. The vertical line to the left of the figure, for example, represents the top of a ridgeline in the 2D environment. As an analogy with real-life human agents, we can imagine a number of hill walkers climbing from different start positions up a ridge, and once they reach the ridge line, they all end up following a single path to the top.  The model uses the uphill command in NetLogo that directs an agent to move to a neighbouring patch that has the highest value for a particular patch variable. The variable in this case is the patch’s color. The go method asks each turtle to head in an upward direction, until that is no longer possible, in which case the turtle variable peak? is set to true to indicate the turtle has reached the top. When all turtles have reached the top, then no more processing is done.  Is it possible for an agent to perform a non-trivial task without knowing they are performing it? In other words, can an agent ‘solve’ a problem without cognitively being aware they are solving a problem? To answer this question, first we must ask what we mean by ‘cognitively being aware’. Cognition refers to the mental processes of an intelligent agent, either natural or artificial, such as comprehension, reasoning, decision-making, planning and learning. It can also be defined in a broader sense by linking it to the mental act of knowing or recognition of an agent in relation to a thought it is thinking or action it is performing. Thus, cognitive behaviour occurs when an agent knowingly processes its thoughts or actions. Under this definition, an autonomous agent exhibits cognitive behaviour if it knowingly processes sensory information, makes decisions, changes its preferences and applies any existing knowledge it may have while performing a task or mental thought process. Cognition is also related to perception. Perception for an agent is the mental process of attaining understanding or awareness of sensory information. Let us now return to the question posed at the beginning of this section. Consider an insect’s abilities to forage for food. Ants, for example, have the ability to quickly find food sources, but do this by sensing chemical scent laid down by other ants, and then react accordingly using a small set of rules. Despite not being cognitively aware of what they are doing, they achieve the goal of fully exploring the environment, efficiently locating nearby food sources and returning back to the nest. An Ants model provided in the NetLogo Models Library simulates one way how this might happen. A screenshot of the model is shown in Figure 5.7. It shows a colony of ants spread out throughout the environment, with its nest shown by the purple region at the centre of the image. Originally, there were three food sources, but at this stage in the simulation, the previous two have already been devoured and the remaining food source has been found and is in the process of being collected for return to the nest. The white and green shaded region represents how much chemical scent has been load down in the environment, with white representing the greatest amount.  The code for the part of the model that defines the behaviour of the ants is shown in NetLogo Code 5.6. A full listing of the code can be found by selecting the Ants model from the Models Library, and then by clicking on the Procedures button at the top of the NetLogo interface.  Each patch agent has a number of variables associated with it – for example, chemical stores the amount of chemical that ants have laid down on top of it, and nest-scent reflects how close the patch is to the nest. The recolor-patch procedure shows how the patch’s colour is reshaded according to how much chemical has been laid down on top it. The go procedure defines the behaviour of the ants. If the ant is not carrying food, it will look for it (by performing the look-for-food procedure) otherwise it will take the food back to the nest (by performing the return-to-nest procedure). As the agent is returning to the nest, it drops a chemical as it moves.
EN32	1	﻿The way an agent behaves is often used to tell them apart and to distinguish what and who they are, whether animal, human or artificial. Behaviour can also be associated with groups of agents, not just a single agent. For example, human cultural behaviour relates to behaviour that is associated with a particular nation, people or social group, and is distinct from the behaviour of an individual human being or the human body. Behaviour also has an important role to play in the survival of different species and subspecies. It has been suggested, for example, that music and art formed part of a suite of behaviours displayed by our own species that provided us with the evolutionary edge over the Neanderthals.  In the two preceding chapters, we have talked about various aspects concerning behaviours of embodied, situated agents, such as how an agent’s behaviour from a design perspective can be characterised in terms of its movement it exhibits in an environment, and how agents exhibit a range of behaviours from reactive to cognitive. We have not, however, provided a more concrete definition of what behaviour is. From the perspective of designing embodied, situated agents, behaviour can be defined as follows. A particular behaviour of an embodied, situated agent is a series of actions it performs when interacting with an environment. The specific order or manner in which the actions’ movements are made and the overall outcome that occurs as a result of the actions defines the type of behaviour. We can define an action as a series of movements performed by an agent in relation to a specific outcome, either by volition (for cognitive-based actions) or by instinct (for reactive-based actions). With this definition, movement is being treated as a fundamental part of the components that characterise each type of behaviour – in other words, the actions and reactions the agent executes as it is performing the behaviour. The distinction between a movement and an action is that an action comprises one or more movements performed by an agent, and also that there is a specific outcome that occurs as a result of the action. For example, a human agent might wish to perform the action of turning a light switch on. The outcome of the action is that the light gets switched on. This action requires a series of movements to be performed such as raising the hand up to the light switch, moving a specific finger up out of the hand, then using that finger to touch the top of the switch, then applying pressure downwards until the switch moves. The distinction between an action and a particular behaviour is that a behaviour comprises one or more actions performed by an agent in a particular order or manner. For example, an agent may prefer an energy saving type of behaviour by only switching lights on when necessary (this is an example of a cognitive type of behaviour as it involves a conscious choice). Another agent may always switch on the light through habit as it enters a room (this is an example of a mostly reactive type of behaviour). Behaviour is the way an agent acts in a given situation or set of situations. The situation is defined by the environmental conditions, its own circumstances and the knowledge the agent currently has available to it. If the agent has insufficient knowledge for a given situation, then it may choose to search for further knowledge about the situation. Behaviours can be made up of sub-behaviours. The search for further knowledge is itself a behaviour, for example, and may be a component of the original behaviour. There are also various aspects to behaviour, including the following: sensing and movement (sensory-motor co-ordination); recognition of the current situation (classification); decision-making (selection of an appropriate response); performance (execution of the response).  Behaviours range from the fully conscious (cognitive) to the unconscious (reactive), from overt (done in an open way) to covert (done in a secretive way), and from voluntary (the agent acts according to its own free will) to involuntary (done without conscious control or done against the will of the agent). The term ‘behaviour’ also has different meanings depending on the context (Reynolds, 1987). The above definition is applicable when the term is being used in relation to the actions of a human or animal, but it is also applicable in describing the actions of a mechanical system, or the complex actions of a chaotic system, if the agent-oriented perspective is considered (here the agents are humans, animals, mechanical systems or complex systems). However, in virtual reality and multimedia applications, the term can sometimes be used as a synonym for computer animation. In the believable agents and artificial life fields, behaviour is used “to refer to the improvisational and life-like actions of an autonomous character” (Reynolds, 1987). We also often anthropomorphically attribute human behavioural characteristics with how a computer operates when we say that a computer system or computer program is behaving in a certain way based on responses to our interaction with the system or program. Similarly, we often (usually erroneously) attribute human behavioural characteristics with animals and inanimate objects such as cars.  In this section, we will further explore the important distinction between reactive and cognitive behaviour that was first highlighted in the previous chapter. Agents can be characterised by where they sit on a continuum as shown in Figure 6.1. This continuum ranges from purely reactive agents that exhibit no cognitive abilities (such as ants and termites), to agents that exhibit cognitive behaviour or have an ability to think. Table 6.1 details the differences between the two types of agents. In reality, many agents exhibit both reactive and cognitive behaviours to varying degrees, and the distinction between reactive and cognitive can be arbitrary.  Comparing the abilities of reactive agents with cognitive agents listed in Table 6.1, it is clear that reactive agents are very limited in what they can do as they do not have the ability to plan, co-ordinate between themselves or set and understand specific goals; they simply react to events when they occur. This does not preclude them from having a role to play in producing intelligent behaviour. The reactive school of thought is that it is not necessary for agents to be individually intelligent. However, they can work together collectively to solve complex problems. ﻿Communication may be defined as the process of sharing or exchanging of information between agents. An agent exhibits communicating behaviour when it attempts to transmit information to another agent. A sender agent or agents transmits a message through some medium to a receiver agent or agents. The term communication in common English usage can also refer to interactions between people that involve the sharing of information, ideas and feelings. Communication is not unique to humans, though, since animals and even plants also have the ability to communicate with each other. Language can be defined as the set of symbols that agents communicate with in order to convey information. In Artificial Intelligence, human language is often called ‘natural language’ in order to distinguish it from computer programming languages. Communicating using language is often considered to be a uniquely human behavioural trait. Human language, such as spoken, written or sign, is distinguished from animal communication systems in that it is learned rather than inherited biologically. Although various animals exhibit the ability to communicate, and some animals such as orangutans and chimpanzees even have the ability to use certain features of human language, it is the degree of sophistication and complexity in human language that distinguishes it from animal communication systems. Human language is based on the unique ability of humans to think abstractly, using symbols to represent concepts and ideas. Language is defined by a set of socially shared rules that define the commonly accepted symbols, their meaning and their structural relationships specified by rules of grammar. These rules describe how the symbols can be manipulated to create a potentially infinite number of grammatically correct symbol sequences. The specific symbols chosen are arbitrary and can be associated with any particular phoneme, grapheme or sign.  Linguistics is the scientific study of language which can be split into separate areas of study: grammar is the study of language structure; morphology is the study of how words are formed and put together; phonology is the study of systems of sounds; syntax concerns the rules governing how words combine into phrases and sentences; semantics is the study of meaning; and pragmatics concerns the study of language and use and the contexts in which it is used.  In all natural languages, there is a wide variation in usage as well as frequent lack of agreement amongst language users. For example, Table 7.1 lists some examples of acceptable ‘English’ sentences from various regions of the world (Newbrook, 2009). Each of the sentences is regarded as ‘normal’ English for the region shown on the right, and yet most people outside those regions would argue differently, and in many cases have difficulty in understanding their meaning.  Concerning the English language, David Crystal (1988) states: “The English language stretches around the world: from Australia to Zimbabwe, over 300 million people speak it as their mother tongue alone… And yet, despite its astonishingly widespread use as a medium of communication, every profession and every province – indeed, every individual person – uses a slightly different variant.” English has many different regional dialects (such as American, British, Australian and New Zealand English), as well as many sub-dialects within those regions. There are also dialects that cut across regional lines, for example, “Public School English” in Britain, Black English in America and Maori English in New Zealand. And in every country, there are countless social variations that “possess their own bewildering variety of cants, jargons and lingoes” (Claiborne 1990, page 20). One of the more colourful examples is a dictionary on Wall Street slang entitled High steppers, fallen angels, and lollipops (Odean, 1989). It illustrates how such language can become almost unintelligible to the uninitiated. (For example, what is a ‘high stepper’ or a ‘fallen angel’?) Hudson (1983, page 69) writes the following in The language of the teenage revolution about the resentment of older people to the language used by contemporary teenagers : “… perhaps, they dislike the fact that teenagers speak another kind of language, using a considerable number of expressions which they themselves find either incomprehensible or repulsive.” As well as language being diverse, there are many different ways that language is expressed and used. Spoken language is markedly different from written language, as illustrated from the following example taken from Crystal (1981): “This is part of a lecture, and I chose it because it shows that even a professional speaker uses structure that would rarely if ever occur in written English, and displays a ‘disjointedness’ of speech that would be altogether lacking there. (Everyday conversation provides even more striking differences.) The dot (.) indicates a short pause, the dash a longer pause, and the erm is an attempt to represent the noises the speaker made when he was audibly hesitating. … – and I want . very arbitrarily if I may to divide this into three headings --- and to ask . erm . three questions . assessment why – assessment of what – and assessment how . so this is really . means I want to talk about . first of all the purposes of assessment – why we are assessing at all – erm secondly the kind of functions and processes that are being assessed – and thirdly I want to talk about techniques – …” Baugh (1957, page 17) reminds us that language is not just “the printed page” relatively uniform and fixed, as many people think it to be. Language is “primarily speech” and writing “only a conventional device for recoding sounds.” He further states that as the repeated muscular movements which generate speech are subject to gradual alteration on the part of the speaker: “each individual is constantly and quite unconsciously introducing slight changes in his speech. There is no such thing as uniformity in language. Not only does the speech of one community differ from that of another, but the speech of different individuals of a single community, even different members of the same family, is marked by individual peculiarities.”  Some other distinctive forms of language are, for example, poetry, legal documents, newspaper reporting, advertising, letter writing, office correspondence, telegrams, telephone conversations, electronic mail, Usenet news articles, scientific papers and political speeches. Each has their own flavour, quirks and style. And within each form there are individual authors who have their own distinctive styles of language. The plays of Shakespeare and the science fiction novels of H. G. Wells are two examples of very distinct literary styles. In fact, every single person has their own style of language, or idiolect with its own unique characteristics that can be readily discerned by other people (Fromkin et al., 1990). ﻿It is generally acknowledged in Artificial Intelligence research that search is crucial to building intelligent systems and for the design of intelligent agents. For example, Newell (1994) has stated that search is fundamental for intelligent behaviour (see the quote at the beginning of this chapter). From a behavioural perspective, search can be considered to be a meta-behaviour where the agent is making a decision on which behaviour amongst a set of possible behaviours to execute in a given situation. In other words, it can be defined as the behavioural process that the agent employs in order to make decisions about which choice of actions it should perform in order to carry out a specific task. The task may include higher-level cognitive behaviours such as learning, strategy, goal-setting, planning, and modelling (these were called Action Selection in Reynold’s Boids model). If there are no decisions to be made, then searching is not required. If the agent already knows that a particular set of actions, or behaviour, is appropriate for a given situation, then there is no need to search for the appropriate behaviour, and therefore the actions can be applied without coming to any decision. Searching can be considered to be a behaviour that an agent exhibits when it has insufficient knowledge in order to solve a problem. The problem is defined by the current situation of the agent, which is determined by the environmental conditions, its own circumstances and the knowledge the agent currently has available to it. If the agent has insufficient knowledge in order to solve a given problem, then it may choose to search for further knowledge about the problem. If it already has sufficient knowledge, it will not need to employ searching behaviour in order to solve the problem. An agent uses search behaviour in order to answer a question it does not already know the answer to or complete a task it does not know how to complete. The agent must explore an environment by following more than one path in order to obtain that knowledge.  The exploration of the environment is carried out in a manner analogous to early explorers of the American or Australian continents, or people exploring a garden maze such as the Hampton Court Palace Maze or the Chevening House Maze. For the two garden mazes, the people trying to get to the centre of the maze must employ search behaviour if they do not have the knowledge about where it is. If they take a map with them, however, they no longer have to use search behaviour as the map provides them with the knowledge of which paths to take to get to the centre. The early American explorers Lewis and Clark were instructed by Thomas Jefferson to explore the Missouri and find a water route across the continent to the Pacific. They did not already know how large the continent was or what was out there and needed to physically explore the land. They chose to head in a westerly then north-westerly direction following a path along the Missouri river. Similarly, the Australian explorers Burke and Wills led an expedition starting from Melbourne with the goal of reaching the Gulf of Carpentaria. The land at the time had yet to be explored by European settlers. The expedition were prevented from reaching their ultimate goal just three miles short of the northern coastline due to mangrove swamps and, worse still, the expedition leaders died on the return journey. When performing a search, an agent can adopt different behaviours that determine the way the search is performed. The search behaviour adopted can have a significant impact on the effectiveness of the search. For example, poor leadership was blamed for the unsuccessful Burke and Wills expedition. The behaviour can be thought of as a strategy the agent adopts when performing the search. We can consider these search behaviours from an embodied agent perspective. The type of search behaviour is determined both by the embodiment of the agent, and whether the agent employs a reactive or more cognitive behaviour when executing the search. We have already seen many examples of how an agent can perform a search of an environment as a side effect of employing purely reactive behaviour (see Section 5.4 and Figures 5.2, 5.7 to 5.10 and 6.9). We can term these types of search behaviours as reactive search. We can also use the term cognitive search for cases when an agent has an ability to recognize that there is a choice to be made when a choice presents itself in the environment (such as when the agent reaches a junction in the maze). As stated in section 5.5, this act of recognition is a fundamental part of cognitive-based searching behaviour, and it is related to the situation that the agent finds itself in, the way its body is moving and interacting with the environment. It is also related to what is happening in the environment externally, and/or what is happening with other agents in the same environment if there are any. In addition, a single agent can be used to perform any given search, but there is nothing stopping us from using more than one agent to perform the search. We can adopt a multi-agent perspective to describe how each particular search can be performed in order to clarify how the searches differ. In this case, the effectiveness of a particular search can be evaluated by comparing the number of agents that are needed to perform the search and the amount of information that is communicated between them.  There are many problems that require search with varying degrees of difficulty. Simplified or ‘toy’ problems are problems that are to the most part synthetic and unrelated to real life. However, these problems can be useful to designers in gaining insight into the problem of search. Once the properties of the different search behaviours have been investigated on toy problems, then they can be adapted and/or scaled up to real life problems. To illustrate different search problems, this section describes three problems in particular that have been simulated in NetLogo – these are the Searching Mazes model, the Missionaries and Cannibals model, and the Searching for Kevin Bacon model. ﻿Knowledge is essential for intelligent behaviour. Without knowledge, an intelligent agent cannot make informed decisions, and instead must rely on using some form of searching type behaviour involving exploration and/or communication in order to gain the missing knowledge. Humans rely on knowledge every moment of their life – knowledge of how to communicate with other humans, knowledge of where they and other people live and work, knowledge of where things are, knowledge of how to behave in different situations, knowledge of how to perform different tasks and so on. Without the ability to store and process knowledge, the cognitive abilities of a human is seriously curtailed. An illness such as Alzheimer’s, for example, can be debilitating when memory loss occurs such as the difficulty in remembering recently learned facts. We all know (or think we know) what we mean when we use the term ‘knowledge’. But what exactly is knowledge? Bertrand Russell (1926) acknowledged the difficult question of how to define the meaning of ‘knowledge’: “It is perhaps unwise to begin with a definition of the subject, since, as elsewhere in philosophical discussions, definitions are controversial, and will necessarily differ for different schools”.  A definition of knowledge is the subject of ongoing philosophical debate and presently there are many competing theories with no single definition universally agreed upon. Consequently, treatment of knowledge from an Artificial Intelligence perspective has often consciously avoided the definition of what knowledge is. However, this avoidance of providing a definition of knowledge upfront results in a lack of preciseness in the literature and research. The following argument will illustrate why. A knowledge-based system is a term used in Artificial Intelligence to refer to a system that processes knowledge in some manner. We can make the analogy of a knowledge-based system as being a repository of knowledge, whereas a database is a repository of data. However, in this definition, we have neglected to define the meaning of the term ‘knowledge’ and how it is different to data. For example, we can ask ourselves the following question – “What constitutes a knowledge-based system, and how does it differ from a database system?” This is a difficult question that cannot readily be answered in a straightforward way. A common approach taken in the literature is that a knowledge-based system can perform reasoning using some form of inferencing (whether rule-based, frame-based; see below). Modern database systems, however, now employ most of these standard ‘knowledge-based’ techniques and more. Clearly, the addition of inferencing capabilities alone is not sufficient to define what a knowledge-based system is. However, a great deal of A.I. literature makes such an assumption. By avoiding a definition of knowledge, the problem becomes that it is no longer clear that what we are building really is in fact ‘knowledge-based’. In Chapter 1, it was stated that early A.I. systems in the 1970s and 1980s suffered from a lack of evaluation – there was a rush to build new systems, but often very little evaluation was undertaken of how well the systems worked. Without a working definition of knowledge, the same problem occurs now with current knowledge-based systems – how can we evaluate how effective our knowledge-base system might be if we do not have a definition of what it should be (or even achieve or do)? We can, however, avoid the philosophical pitfalls, and rather than attempting to define knowledge, and making a claim that this definition is the “right” one, instead we can propose design principles for our knowledge-based system. Hence, we can decide what principles we wish our knowledge-based system to adhere to, and we, as designers, are free to change them as we see fit based on knowledge we gain during the design process. Also, we are no longer standing on shaky ground in the sense that we do not have to provide one particular definition of knowledge which is open to philosophical debate, although we are still open to criticism about whether our principles are worthwhile from an engineering perspective (i.e. whether they produce “good” programs, or aren’t as good as other approaches). But evaluation becomes much simpler – all we need to do is evaluate whether our design principles are met.  The following are some design principles for knowledge-based systems.  The argument for this design principle is that if we design from an embodied, situated agent perspective, then all knowledge cannot exist independently of the agents. That is, knowledge cannot exist by itself – it can only be found in the ‘minds’ of the agents that are embodied and situated in an environment. We also wish to define and use the term ‘knowledge’ in a way similar to the way the term is used in natural language. The root of the word ‘knowledge’ comes from the verb “to know”. From a natural language perspective, ‘knowing’ and ‘knowledge’ are related. A rock, for example, does not ‘know’ anything. But a dog can ‘know’ where it has buried a bone; and it makes sense to say in natural language that the dog has ‘knowledge’ of where the bone is buried. The dog, in this case, is the agent, and the rock is an object in the environment. In other words, knowing behaviour is associated with an agent who has knowledge. A knowledge-based system can then be thought of as an agent whose role is to convey the knowledge that it contains to the users of the system. The interaction between the user agent and the system agent can be characterised by the actions that determine each agent’s behaviour, and whether the user agent perceives the system agent to be acting in a knowledgeable way. This leads to the next design principle.  The following design principles are based on properties of ‘good’ knowledge-base systems proposed by Russell and Norvig (2002):  A behavioural approach to knowledge places the emphasis not on building a specific independent system, but on building agents that exhibit behaviour that demonstrates they have knowledge of their environment and of other agents. In this approach, the act of ‘knowing’ occurs when an agent has information that might potentially aid the performance of an action taken by itself or by another agent. Further, an agent can be considered to have ‘knowledge’ if it knows what the likely outcomes will be of an action it may perform, or of an action another agent is performing, or what is likely to happen to an object in the environment. ﻿What is the nature of intelligence? That is a question that has been pondered, and debated for thousands of years. Many people over the centuries have offered their own view on the matter, as illustrated by the quotes provided in Table 10.1.  It seems that everyone has their own opinion on what intelligence is or isn’t. Intelligence is a concept that everyone knows about, but understands differently. As we have seen in the previous chapter, the way each person understands a particular concept will have its own unique ‘flavour’. Perhaps one of the most interesting quotes above is by Susan Sontag that uses an analogy between taste and intelligence. Taste is a complex sensation in four dimensions – sweetness, sourness, bitterness and saltiness. Similarly, intelligence is a complex concept, with multiple dimensions.  Intelligence is multi-faceted – its nature cannot be defined using one of these quotes alone; it requires all of them. As an analogy, try describing the Mona Lisa. One person’s description of the painting may be anathema to another person. To imagine that we can distil the Mona Lisa down to a few written words, and then naïvely believe other people will agree with us that it is the one and only definitive description, is like believing that people should only ever eat one type of food, or enjoy looking at one type of painting, or read one type of book. The Mona Lisa painting continues to inspire people to write more and more words about it. Similarly, intelligence is not something we can elucidate definitively. But that will not stop people from continuing to do so, since in so doing further insights can be gained into its nature. Although definitions of intelligence are fraught with problems, we can look for desirable properties of intelligence that we can help us to describe the nature of intelligence. In other words, we can help define the nature of intelligence by describing what it ‘looks’ like or what it ‘tastes’ like. Using the taste analogy, we can think of these properties as being ‘ingredients’ in a recipe for intelligence – we need to mix them together in order to make a particular taste, which some people will like, while others may not, preferring alternative tastes. For example, we can use the analogy of African and Australian explorers trying to describe what a giraffe or platypus looks like to someone who has never seen it. These explorers will use words (concepts) that they are familiar with, such as ‘long neck’ and ‘fish-like tail’, but their description will be ‘flavoured’ by their own unique perspective. Whatever words they come up with, they will have over-emphasized certain features and ignored other important ingredients.  Similarly, AI researchers with a background in knowledge engineering and the symbolic approach to AI will describe intelligence using ingredients such as the following: • the capacity to acquire and apply knowledge; • the ability to perform reasoning; and • the ability to make decisions and plan in order to achieve a specific goal. AI researchers who prefer a behavioural-based approach will describe the intelligent behaviour of embodied, situated agents using ingredients such as: • the ability to perform an action that an external intelligent agent would deem to be intelligent; • the ability to demonstrate knowledge of the consequences of its actions; and • the ability to demonstrate knowledge of how to influence or change its environment in order to affect outcomes and achieve its goals. If we think of intelligence using an analogy of mapping, as discussed in the previous chapter, then we might use the following ingredients to describe intelligence: • the ability of an embodied, situated agent to map environments, both real and abstract (i.e. recognize patterns to provide useful simplifications and/or characterizations of its environments); • the ability to use maps to navigate around its environments; • the ability to update its maps when it finds they do not fit reality; and • the ability to communicate details of its maps to other agents. It is important to realise, however, that these are not definitive descriptions, just ingredients in alternative recipes for intelligence. In the previous chapters, we have seen various examples (implemented as models in NetLogo) that have demonstrated some of these ingredients. In some respects, these models have exhibited a small degree of intelligence in the sense that if we observed a human agent with the same behaviour, we would deem that to be a sign of intelligence. In the next volume of this book series, we will also see other models that will demonstrate more advanced technologies. It can be argued, however, that these examples show no true intelligence – but of course that depends on your own perspective, and the ingredients with which you choose for your own recipe for intelligence.  In the last chapter, a question was asked about whether it was possible to have knowledge without representation. Similarly, we can ask ourselves the following question: “Is it possible to have intelligence without representation?” In a seminal paper, Rodney Brooks (1991) considered exactly this same question. In another paper (Brooks, 1991), he also considered the related question: “Is it possible to have intelligence without reasoning?” As discussed in the previous chapter, Brooks favours the embodied, situated approach to AI – the sub-symbolic paradigm rather than the classical symbolic paradigm. When he talks about the possibility of intelligence without ‘representation’, he means that an embodied, situated agent does not need to explicitly represent its environment – it can simply react to it. There is no need for the agent to have an explicit knowledge base about the world it is situated in since the agent can directly ‘consult’ it by interacting with it. Brooks goes further and states that intelligence is an emergent property of certain complex systems (see quote at the beginning of this chapter). Brook’s ideas are interesting in that it raises the possibility that, in designing AI systems, we may not have to do all the work ourselves. If we can find the right way of setting up the initial conditions of the system, the system itself will do the work for us, and through self-organisation, intelligence will emerge as a result. Unfortunately, although this idea is very intriguing, no one as yet has figured out how to set up the necessary initial conditions. ﻿Setting the boid’s vision cone radius-length or radius-angle will result in the boid not seeing the obstacles and as a result will run right over the top of them. Why? What needs to be fixed so that even without any sensing ability, it will still bounce off the obstacles? Increasing the radius-length (while keeping the other variables the same) discernibly changes the behaviour of the boid. (Try doing this dynamically while moving the radius-length slider back and forth). Instead of covering most of the environment, when the radius length is large then the boid covers a much smaller more constrained area usually at the top of the environment. Sometimes it can get stuck, seemingly trapped in the same place. Try adjusting the boid’s speed, radius angle and radius length to see how this affects the boid’s behaviour. Also try changing the Interface Settings to see if this has any affect. Try adding obstacles to see how this affects the boid’s ability to cover the entire environment. For example, add obstacles in the form of a maze. Try to create “black spots” where the boid never visits. Alternatively, try to trap the boid into a small area. The model could be extended to add gradual acceleration and deceleration. This would enhance the boids simulation. The code uses the in-cone command to simulate the boid’s cone of vision. See the following models: Crowd Path Following, Flocking With Obstacles, Follow and Avoid, Obstacle Avoidance 1, Obstacle Avoidance 2, Vision Cone Example 2, Wall Following Example 2. These are basic implementations of various Craig Reynold’s steering behaviours for boids. Another boid related model is the Biology/Flocking model in the Models Library. This model implements a boid (see Craig Reynold's work) that employs basic obstacle avoidance steering behaviour. It does this by generating a wanderer turtle (boid) that simple wanders around randomly in the environment avoiding the obstacles. The boid is implemented using NetLogo’s in-cone command that implements a turtle with a cone of vision. The model’s Interface buttons are defined as follows: - Setup: This sets up the environment with a grid of obstacles and an outside border (drawn using blue patches). One turtle agent (the wanderer boid) is created and placed at a random location. - Go: The boid starts wandering around the environment avoiding obstacles. - Draw Obstacle: The user can draw further obstacles in the environment. These are coloured brown. - Follow Wanderer: This allows the perspective of the visualisation to be altered so that it is centred on the wanderer. - Plot: This instructs the wanderer turtle agent to put its pen down when wandering. Hence this will draw the path it has taken while wandering around. The model’s Interface sliders are defined as follows: - boid-speed: This controls the speed of the boid i.e. how much it moves forward each tick. - rate-of-random-turn: This controls how much the wandering boid turns each time tick. The boid has a tendency to head in a right turning direction as the rate of random turn to the right (as specified by the slider) is twice that of the rate of random turn to the left. - radius-angle: This defines the radius angle of the boid’s vision cone. - radius-length: This defines the radius length of the boid’s vision cone. Press the Setup button first, then press Go. To see where the boid wanders, press Plot. You can draw extra obstacles by pressing the Draw Obstacle button and then holding down the mouse at the point where you want the obstacles to be drawn. You can change the frame of reference so that the visualisation is centred around where the boid currently is situated by pressing the Follow Wanderer button. Setting the boid-speed to 0.1, rate-of-random-turn to 40, radius-angle to 300, radius-length to 1, and pressing the Plot button once, followed by moving the speed slider (just below the Information tab in the Interface) from "normal speed" to "faster" will result in the boid rapidly covering the entire environment while reliably avoiding the blue obstacles. Increasing the radius-length to 5 (while keeping the other variables the same) discernibly changes the behaviour of the boid. Instead of covering most of the environment, the boid covers a rectangular path of width 4 to 5 around the outside of the environment but indented by about 3 to 4 patches in from the outer boundary. The boid seems to refrain from going inside of the rectangular path. Sometimes the boid can get stuck spinning around one of the obstacles. Try adjusting the boid’s speed, radius angle and radius length to see how this affects the boid’s behaviour. Also try changing the Interface Settings to see if this has any affect. Try adding obstacles to see how this affects the boid’s ability to cover the entire environment. For example, add obstacles in the form of a maze. Try to create “black spots” where the boid never visits. Alternatively, try to trap the boid into a small area. The model could be extended to add gradual acceleration and deceleration. This would enhance the simulation of the boids model. The code uses the in-cone command to simulate the boid’s cone of vision. See the following models: Crowd Path Following, Flocking With Obstacles, Follow and Avoid, Obstacle Avoidance 1, Obstacle Avoidance 2, Vision Cone Example 2, Wall Following Example 2. These are basic implementations of various Craig Reynold's steering behaviours for boids. Another boid related model is the Biology/Flocking model in the Models Library. The turtles in this example follow walls made out of colored patches. The blue turtles try to keep the wall on their right; the green turtles keep the wall on their left. Hence, the blue turtles end up heading in a clockwise direction, and the blue turtles end up in an anti-clockwise direction. (Only the Information in the model that is different to the Wall Following Example model provided by Uri Wilensky is included below.) The model implements the standard unmodified behaviour as provided by Uri Wilensky’s original model, but also provides an alternative modified behaviour where the actions have been split into three independent parts as follows: 1. walk-modified-1: The turtle turns right if necessary. 2. walk-modified-2: The turtle turns left if necessary. 3. walk-modified-3: The turtle moves forward.$$$﻿Setting the boid’s vision cone radius-length or radius-angle will result in the boid not seeing the obstacles and as a result will run right over the top of them. Why? What needs to be fixed so that even without any sensing ability, it will still bounce off the obstacles? Increasing the radius-length (while keeping the other variables the same) discernibly changes the behaviour of the boid. (Try doing this dynamically while moving the radius-length slider back and forth). Instead of covering most of the environment, when the radius length is large then the boid covers a much smaller more constrained area usually at the top of the environment. Sometimes it can get stuck, seemingly trapped in the same place. Try adjusting the boid’s speed, radius angle and radius length to see how this affects the boid’s behaviour. Also try changing the Interface Settings to see if this has any affect. Try adding obstacles to see how this affects the boid’s ability to cover the entire environment. For example, add obstacles in the form of a maze. Try to create “black spots” where the boid never visits. Alternatively, try to trap the boid into a small area. The model could be extended to add gradual acceleration and deceleration. This would enhance the boids simulation. The code uses the in-cone command to simulate the boid’s cone of vision. See the following models: Crowd Path Following, Flocking With Obstacles, Follow and Avoid, Obstacle Avoidance 1, Obstacle Avoidance 2, Vision Cone Example 2, Wall Following Example 2. These are basic implementations of various Craig Reynold’s steering behaviours for boids. Another boid related model is the Biology/Flocking model in the Models Library. This model implements a boid (see Craig Reynold's work) that employs basic obstacle avoidance steering behaviour. It does this by generating a wanderer turtle (boid) that simple wanders around randomly in the environment avoiding the obstacles. The boid is implemented using NetLogo’s in-cone command that implements a turtle with a cone of vision. The model’s Interface buttons are defined as follows: - Setup: This sets up the environment with a grid of obstacles and an outside border (drawn using blue patches). One turtle agent (the wanderer boid) is created and placed at a random location. - Go: The boid starts wandering around the environment avoiding obstacles. - Draw Obstacle: The user can draw further obstacles in the environment. These are coloured brown. - Follow Wanderer: This allows the perspective of the visualisation to be altered so that it is centred on the wanderer. - Plot: This instructs the wanderer turtle agent to put its pen down when wandering. Hence this will draw the path it has taken while wandering around. The model’s Interface sliders are defined as follows: - boid-speed: This controls the speed of the boid i.e. how much it moves forward each tick. - rate-of-random-turn: This controls how much the wandering boid turns each time tick. The boid has a tendency to head in a right turning direction as the rate of random turn to the right (as specified by the slider) is twice that of the rate of random turn to the left. - radius-angle: This defines the radius angle of the boid’s vision cone. - radius-length: This defines the radius length of the boid’s vision cone. Press the Setup button first, then press Go. To see where the boid wanders, press Plot. You can draw extra obstacles by pressing the Draw Obstacle button and then holding down the mouse at the point where you want the obstacles to be drawn. You can change the frame of reference so that the visualisation is centred around where the boid currently is situated by pressing the Follow Wanderer button. Setting the boid-speed to 0.1, rate-of-random-turn to 40, radius-angle to 300, radius-length to 1, and pressing the Plot button once, followed by moving the speed slider (just below the Information tab in the Interface) from "normal speed" to "faster" will result in the boid rapidly covering the entire environment while reliably avoiding the blue obstacles. Increasing the radius-length to 5 (while keeping the other variables the same) discernibly changes the behaviour of the boid. Instead of covering most of the environment, the boid covers a rectangular path of width 4 to 5 around the outside of the environment but indented by about 3 to 4 patches in from the outer boundary. The boid seems to refrain from going inside of the rectangular path. Sometimes the boid can get stuck spinning around one of the obstacles. Try adjusting the boid’s speed, radius angle and radius length to see how this affects the boid’s behaviour. Also try changing the Interface Settings to see if this has any affect. Try adding obstacles to see how this affects the boid’s ability to cover the entire environment. For example, add obstacles in the form of a maze. Try to create “black spots” where the boid never visits. Alternatively, try to trap the boid into a small area. The model could be extended to add gradual acceleration and deceleration. This would enhance the simulation of the boids model. The code uses the in-cone command to simulate the boid’s cone of vision. See the following models: Crowd Path Following, Flocking With Obstacles, Follow and Avoid, Obstacle Avoidance 1, Obstacle Avoidance 2, Vision Cone Example 2, Wall Following Example 2. These are basic implementations of various Craig Reynold's steering behaviours for boids. Another boid related model is the Biology/Flocking model in the Models Library. The turtles in this example follow walls made out of colored patches. The blue turtles try to keep the wall on their right; the green turtles keep the wall on their left. Hence, the blue turtles end up heading in a clockwise direction, and the blue turtles end up in an anti-clockwise direction. (Only the Information in the model that is different to the Wall Following Example model provided by Uri Wilensky is included below.) The model implements the standard unmodified behaviour as provided by Uri Wilensky’s original model, but also provides an alternative modified behaviour where the actions have been split into three independent parts as follows: 1. walk-modified-1: The turtle turns right if necessary. 2. walk-modified-2: The turtle turns left if necessary. 3. walk-modified-3: The turtle moves forward.
EN22	0	﻿While there is a study guide (available from Ventus) that focuses largely on objects and their characteristics, it will be instructive to the learner (of the Java programming language) to understand how the concept of an object is applied to their construction and use in Java applications. Therefore, Chapter One (of this guide) introduces the concept of an object from a language-independent point of view and examines the essential concepts associated with object-oriented programming (OOP) by briefly comparing how OOP and non-OOP approach the representation of data and information in an application. The chapter goes on to explain classes, objects and messages and concludes with an explanation of how a class is described with a special diagram known as a class diagram.  Despite the wide use of OOP languages such as Java, C++ and C#, non-OOP languages continue to be used in specific domains such as for some categories of embedded applications. In a conventional, procedural language such as C, data is sent to a procedure for processing; this paradigm of information processing is illustrated in Figure 1.1 below.  The figure shows that the number 4 is passed to the function (SQRT) which is ‘programmed’ to calculate the result and output it (to the user of the procedure). In general, we can think of each procedure in an application as ready and waiting for data items to be sent to them so that they can do whatever they are programmed to do on behalf of the user of the application. Thus an application written in C will typically comprise a number of procedures along with ways and means to pass data items to them.  The way in which OOP languages process data, on the other hand, can be thought of as the inverse of the procedural paradigm. Consider Figure 1.2 below.  In the figure, the data item – the number 4 – is represented by the box (with the label ‘4’ on its front face). This representation of the number 4 can be referred to as the object of the number 4. This simple object doesn’t merely represent the number 4, it includes a button labeled sqrt which, when pressed, produces the result that emerges from the slot labeled return. Whilst it is obvious that the object-oriented example is expected to produce the same result as that for the procedural example, it is apparent that the way in which the result is produced is entirely different when the object-oriented paradigm considered. In short, the latter approach to producing the result 2 can be expressed as follows.  A message is sent to the object to tell it what to do. Other messages might press other buttons associated with the object. However for the present purposes, the object that represents the number 4 is a very simple one in that it has only one button associated with it. The result of sending a message to the object to press its one and only button ‘returns’ another object. Hence in Figure 1.2, the result that emerges from the ‘return’ slot - the number 2 – is an object in its own right with its own set of buttons. Despite the apparent simplicity of the way in which the object works, the question remains: how does it calculate the square root of itself? The answer to this question enshrines the fundamental concept associated with objects, which is to say that objects carry their programming code around with them. Applying this concept to the object shown in Figure 1.2, it has a button which gives access to the programming code which calculates the square root (of the number represented by the object). This amalgam of data and code is further illustrated by an enhanced version of the object shown in Figure 1.3 below.  The enhanced object (representing the number 4) has two buttons: one to calculate the square root of itself – as before - and a second button that adds a number to the object. In the figure, a message is sent to the object to press the second button – the button labeled ‘+’ – to add the object that represents the number 3 to the object that represents the number 4. For the ‘+’ button to work, it requires a data item to be sent to it as part of the message to the object. This is the reason why the ‘+’ button is provided with a slot into which the object representing the number 3 is passed. The format of the message shown in the figure can be expressed as follows.  When this message is received and processed by the object, it returns an object that represents the number 7. In this case, the message has accessed the code associated with the ‘+’ button. The enhanced object can be thought of as having two buttons, each of which is associated with its own programming code that is available to users of the object.  Extrapolating the principal of sending messages to the object depicted in Figure 1.3 gives rise to the notion that an object can be thought of as comprising a set of buttons that provide access to operations which are carried out depending on the details in the messages sent to that object.  In summary: in procedural programming languages, data is sent to a procedure; in an object-oriented programming language, messages are sent to an object; an object can be thought of as an amalgam of data and programming code: this is known as encapsulation.  Whilst the concept of encapsulation is likely to appear rather strange to learners who are new to OOP, working with objects is a much more natural way of designing applications compared to designing them with procedures. Objects can be constructed to represent anything in the world around us and, as such, they can be easily re-used or modified. Given that we are surrounded by things or objects in the world around us, it seems natural and logical that we express this in our programming paradigm.  The next section takes the fundamental concepts explored in this section and applies them to a simple object.  ﻿The aim of Chapter Two is to take the simple class diagram shown at the end of Chapter One and explain how it is translated into Java source code. The code is explained in terms of its attributes, constructor and behaviour and a test class is used to explain how its constructor and behaviour elements are used. Before we embark on our first Java programme, let us recall the class diagram with which we concluded Chapter One. The class diagram is reproduced in Figure 2.1 below, with the omission of the constructor: this is to keep the code simple to begin with. We will replace the constructor in the class diagram and provide code for it later in this chapter. In Figure 2.1, let us be reminded that the qualifier ‘-‘ means private and the qualifier ‘+’ means public. The purpose of these qualifiers will be revealed when we write the code for the class. The next section explains how the information in the class diagram shown in Figure 2.1 is translated into Java source code. Remember that, in general, a class definition declares attributes and defines constructors and behaviour. The Java developer concentrates on writing types called classes, as a result of interpreting class diagrams and other elements of the OOA & D of an application’s domain. The Java developer also makes extensive use of the thousands of classes provided by the originators of the Java language (Sun Microsystems Inc.) that are documented in the Java Applications Programming Interface (API). We have established that classes typically comprise attributes and the behaviour that is used to manipulate these data. Attributes are implemented, in Java, as variables, whose value determines the condition or state of an object of that class and behaviour elements are implemented using a construct known as a method. When a method is executed, it is said to be called or invoked. As has been mentioned earlier, an instance of a class is also called an object, such that, perhaps somewhat confusingly, the terms instance and object are interchangeable in Java. The requirement to create an instance of a class from the definition of the class gives rise to a fundamental question: how do we actually create an instance of a class so that its methods can be executed? We will address this question in this section. One of the components of a class, which we haven’t explained fully so far in the discussion of the Member class, is its constructor. A constructor is used to create or construct an instance of that class. Object construction is required so that the Java run-time environment (JRE) can respond to a call to an object’s constructor to create an actual object and store it in memory. An instance does not exist in memory until its constructor is called; only its class definition is loaded by the (JRE). We will meet the constructor for the Member class later. Broadly, then, we can think of the Java developer as writing Java classes, from which objects can be constructed (by calling their constructors). Classes are to objects as an architect’s plan is to a house, i.e. we can produce many houses from a single plan and we can construct or instantiate many instances from a single template known as a class. Given that objects can communicate with other objects, this gives the developer the means to re-use classes from one application in another application. Therefore, with Java object technology, we can build software applications by combining re-useable and interchangeable objects, some of which can be standardised in terms of their interface. This is probably the single-most important advantage of object-oriented programming (OOP) compared with non-OOP in application development. We are now at the stage when we can translate the class diagram for the Member class into Java source code, often shortened to ‘code’. The code that follows is the class definition of the class named Member but includes only some of the attributes and methods that do not involve object types: this is to keep the example straightforward. The reason for this restriction is that if we were to declare attributes or parameters of the MembershipCard class type in the class Member, as required by the class diagram, the Java compiler would look for the class definition of the class MembershipCard. In order to keep the example straightforward, we will only write the class definition for the class Member for the time being; we will refer to the class definition of the class MembershipCard in a later chapter. Thus, in this section, we will work with a single class that includes only primitive data types; there are no class types included in the simplified class diagram. In order to make the example code even more straightforward, the class diagram is further simplified as shown in the next diagram. The class diagram that we will translate into Java code declares two variables and their corresponding ‘setter’ (or mutator) and ‘getter’ (or accessor) methods, as follows. The reason for the simplification (of the full class diagram) is so that the class definition can be more easily understood, compared to its full definition. In short, we well keep our first Java programme as simple as possible. In the class definition that follows below, ‘ // ‘ is a single-line comment and ‘ /** … */ ‘ is a block comment and, as such, are ignored by the Java compiler. For the purposes of the example, Java statements are written in bold and comments in normal typeface. // Class definition for the class diagram shown in Figure 2.2. Note that the name of // the class starts, by convention, with a capital letter and that it is declared as public. // The first Java statement is the class declaration. Note that the words public and // class must begin with a lower case letter. public class Member { // The class declaration. // Declare instance variables first. Things to note: // String types in Java are objects and are declared as ‘String’, not ‘string’. // The qualifier 'private' is used for variables. // 'String' is a type and 'userName' and ‘password’ are variable names, also // known as identifiers. Thus, we write the following: ﻿In Chapter Two, we see that class attributes are implemented in Java programmes as variables, whose values determine the state of an object. To some extent Chapter Two addresses the question of how we name variables; this question is explored further in this chapter. Chapter Three explores some of the basic elements of the Java language. Given the nature of this guide, it is not the intention to make this chapter exhaustive with respect to all of the basic elements of the Java language. Further details can be found in the on-line Java tutorial. We see in Chapter Two that the two broad categories of Java types are primitives and classes. There are eight of the former and a vast number of classes, including several thousand classes provided with the Java language development environment and an infinitude of classes written by the worldwide community of Java developers. This chapter examines aspects of both categories of types. An identifier is a meaningful name given to a component in a Java programme. Identifiers are used to name the class itself – where the name of the class starts with an upper case letter – and to name its instances, its methods and their parameters. While class identifiers always – by convention – start with an upper case letter, everything else is identified with a word (or compound word) that starts with a lower case letter. Identifiers should be made as meaningful as possible, in the context of the application concerned. Thus compound words or phrases are used in practice. Referring to elements of the themed application, we can use the following identifiers for variables in the Member class: because we wouldn’t name a class membershipCard and spaces are not permitted in identifiers. We could have declared other variables in the class definition as follows: We cannot use what are known as keywords for identifiers. These words are reserved and cannot be used solely as an identifier, but can be used as part of an identifier. Thus we cannot identify a variable as follows: // not permitted because int is a keyword but we could write The table below lists the keywords in the Java language. Java is case-sensitive: this means that we cannot expect the following statement to compile: if we have not previously declared the identifier newint. On the other hand, if we write as the last statement of the getNewInt method, it will compile because the identifier named newInt has been declared previously. Similarly we cannot expect the compiler to recognise identifiers such as the following if they have not been declared before we refer to them later in our code. In one of the declarations in Section 3.2, we declared a variable with the identifier newInt to be of the int type, in the following statement: Let us deconstruct this simple statement from right to left: we declare that we are going to use an identifier named newInt to refer to integer values and ensure that access to this variable is private. This kind of declaration gives rise to an obvious question: what primitive data types are there in the Java language? The list on the next page summarises the primitive data types supported in Java. Before we move on to discuss assignment of actual values to variables, it will be instructive to find out if Java can convert between types automatically or whether this is left to the developer and if compile-time and run-time rules for conversion between types are different. In some situations, the JRE implicitly changes the type without the need for the developer to do this. All conversion of primitive data types is checked at compile-time in order to establish whether or not the conversion is permissible. Consider, for example, the following code snippet: A value of 10.0 is displayed when d is output. Evidently the implicit conversion from an int to a double is permissible. Consider this code snippet: The first statement compiles; this means that the implicit conversion from an int to a double is permissible when we assign a literal integer value to a double. However the second statement does not compile: the compiler tells us that there is a possible loss of precision. This is because we are trying to squeeze, as it were, an eight byte value into a four byte value (see Table 3.2); the compiler won’t let us carry out such a narrowing conversion. On the other hand, if we write: // the cast ( int ) forces d to be an int; we will examine the concept of casting // or explicit conversion later in this section Both statements compile and a value of 10 is displayed when i is output. The general rules for implicit assignment conversion are as follows: a boolean cannot be converted to any other type; a non-boolean type can be converted to another non-boolean type provided that the conversion is a widening conversion; a non-boolean type cannot be converted to another non-boolean type if the conversion is a narrowing conversion. Another kind of conversion occurs when a value is passed as an argument to a method when the method defines a parameter of some other type. For example, consider the following method declaration: The method is expecting a value of a double to be passed to it when it is invoked. If we pass a float to the method when it is invoked, the float will be automatically converted to a double. Fortunately the rules that govern this kind of conversion are the same as those for implicit assignment conversion listed above. The previous sub-section shows that Java is willing to carry out widening conversions implicitly. On the other hand, a narrowing conversion generates a compiler error. Should we actually intend to run the risk of the possible loss of precision when carrying out a narrowing conversion, we must make what is known as an explicit cast. Let us recall the following code snippet from the previous sub-section: Casting means explicitly telling Java to force a conversion that the compiler would otherwise not carry out implicitly. To make a cast, the desired type is placed between brackets, as in the second statement above, where the type of d – a double - is said to be cast (i.e. flagged by the compiler to be converted at run-time) into an int type.  ﻿By now the learner will be familiar, to some extent, with method invocation from earlier chapters, when objects of the Member class in the themed application are used to give some examples of passing arguments to methods. Chapter Four goes into more detail about methods and gives a further explanation about how methods are defined and used. Examples from the themed application are used to illustrate the principal concepts associated with an object’s methods. Chapter Three examines an object’s variables, i.e. its state or what it knows what its values are. An object’s methods represent the behaviour of an object, or what is knows what it can do, and surround, or encapsulate, an object’s variables. This section answers the question about how we get computable values into methods. As we know from previous chapters, a method is invoked by selecting the object reference for the instance required. The general syntax of a method invocation can be summarised as follows. Referring, again, to the Member class of the themed application, we could instantiate a number of Member objects (in a main method) and call their methods as in the following code snippet. // Instantiate three members; call the no-arguments constructor for the Member class. // Call one of the set methods of these objects. // Call one of the get methods of these objects in a print statement. The screen output from executing this fragment of main is: In short, we must ensure that we know which method we are calling on which object and in which order. In the code snippet above, it is evident that setUserName expects a String argument to be passed to it; this is because its definition is written as: The single parameter is replaced by a computable value, i.e. an argument, when the method is invoked. The general syntax of a method’s declaration is modifier return_type method_name( parameter_list ) exception_list The method’s definition is its declaration, together with the body of the method’s implementation between braces, as follows: The method’s signature is its name and parameter list. It is in the body of a method where application logic is executed, using statements such as: invocations: calls to other methods; assignments: changes to the values of fields or local variables; selection: cause a branch; repetition: cause a loop; detect exceptions, i.e. error conditions. If the identifier of a parameter is the same as that of an instance variable, the former is said to hide the latter. The compiler is able to distinguish between the two identifiers by the use of the keyword ‘this’, as in the following method definition that we met in Chapter One: If, on the other hand, we wish to avoid hiding, we could write the method definition as follows: where the identifier of the parameter is deliberately chosen to be different from that of the instance variable. In this case, the keyword ‘this’ can be included but it is not necessary to do so. In both versions of the method setUserName, the value of the parameter’s argument has scope only within the body of the method. Thus, in general, arguments cease to exist when a method completes its execution. A final point to make concerning arguments is that a method cannot be passed as an argument to another method or a constructor. Instead, an object reference is passed to the method or constructor so that the object reference is made available to that method or constructor or to other members of the class that invoke that method. For example, consider the following code snippet from the graphical version of the themed application shown on the next page. The examples and discussion in this section are meant to raise a question in the mind of the learner: are arguments passed by value or by reference? This question is addressed in the next sub-section. All arguments to methods (and constructors) are, in Java, passed by value. This means that a copy of the argument is passed in to a method (or a constructor) call. The example that follows aims to illustrate what pass by value semantics means in practice: detailed code documentation is omitted for the sake of clarity. The method changeValue changes the value of the argument passed to it – a copy of x – but it does not change the original value of x, as shown by the output. Thus the integer values 1235 and 1234 are output according to the semantics of pass by value as they apply to arguments. When a parameter is an object reference, it is a copy of the object reference that is passed to the method. You can change which object the argument refers to inside the method, without affecting the original object reference that was passed. However if the body of the method calls methods of the original object – via the copy of its reference - that change the state of the object, the object’s state is changed for the duration of its scope in a programme. Thus, in the example above, the strings “Bonjour” and “Hello there!” are output according to the semantics of pass by value as they apply to object references. A common misconception about passing object references to methods or constructors is that Java uses pass by reference semantics. This is incorrect: pass by reference would mean that if used by Java, the original reference to the object would be passed to the method or constructor, rather than a copy of the reference, as is the case in Java. The Java language passes object references by value, in that a copy of the object reference is passed to the method or constructor. The statement in the box isn’t true when objects are passed amongst objects in a distributed application. However, such applications are beyond the scope of this guide. For the purposes of the present guide, the learner should use the examples above to understand the consequences of Java’s use of pass by value semantics. In previous chapters, we have encountered a number of references to a method’s return type. In the definition of a method, the return type is declared as part of the method’s declaration and its value is returned by the final statement of the method.  ﻿There are several examples in previous chapters that illustrate how constructors are used to instantiate objects of a class. Let us recall the overall technique before we bring together a number of features of constructors in this chapter. One of the constructors for Member objects in the themed application is as follows: An object’s constructors have the same name as the class they instantiate. To access an object of the class Member in an application, we first declare a variable of the Member type in a main method in a test class as follows: The statement above does not create a Member object; it merely declares a variable of the required type that can subsequently be initialised to refer to an instance of the Member type. The variable that refers to an object is known as its object reference. The object that an object reference refers to must be created explicitly, in a statement that instantiates a Member object as follows. The two statements above can be combined as follows. When the Member object is created by using ‘new’, the type of object required to be constructed is specified and the required arguments are passed to the constructor. The JRE allocates sufficient memory to store the fields of the object and initialises its state. When initialisation is complete, the JRE returns a reference to the new object. Thus, we can regard a constructor as returning an object reference to the object stored in memory. While objects are explicitly instantiated using ‘new’, as shown above for a Member object, there is no need to explicitly destroy them (as is required in some OO run-time systems). The Java Virtual Machine (JVM) manages memory on behalf of the developer so that memory for objects that is no longer used in an application is automatically reclaimed without the intervention of the developer. In general, an object’s fields can be initialised when they are declared or they can be declared without being initialised. For example, the code snippet on the next page shows part of the class declaration for a version of the Member class: The code snippet illustrates an example where some of the instance variables are initialised and some are only declared. In the case of the latter type of declaration, the instance variable is initialised to its default value when the constructor returns an object reference to the newly-created object. For example, the instance variable noOfCards is initialised to 0 when the object is created. Declaring and initialising none, some or all instance variables in this way if often sufficient to establish the initial state of an object. On the other hand, where more than simple initialisation to literals or default values is required and where other tasks are required to be performed, the body of a constructor can be used to do the work of establishing the initial state of an object. Consider the following part of the constructor for the Member class. This constructor is used when simple initialisation of Member objects is insufficient. Thus, in the code block of the constructor above, the arguments passed to the constructor are associated with four of the fields of the Member class. The effect of the four statements inside the constructor’s code block is to initialise the four fields before the constructor returns a reference to the object. Constructors can, like methods, generate or throw special objects that represent error conditions. These special objects are instances of Java’s in-built Exception class. We will explore how to throw and detect Exception objects in Chapter Four in An Introduction to Java Programming 2: Classes in Java Applications. It is worthwhile being reminded at this point in the discussion about constructors that the compiler inserts a default constructor if the developer has not defined any constructors for a class. The default constructor takes no arguments and contains no code. It is provided automatically only if the developer has not provided any constructors in a class definition. We saw in the previous chapter that methods can be overloaded. Constructors can be similarly overloaded to provide flexibility in initialising the state of objects of a class. For example, the following class definition includes more than one constructor. The example class – SetTheTime – is a simple illustration of a class which provides more than one constructor. The example also shows that a constructor can be called from the body of another constructor by using the ‘this’ invocation as the first executable statement in the constructor. Thus, in the example above, the two argument constructor is called in the first statement of the three argument constructor. Complex initialisation of fields can be achieved by using what is known as an initialisation block. An initialisation block is a block of statements, delimited by braces, that appears near the beginning of a class definition outside of any constructor definitions. The position of such a block can be generalised in the following simple template for a typical class definition: An initialisation block is executed as if it were placed at the beginning of every constructor of a class. In other words, it represents a common block of code that every constructor executes. Thus far, in this study guide, we have only been able to work with single values of primitive data types and object references. In the next chapter, we will find out how we can associate multiple values of types with a single variable so that we can work with multiple values of primitives or object references in an application. ﻿When writing the getClient() method the author was fully aware that a client may not be found and in this case decided to return a NULL value. However this relies on every programmer who ever uses this method to recognise and protect against this eventuality. If any programmer using this method failed to protect against a NULL return then their program could crash – potentially in this case losing the bank large sums of money. Of course in other applications, such as an aircraft control system, a program crash could have life threatening results. A more secure programming method is required to ensure that that a potential crash situation is always dealt with! Such a mechanism exists - it is a mechanism called ‘exceptions’. By using this mechanism we can ensure that other programmers who use our code will be alerted to potential crash situations and the compiler will ensure that these programmers deal with the ‘issue’. Thus we can ensure that no such situation is ‘forgotten’. How they are dealt with remains a choice with a programmer who uses our methods but the compiler will ensure that they at least recognise a potential crash situation. In the situation above rather than return a NULL value the getClient() method should generate an exception. By generating an exception the Java compiler will ensure that this situation is dealt with. In order to generate meaningful exceptions we need to extend the Exception classes built into the Java language – there are two of these (normal exceptions and run time exceptions). Subclasses of java.lang.Exception are used for anticipated problems which need to be managed. They must be declared in the originating method’s throws clause and a call to method must be placed in try/catch block. Subclasses of java.lang.RuntimeException are used for situations which lead to runtime failure and where it may not be possible to take any sensible remedial actions. They do not need to be declared in throws clause and a call need not be in try/catch block (but can be). Thus we have the choice as to whether the Java compiler should force us to explicitly deal with a particular kind of exception. Exception subclasses are appropriate for things which we know might go wrong and where we can take sensible recovery action – e.g. IO errors. RuntimeException subclasses are appropriate for things which should not happen at all and where there is probably nothing we can do to recover the situation, e.g. an out of memory error or discovering that the system is in an inconsistent state which should never be able to arise. When writing our own methods we should look for potential failure situations (e.g. value that cannot be returned, errors that may occur in calculation etc). When a potential error occurs we should generate an ‘Exception’ object i.e. an object of the Exception class. However it is best to first define a subclass of the general Exception i.e. to create a specialised class and throw an object of this subtype. A new exception is just like any new class in this case it is a subclass of java.lang.Exception In the case above an error could occur if no client is found with a specified ID. Therefore we could create a new exception class called ‘UnknownClientException’. The parameter to the constructor for the Exception requires a Sting thus the constructor for UnknownClientException also requires a String. This string is used to give details of the problem that may generate an exception. The code to create this new class is given below….. In some respects this looks rather odd. Here we are creating a subclass of Exception but our subclass does not contain any new methods – nor does it override any existing methods. Thus its functionality is identical to the superclass – however it is a subtype with a meaningful and descriptive name. If sublasses of Exception did not exist we would only be able to catch the most general type of exception i.e an Exception object. Thus we would only be able to write a catch block that would catch every single type of exception. Having defined a subclass we instead have a choice… a) we could define a catch block to catch objects of the general type ‘Exception’ i.e. it would catch ALL exceptions or b) we could define a catch block that would catch UnknownClientExceptions but would ignore other types of exception. By looking at the online API we can see that many predefined subclass of exception already exist. There are many of these including :- IOException o CharConversionException o EOFException o FileNotFoundException o ObjectStreamException NullPointerException PrinterException SQLexception Thus we could write a catch block that would react to any type of exception, or we could limited it to input \ output exceptions or we could be even more specific and limit it to FileNotFound exceptions. Having defined our own exception we must then instruct the getClient() method to throw this exception (assuming a client has not been found with the specified ID). To do this we must first tell the compiler that this class may generate an exception – the complier will then ensure that any future programmer who makes use of this method catches this exception. To tell the compiler this method throws an exception we add the following statement to the methods signature ‘throws UnknownClientException’. We must create a new instance of the UnknownClientException class and apply the throw keyword to this newly created object. We use the keyword ‘throw’ to throw an exception at the appropriate point within the body of the method. In the example above if a client is found the method will return the client object. However it will no longer return a NULL value. Instead if a client has not been found the constructor for UnknownClientException is invoked, using ‘new’. This constructor requires a String parameter – and the string we are passing here is an error message that is trying to be informative and helpful. The message is specifying :- the class which generated the exception (i.e. BookOfClients), the method within this class (i.e. getClient()), some text which explains what caused the exception and the value of the parameter for which a client could not be found.$$$﻿When writing the getClient() method the author was fully aware that a client may not be found and in this case decided to return a NULL value. However this relies on every programmer who ever uses this method to recognise and protect against this eventuality. If any programmer using this method failed to protect against a NULL return then their program could crash – potentially in this case losing the bank large sums of money. Of course in other applications, such as an aircraft control system, a program crash could have life threatening results. A more secure programming method is required to ensure that that a potential crash situation is always dealt with! Such a mechanism exists - it is a mechanism called ‘exceptions’. By using this mechanism we can ensure that other programmers who use our code will be alerted to potential crash situations and the compiler will ensure that these programmers deal with the ‘issue’. Thus we can ensure that no such situation is ‘forgotten’. How they are dealt with remains a choice with a programmer who uses our methods but the compiler will ensure that they at least recognise a potential crash situation. In the situation above rather than return a NULL value the getClient() method should generate an exception. By generating an exception the Java compiler will ensure that this situation is dealt with. In order to generate meaningful exceptions we need to extend the Exception classes built into the Java language – there are two of these (normal exceptions and run time exceptions). Subclasses of java.lang.Exception are used for anticipated problems which need to be managed. They must be declared in the originating method’s throws clause and a call to method must be placed in try/catch block. Subclasses of java.lang.RuntimeException are used for situations which lead to runtime failure and where it may not be possible to take any sensible remedial actions. They do not need to be declared in throws clause and a call need not be in try/catch block (but can be). Thus we have the choice as to whether the Java compiler should force us to explicitly deal with a particular kind of exception. Exception subclasses are appropriate for things which we know might go wrong and where we can take sensible recovery action – e.g. IO errors. RuntimeException subclasses are appropriate for things which should not happen at all and where there is probably nothing we can do to recover the situation, e.g. an out of memory error or discovering that the system is in an inconsistent state which should never be able to arise. When writing our own methods we should look for potential failure situations (e.g. value that cannot be returned, errors that may occur in calculation etc). When a potential error occurs we should generate an ‘Exception’ object i.e. an object of the Exception class. However it is best to first define a subclass of the general Exception i.e. to create a specialised class and throw an object of this subtype. A new exception is just like any new class in this case it is a subclass of java.lang.Exception In the case above an error could occur if no client is found with a specified ID. Therefore we could create a new exception class called ‘UnknownClientException’. The parameter to the constructor for the Exception requires a Sting thus the constructor for UnknownClientException also requires a String. This string is used to give details of the problem that may generate an exception. The code to create this new class is given below….. In some respects this looks rather odd. Here we are creating a subclass of Exception but our subclass does not contain any new methods – nor does it override any existing methods. Thus its functionality is identical to the superclass – however it is a subtype with a meaningful and descriptive name. If sublasses of Exception did not exist we would only be able to catch the most general type of exception i.e an Exception object. Thus we would only be able to write a catch block that would catch every single type of exception. Having defined a subclass we instead have a choice… a) we could define a catch block to catch objects of the general type ‘Exception’ i.e. it would catch ALL exceptions or b) we could define a catch block that would catch UnknownClientExceptions but would ignore other types of exception. By looking at the online API we can see that many predefined subclass of exception already exist. There are many of these including :- IOException o CharConversionException o EOFException o FileNotFoundException o ObjectStreamException NullPointerException PrinterException SQLexception Thus we could write a catch block that would react to any type of exception, or we could limited it to input \ output exceptions or we could be even more specific and limit it to FileNotFound exceptions. Having defined our own exception we must then instruct the getClient() method to throw this exception (assuming a client has not been found with the specified ID). To do this we must first tell the compiler that this class may generate an exception – the complier will then ensure that any future programmer who makes use of this method catches this exception. To tell the compiler this method throws an exception we add the following statement to the methods signature ‘throws UnknownClientException’. We must create a new instance of the UnknownClientException class and apply the throw keyword to this newly created object. We use the keyword ‘throw’ to throw an exception at the appropriate point within the body of the method. In the example above if a client is found the method will return the client object. However it will no longer return a NULL value. Instead if a client has not been found the constructor for UnknownClientException is invoked, using ‘new’. This constructor requires a String parameter – and the string we are passing here is an error message that is trying to be informative and helpful. The message is specifying :- the class which generated the exception (i.e. BookOfClients), the method within this class (i.e. getClient()), some text which explains what caused the exception and the value of the parameter for which a client could not be found.
EN27	0	﻿A regular feature in the New Scientist magazine is Enigma, a weekly puzzle entry which readers are invited to solve. In the 8 February 2003 issue [1] the following puzzle was published. First, draw a chessboard. Now number the horizontal rows 1, 2, ..., 8, from top to bottom and number the vertical columns 1, 2, ..., 8, from left to right.You have to put a whole number in each of the sixty-four squares, subject to the following: 1. No two rows are exactly the same. 2. Each row is equal to one of the columns, but not to the column with the same number as the row. 3. If N is the largest number you write on the chessboard then you must also write 1, 2, ...,N −1 on the chessboard. The sum of the sixty-four numbers you write on the chessboard is called your total. What is the largest total you can obtain? We are going to solve this puzzle here using Prolog. The solution to be described will illustrate two techniques: unification and generate-and-test. Unification is a built-in pattern matching mechanism in Prolog which has been used in [9]; for example, the difference list technique essentially depended on it. For our approach here, unification will again be crucial in that the proposed method of solution hinges on the availability of built-in unification. It will be used as a kind of concise symbolic pattern generating facility without which the current approach wouldn’t be viable. Generate-and-test is easily implemented in Prolog. Prolog’s backtracking mechanism is used to generate candidate solutions to the problem which then are tested to see whether certain of the problem-specific constraints are satisfied. Fig. 1.1 shows a board arrangement with all required constraints satisfied. It is seen that the first requirement is satisfied since the rows are all distinct. The second condition is also seen to hold whereby rows and columns are interrelated in the following fashion: We use the permutation to denote the corresponding column–to–row transformation. The board also satisfies the latter part of the second condition since no row is mapped to a column in the same position. In terms of permutations, this requirement implies that no entry remains fixed; these are those permutations which in our context are permissible. 2 The third condition is obviously also satisfied with N = 6. The board’s total is 301, not the maximum, which, as we shall see later, is 544. The solution scheme described below in i–v is based on first generating all feasible solutions (an example of which was seen in Sect. 1.2) and then choosing a one with the maximum total. i. Take an admissible permutation, such as π in (1.1). ii. Find an 8 ×8 matrix with symbolic entries whose rows and columns are interrelated by the permutation in i. As an example, let us consider for the permutation π two such matrices, M1 and M2, with M1 and M2 both satisfy conditions 1 and 2. We also observe that the pattern of M2 may be obtained from that of M1 by specialization (by matching the variables X1 and X6). Thus, any total achievable for M2 is also achievable for M1. For any given permissible permutation, we can therefore concentrate on the most general pattern of variables, M. (We term a pattern of variables most general if it cannot be obtained by specialization from a more general one.) All this is reminiscent of ‘unification’ and the ‘most general unifier’, and we will indeed be using Prolog’s unification mechanism in this step. iii. Verify condition 1 for the symbolic matrix M. 3 Once this test is passed, we are sure that also the latter part of condition 2 is satisfied. 4 iv. We now evaluate the pattern M. If N symbols have been used in M, assign the values 1, ...,N to them in reverse order by first assigning N to the most frequently occurring symbol, N − 1 to the second most frequently occurring symbol etc. The total thus achieved will be a maximum for the given pattern M. v. The problem is finally solved by generating and evaluating all patterns according to i–iv and selecting a one with the maximum total. The original formulation from the New Scientist uses a chessboard but the problem can be equally set with a square board of any size. In our implementation, we shall allow for any board size since this will allow the limitations of the method employed to be explored. We write matrices in Prolog as lists of their rows which themselves are lists. Permutations will be represented by the list of the bottom entries of their two-line representation; thus, [2, 3, 1, 5, 6, 7, 8, 4] stands for π in (1.1). First, we want to generate all permutations of a list. Let us assume that we want to do this by the predicate permute(+List,-Perm) and let us see how List = [1, 2, 3, 4] might be permuted. A permuted list, Perm = [3, 4, 1, 2] say, may be obtained by • Removing from List the entry E = 3, leaving the reduced list R = [1, 2, 4] • Permuting the reduced list R to get P = [4, 1, 2] • Assembling the permuted list as [E|P] = [3, 4, 1, 2] . Lists with a single entry are left unchanged. This gives rise to the definition with the predicate remove one(+List,?Entry,?Reduced) defined by (Here we remove either the head or an entry from the tail.) For a permutation to be admissible, all entries must have changed position. We implement this by To generate a list of N unbound variables, L, we use var list(+N,-L) which is defined in terms of length(-L,+N) By Matrices with distinct symbolic entries may now be produced by mapping; for example, a 3 × 2 matrix is obtained by It is now that Prolog shows its true strength: we use unification to generate symbolic square matrices with certain patterns.5 For example, we may produce a 3 × 3 symmetric matrix thus ﻿Many problems in Artificial Intelligence (AI) can be formulated as network search problems. The crudest algorithms for solving problems of this kind, the so called blind search algorithms, use the network’s connectivity information only. We are going to consider examples, applications and Prolog implementations of blind search algorithms in this chapter. Since implementing solutions of problems based on search usually involves code of some complexity, modularization will enhance clarity, code reusability and readibility. In preparation for these more complex tasks in this chapter, Prolog’s module system will be discussed in the next section. In some (mostly larger) applications there will be a need to use several input files for a Prolog project. We have met an example thereof already in Fig. 3.5 of [9, p. 85] where consult/1 was used as a directive to include in the database definitions of predicates from other than the top level source file. As a result, all predicates thus defined became visible to the user: had we wished to introduce some further predicates, we would have had to choose the names so as to avoid those already used. Clearly, there are situations where it is preferable to make available (that is, to export ) only those predicates to the outside world which will be used by other non-local predicates and to hide the rest. This can be achieved by the built-in predicates module/2 and use module/1 . As an illustrative example, consider the network in Fig. 2.1.1 The network connectivity in links.pl is defined by the predicate link/2 which uses the auxiliary predicate connect/2 (Fig. 2.2). The first line of links.pl is the module directive indicating that the module name is edges and that the predicate link/2 is to be exported. All other predicates defined in links.pl (here: connect/2) are local to the module and (normally) not visible outside this module. Suppose now that in some other source file, link/2 is used in the definition of some new predicate (Fig. 2.3). Then, the (visible) predicates from links.pl will be imported by means of the directive The new predicate thus defined may be used as usual: In our example, the predicate connect/2 will not be available for use (since it is local to the module edges that resides in links.pl). A local predicate may be accessed, however, by prefixing its name by the module name in the following fashion:3 Let us assume that for the network in Fig. 2.1 we want to find a path from the start node s to the goal node g. The search may be conducted by using the (associated) search tree shown in Fig. 2.4. It is seen that the search tree is infinite but highly repetitive. The start node s is at the root node (level 0). At level 1, all tree nodes are labelled by those network nodes which can be reached in one step from the start node. In general, a node labelled n in the tree at level _ has successor (or child ) nodes labelled s1, s2, . . . if the nodes s1, s2, . . . in the network can be reached in one step from node n. These successor nodes are said to be at level _ + 1. The node labelled n is said to be a parent of the nodes s1, s2, . . .. In Fig. 2.4, to avoid repetition, those parts of the tree which can be generated by expanding a node from some level above have been omitted. Some Further Terminology • The connections between the nodes in a network are called links. • The connections in a tree are called branches. • In a tree, a node is said to be the ancestor of another if there is a chain of branches (upwards) which connects the latter node to the former. In a tree, a node is said to be a descendant of another node if the latter is an ancestor of the former. In Fig. 2.5 we show, for later reference, the fully developed (and ’pruned’) search tree. It is obtained from Fig. 2.4 by arranging that in any chain of branches (corresponding to a path in the network) there should be no two nodes with the same label (implying that in the network no node be visited more than once). All information pertinent to the present problem is recorded thus in the file links.pl (Fig. 2.2) by link/2. Notice that the order in which child nodes are generated by link/2 will govern the development of the trees in Figs. 2.4 and 2.5: children of the same node are written down from left to right in the order as they would be obtained by backtracking; for example, the node labelled d at level 1 in Fig. 2.4 is expanded by (The same may be deduced, of course, by inspection from links.pl, Fig. 2.2.) link/2 will serve as input to the implementations of the search algorithms to be discussed next. The most concise and easy to remember illustration of Depth First is by the conduit model (Fig. 2.6). We start with the search tree in Fig. 2.5 which is assumed to be a network of pipes with inlet at the root node s. The tree is rotated by 90◦ counterclockwise and connected to a valve which is initially closed. The valve is then opened and the system is observed as it gets flooded under the influence of gravity. The order in which the nodes are wetted corresponds to Depth First. We may be tempted to use Prolog’s backtracking mechanism to furnish a solution by recursion; our attempt is shown in Fig. 2.7.4 However, it turns out that the implementation does not work due to cycling in the network. The query shown below illustrates the problems arising. We implement Depth First search incrementally using a new approach. The idea is keeping track of the nodes to be visited by means of a list, the so called list of open nodes, also called the agenda. This book–keeping measure will turn out to be amenable to generalization; in fact, it will be seen that the various search algorithms differ only in the way the agenda is updated. ﻿In this chapter we are going to discuss graph search algorithms and applications thereof for finding a minimum cost path from a start node to the goal node. The network search problem in Sect. 2.2 (Fig. 2.1) was devoid of any cost information. Let us now assume that the costs to traverse the edges of the graph in Fig. 2.1 are as indicated in Fig. 3.1. There are two possible interpretations of the figures in Fig. 3.1: they can be thought of as costs of edge traversal or, alternatively, as edge lengths. (We prefer the latter interpretation in which case, of course, Fig. 3.1 is not to scale.) The task is to determine a minimum length path connecting s and g, or, more generally, minimum length paths connecting any two nodes. The algorithms considered in this chapter assume the knowledge of an heuristic distance measure, H, between nodes. Values of H for the network in Fig. 3.1 are shown in Table 3.1. They are taken to be the estimated straight line distances between nodes and may be obtained by drawing the network in Fig. 3.1 to scale and taking measurements. Three algorithms will be introduced here: the A–Algorithm, Iterative Deepening A∗ and Iterative Deepening A∗–_. An estimated overall cost measure, calculated by the heuristic evaluation function F, will be attached to every path; it is represented as where G is the actual cost incurred thus far by travelling from the start node to the current node and H, the heuristic, is the estimated cost of getting from the current node to the goal node. Assume, for example, that in the network shown in Fig. 3.1 we start in d and want to end up in c. Equation (3.1) then reads for the path d → s → a (with obvious notation) as follows We know from Chap. 2 that for blind search algorithms the updating of the agenda is crucial: Breadth First comes about by appending the list of extended paths to the list of open paths; Depth First requires these lists to be concatenated the other way round. For the A–Algorithm, the updating of the agenda is equally important. The new agenda is obtained from the old one in the steps 1 _ and 2 _ below. 1 _ Extend the head of the old agenda to get a list of successor paths. An intermediate, ‘working’ list will be formed by appending the tail of the old agenda to this list. 2 _ The new agenda is obtained by sorting the paths in the working list from 1 _ in ascending order of their F–values. 3 _ The steps 1 _ and 2 _ are iterated until the path at the head of the agenda leads to the goal node. In the example shown in Fig. 3.2, the paths are prefixed by their respective F–values and postfixed by their respective G–values. Using this notation and the cost information, the example path in (3.2) is now denoted by 242 − [a, s, d] − 147. Notice that this path also features in Fig. 3.2. It can be shown (e.g. [23]) that if the heuristic H is admissible, i.e. it never overestimates the actual minimum distance travelled between two nodes, the A–Algorithm will deliver a minimum cost path if such a path exists.1In this case the A–Algorithm is referred to as an A∗–Algorithm and is termed admissible. (As the straight line distance is a minimum, the heuristic defined by Table 3.1 is admissible.) The predicate a search(+Start,+Goal,-PathFound) in asearches.pl implements the A–Algorithm. A few salient features of a search/3 will be discussed only; for details, the reader is referred to the source code which broadly follows the pattern of implementation of the blind search algorithms (Fig. 2.15, p. 65 and Fig. 2.20, p. 69). The implementation of the A–Algorithm in asearches.pl uses the built-in predicate keysort/2 to implement step 2 _ (see inset on p. 108). The module invoking a search/3 should have defined (or imported) the following predicates. • The connectivity predicate link/2 . For the network search problem, this is imported from links.pl (Fig. 2.2, p. 49). • The estimated cost defined by e cost/3 . For the network search problem, this is defined in graph a.pl by with dist/3 essentially implementing Table 3.1, The actual edge costs defined by edge cost/3 . For the network search problem, this is defined in graph a.pl by Application of the A–Algorithm to a more substantial example in Sect. 3.2 will reveal that the A–Algorithm may fail due to excessive memory requirements.2 Clearly, there is scope for improvement. In the mid 1980s, a new algorithm was conceived by Korf [20] combining the idea of Iterative Deepening (Sect. 2.6) with a heuristic evaluation function; the resulting algorithm is known as Iterative Deepening A∗ (IDA∗).3 The underlying idea is as follows. • Use Depth First as the ‘core’ of the algorithm. • Convert the core into a kind of Bounded Depth First Search with the bound (the horizon) now not being imposed on the length of the paths but on their F-values. • Finally, imbed this ‘modified’ Bounded Depth First Search into a framework which repeatedly invokes it with a sequence of increasing bounds. The corresponding sequence of bounds in Iterative Deepening was defined as a sequence of multiples of some constant increment; a unit increment in the model implementation. The approach here is more sophisticated. Now, in any given phase of the iteration, the next value of the bound is obtained as the minimum of the F-values of all those paths which had to be ignored in the present phase. This approach ensures that in the new iteration cycle the least number of paths is extended. The pseudocode of IDA∗ won’t be given here; it should be possible to reconstruct it from the above informal description. It can be shown that IDA∗ is admissible under the same assumptions as A∗. The so-called _–admissible version of IDA∗ (IDA∗–_) is a generalization of IDA∗. It is obtained by extending the F-horizon to ﻿Whereas the problems considered thus far were taken from Artificial Intelligence, we are going now to apply Prolog to problems in text processing. The present chapter is in three parts. First, the Prolog implementation is described of a tool for removing from a file sections of text situated between marker strings. (The tool is therefore a primitive static program slicer; [32] and [12].) This tool then is used in a practical context for removing sample solutions from the LATEX source code of a solved exam script. It is also shown in this context how SWI-Prolog code can be embedded into a Linux shell script. The second part addresses the question of how Prolog can be used to generate LATEX code for drawing parametric curves. Some new features of Prolog will thereby also be introduced. The final part comprises a sequence of solved Prolog exercises, implementing a tool for drawing families of parametric curves in LATEX. The exercises are of increasing complexity and finally describe how SWI-Prolog can interact with Linux through a shell script. I use LATEX on Linux for preparing examination papers. This is done in the following steps. 1. Create a LATEX source file in a text editor. 2. Translate the LATEX file into a a DVI file. 3. Translate the DVI file into a PDF file. 4. View the PDF file. These steps are performed for exam.tex by running the Linux commands in Fig. 4.1.1 Upon execution of the last line in Fig. 4.1, a new window will pop up and the exam paper may be viewed. External examiners require examination papers with model answers. I create therefore a PDF file with model solutions in the first instance where answers are appended to each subquestion. The answers are placed between some marker strings enabling me eventually to locate and remove all text between them when creating the final LATEX source leading to the printed PDF for students. It is this text removal process which is automated by the Prolog implementation to be discussed here. Write a predicate sieve(+Infile,-Outfile,+Startmarker,+Endmarker) of arity 4 for removing all text in the file named in Infile in between all occurrences of lines starting with text in Startmarker and those starting with text in Endmarker. The result should be saved in the file named in Outfile . Outfile is without marker lines. If Outfile already exists, its old version should be overwritten, if it does not exist, it should be newly created. The file shown in Fig. 4.2 is an example of Infile with the marker phrases ‘water st’ and ‘water e’, say. (The file comprises a random collection of geographical names.) After the Prolog query the file without_waters will have been created. This is shown in Fig. 4.3. The main predicate sieve/4 is defined in terms of sieve/2 , both are shown in (P-4.1). The predicates get line/1 (and its auxiliary get line/2 ), switch off/1 and switch on/1 are defined in (P-4.2). For the SWI-Prolog built-ins atom chars/2 and atom codes/2 , the reader is referred respectively to pages 126 and 19 of [9]. Noteworthy are three more built-in predicates used here: the standard Prolog predicates see/1 , seen/0 (respectively for directing the input stream to a file and redirecting it) and get char/1 for reading a character; the example below illustrates their use by reading the first three characters of the file with_waters in Fig. 4.2. The predicate get line/1 in (P-4.2) is defined in terms of get line/2 by the accumulator technique. It reads into its argument the next line from the input stream. Example: The following observations apply. 1. It is seen from the above query that a line read by get line/1 is represented as a list of the characters it is composed of. 2. By definition the last character of each line in a file is the new line character ‘\n’. That explains the line break seen in the above query. 3. Finally (not demonstrated here), each file ends with the end-of-file marker ‘end_of_file’. The one-entry list [end_of_file] is deemed to be the last line of every file by the definition in (P-4.2). • The switches switch off/0 and switch on/0 are used, writing respectively switch(off) and switch(on) in the Prolog database, respectively for removal and retention of lines from the input file. • The main predicates are sieve/4 and sieve/2 in (P-4.1), the latter defined by recursion and called by the former. sieve/4 : this is the top level predicate. 1. Line 2 opens the input file. 2. The goals in lines 3-4 in (P-4.1) make sure that the earlier version of the output file (if there is such a file) is deleted. 3. In line 5, the new output stream is opened via append/1 3. 4. In line 6, the switch is set to the position (‘off’), anticipating that initially lines will be retained. 5. In line 7, sieve/2 is invoked and processing is carried out. 6. Lines 8 and 9 close respectively output and input. sieve/2 : this is called from sieve/4 . 1. Lines 14 and 18 contain the most interesting feature of this predicate: append/3 is used in them for pattern matching. For example, the goal succeeds if the initial segment of the list Line is Start_List. 2. atom chars/2 is used in sieve/2 to disassemble the start and end markers into lists in preparation for pattern matching. 3. Notice that the built-in predicate atom codes/2 can be used in two roles as the interactive session below demonstrates. In line 16 of (P-4.1), atom codes/2 is used in its first role, i.e. to convert a list of characters to an atom. This atom is the current line, it is written to the output file. 4. Recursion is stopped in sieve/2 (and control is returned to line 8 of sieve/4 ) when the end-of-file marker is read (line 15). Imbed the Prolog implementation from Sect. 4.1.3 into a Linux shell script for providing the same functionality as the predicate sieve/4 does. The application obtained thereby will run without explicitly having to use the SWI-Prolog system. The intended behaviour of the script is illustrated in Fig. 4.4. ﻿We saw in Chapter 11 that the Field Values Table (or Tables, plural) will always be in memory at run time, at least to a first approximation. And we saw in Chapter 12 how to use file factoring to reduce the space requirements for the Record Reconstruction Table(s) as well; basically, what we do is decompose the original file—at least conceptually—so that we wind up with one “large” Record Reconstruction Table and several “small” ones. And the small Record Reconstruction Tables too will always be in memory at run time (again to a first approximation). So we’re left with the large Record Reconstruction Table on disk. That large table can’t be compressed any further, more or less by definition; in other words, if we regard its contents just as simple bit strings, then those bit strings are essentially random sequences of zeros and ones.1 The techniques discussed in this chapter and the next are specifically aimed at getting the best possible performance out of that large table, despite the fact that it’s necessarily disk-resident. Before I go any further, I should make it clear that, although I call it “large,” the table we’re dealing with—in fact, the entire TR data representation, including the Field Values Table(s) and all of the corresponding Record Reconstruction Tables—is still likely to be far smaller than a conventional direct-image representation. Actual experiments have shown that a reduction of five to one is quite typical (if anything, that estimate is probably on the low side). And that’s just for the raw data; when indexes and other auxiliary structures are taken into account, the direct-image space requirement can increase by another five-to-one ratio, possibly even more.2 However, when I need to appeal to such matters later in this chapter, I’ll stick, conservatively, to the five-to-one figure.  Anyway, to remind you from Chapter 11, the problem with the large table is that the zigzags in that table, even if their starting points are physically contiguous (as indeed they are, because the table is stored column-wise), quickly splay out to essentially random positions “all over the disk,” with the consequence that we might have to do a separate seek for every point after the starting point every time we chase such a zigzag. File banding, or just banding for short, is a technique for addressing this problem. Here in outline is how it works: ■■Starting with a given user-level relation and hence a corresponding file, we sort that file into order based on values of some characteristic field (or field combination; for simplicity, I’ll assume throughout what follows that we’re always dealing with a single characteristic field, barring explicit statements to the contrary). ■■Next, we decompose that sorted file horizontally3 into two or more subfiles of approximately equal size. Each subfile is smaller than the original file in the sense that it has fewer records (usually far fewer) than the original file did. Note: The official term for “horizontal subfiles” is bands, and I’ll favor this latter term in subsequent sections. You can think of those bands or horizontal subfiles as partitions, if you like; note, however, that specifics of the partitioning in question are determined primarily by physical space requirements and only secondarily by values of the characteristic field. We’ll see some implications of this state of affairs in the next section (in particular, we’ll see that a given characteristic field value might appear in more than one band, something that couldn’t happen if the partitioning were done purely on the basis of values of that field). ■■We then represent each of those subfiles or bands by its own Field Values Table and its own Record Reconstruction Table. Because the bands are smaller than the original file, those Field Values and Record Reconstruction Tables too are smaller than their counterparts would have been for the original file. In fact, we choose the band size such that any given band can fit into memory in its entirety at run time, and we lay the bands out on the disk in such a way as to allow any given band to be streamed into memory as and when it’s needed. (When I say the entire band fits into memory, what I’m mainly talking about is the Record Reconstruction Table for the given band, of course. If the Record Reconstruction Table for a given band can be entirely contained in memory, then all of the zigzags within that table will also be entirely contained in memory a fortiori, and—insofar as that particular table is concerned, at least—the splay problem thus won’t arise.) Note: Please understand that the foregoing account is deliberately somewhat simplified; I’ll come back and explain later (in Section 13.4) how banding is really done. However, the foregoing explanation is accurate enough to serve as a basis for discussions prior to that section. The structure of the chapter is as follows. Following this introductory section, I’ll explain the basic idea of banding by means of a simple example in Section 13.2; then I’ll elaborate on and generalize from that example in Section 13.3, and introduce the important idea of controlled redundancy. As already mentioned, in Section 13.4 I’ll build on the ideas of previous sections to show how banding is really done. Finally, in Section 13.5, I’ll discuss the concept of controlled redundancy in more detail.  In the interests of “user-friendliness,” I’ll continue to work with the familiar parts example (or an extended version of that example, rather). Assume again—as in Chapter 12, Section 12.5—that we’ve factored the parts file into large and small files that look like this:  Sample values for these files are shown in Fig. 13.1 (an extended version of Fig. 12.2 from Chapter 12). Note: Of course, it’s the large file we’re interested in here, not the small one. In the figure, of course, that file is hardly very “large” (obviously, since it has just nine records); however, don’t lose sight of the fact that if we’re really supposed to be building on the example from Chapter 12, then the file is really supposed to have some ten million records. What’s more, fields in that file—with the obvious exception of the introduced artificial identifier CC#—are supposed to be of high cardinality, meaning each such field has around ten million distinct values as well; in fact, the data in the large file isn’t supposed to display any “statistical clumpiness” at all.$$$﻿We saw in Chapter 11 that the Field Values Table (or Tables, plural) will always be in memory at run time, at least to a first approximation. And we saw in Chapter 12 how to use file factoring to reduce the space requirements for the Record Reconstruction Table(s) as well; basically, what we do is decompose the original file—at least conceptually—so that we wind up with one “large” Record Reconstruction Table and several “small” ones. And the small Record Reconstruction Tables too will always be in memory at run time (again to a first approximation). So we’re left with the large Record Reconstruction Table on disk. That large table can’t be compressed any further, more or less by definition; in other words, if we regard its contents just as simple bit strings, then those bit strings are essentially random sequences of zeros and ones.1 The techniques discussed in this chapter and the next are specifically aimed at getting the best possible performance out of that large table, despite the fact that it’s necessarily disk-resident. Before I go any further, I should make it clear that, although I call it “large,” the table we’re dealing with—in fact, the entire TR data representation, including the Field Values Table(s) and all of the corresponding Record Reconstruction Tables—is still likely to be far smaller than a conventional direct-image representation. Actual experiments have shown that a reduction of five to one is quite typical (if anything, that estimate is probably on the low side). And that’s just for the raw data; when indexes and other auxiliary structures are taken into account, the direct-image space requirement can increase by another five-to-one ratio, possibly even more.2 However, when I need to appeal to such matters later in this chapter, I’ll stick, conservatively, to the five-to-one figure.  Anyway, to remind you from Chapter 11, the problem with the large table is that the zigzags in that table, even if their starting points are physically contiguous (as indeed they are, because the table is stored column-wise), quickly splay out to essentially random positions “all over the disk,” with the consequence that we might have to do a separate seek for every point after the starting point every time we chase such a zigzag. File banding, or just banding for short, is a technique for addressing this problem. Here in outline is how it works: ■■Starting with a given user-level relation and hence a corresponding file, we sort that file into order based on values of some characteristic field (or field combination; for simplicity, I’ll assume throughout what follows that we’re always dealing with a single characteristic field, barring explicit statements to the contrary). ■■Next, we decompose that sorted file horizontally3 into two or more subfiles of approximately equal size. Each subfile is smaller than the original file in the sense that it has fewer records (usually far fewer) than the original file did. Note: The official term for “horizontal subfiles” is bands, and I’ll favor this latter term in subsequent sections. You can think of those bands or horizontal subfiles as partitions, if you like; note, however, that specifics of the partitioning in question are determined primarily by physical space requirements and only secondarily by values of the characteristic field. We’ll see some implications of this state of affairs in the next section (in particular, we’ll see that a given characteristic field value might appear in more than one band, something that couldn’t happen if the partitioning were done purely on the basis of values of that field). ■■We then represent each of those subfiles or bands by its own Field Values Table and its own Record Reconstruction Table. Because the bands are smaller than the original file, those Field Values and Record Reconstruction Tables too are smaller than their counterparts would have been for the original file. In fact, we choose the band size such that any given band can fit into memory in its entirety at run time, and we lay the bands out on the disk in such a way as to allow any given band to be streamed into memory as and when it’s needed. (When I say the entire band fits into memory, what I’m mainly talking about is the Record Reconstruction Table for the given band, of course. If the Record Reconstruction Table for a given band can be entirely contained in memory, then all of the zigzags within that table will also be entirely contained in memory a fortiori, and—insofar as that particular table is concerned, at least—the splay problem thus won’t arise.) Note: Please understand that the foregoing account is deliberately somewhat simplified; I’ll come back and explain later (in Section 13.4) how banding is really done. However, the foregoing explanation is accurate enough to serve as a basis for discussions prior to that section. The structure of the chapter is as follows. Following this introductory section, I’ll explain the basic idea of banding by means of a simple example in Section 13.2; then I’ll elaborate on and generalize from that example in Section 13.3, and introduce the important idea of controlled redundancy. As already mentioned, in Section 13.4 I’ll build on the ideas of previous sections to show how banding is really done. Finally, in Section 13.5, I’ll discuss the concept of controlled redundancy in more detail.  In the interests of “user-friendliness,” I’ll continue to work with the familiar parts example (or an extended version of that example, rather). Assume again—as in Chapter 12, Section 12.5—that we’ve factored the parts file into large and small files that look like this:  Sample values for these files are shown in Fig. 13.1 (an extended version of Fig. 12.2 from Chapter 12). Note: Of course, it’s the large file we’re interested in here, not the small one. In the figure, of course, that file is hardly very “large” (obviously, since it has just nine records); however, don’t lose sight of the fact that if we’re really supposed to be building on the example from Chapter 12, then the file is really supposed to have some ten million records. What’s more, fields in that file—with the obvious exception of the introduced artificial identifier CC#—are supposed to be of high cardinality, meaning each such field has around ten million distinct values as well; in fact, the data in the large file isn’t supposed to display any “statistical clumpiness” at all.
EN26	0	﻿Since its creation in 1987 Perl has become one of the most widely used programming languages. One measure of this is the frequency with which various languages are mentioned in job adverts. The site www.indeed.com monitors trends: in 2010 it shows that the only languages receiving more mentions on job sites are C and its offshoots C++ and C#, Java, and JavaScript. Perl is a general-purpose programming language, but it has outstanding strengths in processing text files: often one can easily achieve in a line or two of Perl code some text-processing task that might take half a page of C or Java. In consequence, Perl is heavily used for computer-centre system admin, and for Web development – Web pages are HTML text files. Another factor in the popularity of Perl is simply that many programmers find it fun to work with. Compared with Perl, other leading languages can feel worthy but tedious. Perl is a language in which it is easy to get started, but – because it offers handy ways to do very many different things – it takes a long time before anyone finishes learning Perl (if they do ever finish). One standard reference, Steven Holzner’s Perl Black Book (second edn, Paraglyph Press, 2001) is about 1300 dense pages long. So, for the beginner, it is important to focus on the core of the language, and avoid being distracted by all the other features which are there, but are not essential in the early stages. This book helps the reader to do that. It covers everything he or she needs to know in order to write successful Perl programs and grow in confidence with the language, while shielding him or her from confusing inessentials.1 Later chapters contain pointers towards various topics which have deliberately been omitted here. When the core of the language has been thoroughly mastered, that will be soon enough to begin broadening one’s knowledge. Many productive Perl programmers have gaps in their awareness of the full range of language features. The book is intended for beginners: readers who are new to Perl, and probably new to computer programming. The book takes care to spell out concepts that would be very familiar to anyone who already has experience of programming in some other language. However, there will be readers who use this book to begin learning Perl, but who have worked with another language in the past. For the benefit of that group, I include occasional brief passages drawing attention to features of Perl that could be confusing to someone with a background in another language. Programming neophytes can skim over those passages.  The reader I had in mind as I was writing this book was a reader much like myself: someone who is not particularly interested in the fine points of programming languages for their own sake, but who wants to use a programming language because he has work he wants to get done, and programming is a necessary step towards doing it. As it happens, I am a linguist by training, and much of my own working life is spent studying patterns in the way the English language is used in everyday talk. For this I need to write software to analyse files of transcribed tape-recordings, and Perl is a very suitable language to use for this. Often I am well aware that the program I have written is not the most elegant possible solution to some task at hand, but so long as it works correctly I really don’t care. If some geeky type offered to show me how I could eliminate several lines of code, or make my program run twice as fast, by exploiting some little-known feature of the language which would yield a program delivering exactly the same results, I would not be very interested. Too many computing books are written by geeks who lose sight of the fact that, for the rest of us, computers are tools to get work done rather than ends in themselves. Making programs short is good if it makes them easier to grasp and hence easier to get right; but if brevity is achieved at the cost of obscurity, it is bad. As for speed: computer programs run so fast that, for most of us, speeding them up further would be pointless. (For every second of time my programs take to run, I probably spend a day thinking about the results they produce.) That does not mean that, in writing this book, I would have been justified in focusing only on those particular elements of Perl which happen to be useful in my own work and ignoring the rest – certainly not. Readers will have their own tasks for which they want to write software, which will often be very different from my tasks and will sometimes make heavy use of aspects of Perl that I rarely exploit. I aim to cover those aspects, as well as the ones which I use frequently. But it does mean that the book is oriented towards Perl programming as a practical tool – rather than as a labyrinth of fascinating intellectual arcana. If, after working through this book, you decide to make serious use of Perl, sooner or later you will need to consult some larger-scale Perl book – one organized more as a reference manual than a teaching introduction. This short book cannot pretend to cover the reference function, but there is a wide choice of books which do. (And of course there are plenty of online reference sources.) Many Perl users will not need to go all the way to Steven Holzner’s 1300-pager quoted above. The manual which I use constantly is a shorter one by the same author, Perl Core Language Little Black Book (second edn, Paraglyph Press, 2004) – I find Holzner’s approach particularly well suited to my own style of learning, but readers whose learning styles differ might find that other titles suit them better. Because the present book deliberately limits the aspects of Perl which it covers, it is important that readers should not fall into the trap of thinking “Doesn’t Perl have a such-and-such function, then? – that sounds like an awkward gap to have to work round”. Whatever such-and-such may be, very likely Perl has got it, but it is one of the things which this book has chosen not to cover. ﻿For the purposes of this textbook, I shall assume that you have access to a computer system on which Perl is available, and that you know how to log on to the system and get to a point where the system is displaying a prompt and inviting you to enter a command. Perl is free, and versions are available for all the usual operating systems, so if you are working in a multi-user environment such as a university computer centre then Perl is almost sure to be on your system already. (It would take us too far out of our way to go through the details of installing Perl on a home computer which does not already have it; though, if the home computer is a Mac running OS X, it will already have Perl – available from the Terminal utility under Applications  Utilities.) Assuming, then, that you have access to Perl, let us get started by creating and running a very simple program.2 Adding two and two is perhaps as simple as it gets. This could be a very short Perl program indeed, but I’ll offer a slightly longer one which illustrates some basics of the language. First, create a file with the following contents. Use a text editor to create it, not a word-processing application such as Word – files created via WP apps contain a lot of extra, hidden material apart from the wording typed by the user and displayed on the screen, but we need a file containing just the characters shown below and no others.  Save it under some suitable name – twoandtwo.pl is as good a name as any. The .pl extension is optional – Perl itself does not care about the format of filenames, and it would respond to the program just the same if you called it simply twoandtwo – but some operating systems want to see filename extensions in some circumstances, so it is probably sensible to get in the habit of including .pl in the names of your Perl programs. Your twoandtwo.pl file will contain just what is shown above. But later in this book, when we look at more extended examples of Perl code I shall give them a label in brackets and number the lines, like this:  These labels will be purely for convenience in discussing the code, for instance I shall write “line 1.3” to identify the line print $b. The labels are not part of what you will type to create a program. However, when your programs grow longer you may find it helpful to create them using an editor which shows line-numbers; the error messages generated by the Perl interpreter will use line numbers to identify places where it finds problems.  In (1), the symbols $a and $b are variables – names for pigeonholes containing values (in this case, numbers). Line 1.1 means “assign the value 2 to the variable $a”. Line 1.2 means “assign the result of adding the value of $a to itself to the variable $b”. Line 1.3 means “display the value of $b”. Note that each instruction (the usual word is statement) ends in a semicolon. To run the program, enter the command  to which the system will respond (I’ll show system responses in italics) with  Actually, if your system prompt is, say, %, what you see will be  – since nothing in the twoandtwo.pl program has told the system to output a newline after displaying the result and before displaying the next prompt. For that matter, nothing in our little program has told the system how much precision to include in displaying the answer; rather than responding with 4, some systems might respond with 4.00000000000000 (which is a more precise way of saying the same thing). In due course we shall see how to include extra material in a program to deal with issues like these. For now, the point is that the job in hand has been correctly done.  If you have typed the code exactly as shown and Perl does not respond correctly (or at all) when you try running it, various system-dependent problems may be to blame. I assume that, where you are working, there will be someone responsible for telling you what is needed to run Perl on your local system. But meanwhile, I can offer two suggestions. It may be that your program needs to tell the system where the Perl interpreter is located (this is likely if you are seeing an error message suggesting that the command perl is not recognized). In that case it is worth trying the following. Include as the first line of your program this “magic line”:3  This will not be the right “magic line” for every system, but for many systems it will be. Secondly, if Perl appears to run without generating error messages, but outputs no result, or outputs material suggesting that it stopped reading your program before the end, it may be that your editor is supplying the wrong newline symbols – so that the sequence of lines looks to the system like one long line. That will often lead to problems; for instance, if the first line of your program is the above “magic line”, but Perl sees your whole program as one long line, then nothing will happen when you run it, because the Perl interpreter will only begin to operate on the line following the “magic line”. Set your editor to use Unix (decimal 10) newlines. If neither of these solutions works, then, sorry, you really will need to find that computer-support staff member to tell you how to run Perl on the particular system you are working at!  Let’s now go back to the contents of program (1). One point which may have surprised you about our first program is the dollar signs in the variable names $a and $b. Why not simply name our variables a and b? In many programming languages, these latter names would be fine, but in Perl they are not. One of the rules of Perl is that any variable name must begin with a special character identifying what kind of entity it is, and for individual variables – names for single separate pigeonholes, as opposed to names for whole sets of pigeonholes – the identifying character is a dollar sign. ﻿Programming, in any language, involves creating named entities within the machine and manipulating them – using their values to calculate the value for a new entity, changing the values of existing entities, and so forth. Some languages recognize many different kinds of entity, and require the programmer to be very explicit and meticulous about “declaring” what entities he will use and what kind each one will be before anything is actually done with them.4 In C, for instance, if a variable represents a number, one must say what kind of number – whether an integer (a whole number) or a “floating-point number” (what in everyday life we call a decimal), and if the latter then to what degree of precision it is recorded. (Mathematically, a decimal may have any number of digits after the decimal point, but computers have to use approximations which round numbers off after some specific number of digits.) Perl is very free and easy about these things. It recognizes essentially just three types of entity: individual items, and two kinds of sets of items – arrays, and hashes. Individual entities are called scalars (for mathematical reasons which we can afford to ignore here – just think of “scalar” as Perl-ese for an individual data item); a scalar can have any kind of value – it can be a whole number, a decimal, a single character, a string of characters (for instance, an English word or sentence) … We have already seen that variable names representing scalars (the only variables we shall be considering for the time being) begin with the $ symbol; for arrays and hashes, which we shall discuss in chapters 12 and 17, the corresponding symbols are @ and % respectively.  Furthermore, Perl does not require us to declare entity names before using them. In the mini-program (1), the scalars $a and $b came into existence when they were assigned values; we gave no prior notice that these variable names were going to be used. In program (1), the variable $b ended up with the value 4. But, if we had added a further line:  then $b would have ceased to stand for a number and begun to stand for a character-string – both are scalars, so Perl is perfectly willing to switch between these different kinds of value. That does not mean that it is a good idea to do this in practice; as a programmer you will need to bear in mind what your different variable names are intended to represent, which might be hard to do if some of them switch between numerical and alphabetic values. But the fact that one can do this makes the point that Perl does not force us to be finicky about housekeeping details. Indeed, it is even legal to use a variable’s value before we have given it a value. If line 1.2 of (1) were changed to $b = $a + $c, then $b would be given the sum of 2 plus the previously-unmentioned scalar $c. Because $c has not been given a value by the programmer, its value will be taken as zero (so $b will end up with the value 2). Relying on Perl to initialize our variables in this way is definitely a bad idea – even if we need a particular variable to have the initial value zero, it is much less confusing in the long run to get into the habit of always saying so explicitly. But Perl will not force us to give our variables values before we use them. Because this free-and-easy programming ethos makes it tempting to fall into bad habits, Perl gives us a way of reminding ourselves to avoid them. We ran program (1) with the command:  The perl command can be modified by various options beginning with hyphens, one of which is -w for “give warnings”. If we ran the program using the command:  then, when Perl encounters the line $b = $a + $c in which $c is used without having been assigned a value, it will obey the instruction but will also print out a warning:  If a skilled programmer gets that warning, it is very likely to be because he thinks he has given $c a value but in fact has omitted to do so. And perl -w gives other warnings about things in our code which, while legal, might well be symptoms of programming errors. It is a good idea routinely to use perl -w to run your programs, and to modify the programs in response to warning messages until the warnings no longer appear – even if the programs seem to be giving the right results.  In program (1) we saw the operator +, which as you would expect takes a pair of numerical values and gives their sum. Likewise - is used as a minus sign. Some further operators (not a complete list, but the ones you are most likely to need) include: * multiplication / division ** exponentiation: 2 ** 3 means 23, i.e. eight These operators apply to numerical values, but others apply to character-strings. Notably, the full stop . represents concatenation (making one string out of two):  (Beware of possible confusion here. Some programming languages make the plus sign do double duty, to represent concatenation of strings as well as addition of numbers, but in Perl the plus sign is used only for numerical values.) Another string operator is x (the letter x), which is used to concatenate a string with itself a given number of times: "a" x 6 is equivalent to "aaaaaa", "pom" x 3 is equivalent to "pompompom". (And "pom" x 0 would yield the empty string – the length-zero string containing no characters – which is more straightforwardly specified as "".) Note, by the way, that for Perl a single character is just a string of length one – there is no difference, as there is for instance in C, between "a" and 'a', these are equivalent ways of representing the length-one string containing just the character a. However, single and double quotation marks are not always equivalent. Perl uses backslash as an escape character to create codes for string elements which would be awkward to type: for instance, \n represents a newline character, and \t a tab. Between double quotation marks these sequences are interpreted as codes: ﻿We have seen the word if used to control which instruction is executed next. Commonly, we want to do one thing in one case and another thing in a different case. An if can be followed by an elsif (or more than one elsif), with an else at the end to catch any remaining possibilities:  When any one of the tests is passed, the remaining tests are ignored; if $price is 200, then since 200 100 Perl will print It's expensive, and the message in 4.7 will not be printed even though it is also true that 200 > 0. Curly brackets are used to keep together the block of code to be executed if a test is passed. Notice that (unlike in some programming languages) even if the block contains just a single line of code, that line must still have curly brackets round it. The last statement before the } does not actually have to end in a semicolon, but it is sensible to include one anyway. We might want to modify our code by adding further statements, in which case it would be easy to overlook the need to add a missing semicolon.  Not everyone sets out the curly brackets on separate lines, as I did in (4) above. Within reason, Perl does not care where in a program we put whitespace (spaces, tabs, and newline characters). Obviously we cannot put a space in the middle of a number – 56237 cannot be written 56 237, or Perl would have no way to tell that it was all one number 8 – and likewise putting a space in the middle of a string within quotation marks turns it into a different string. But we can set the program out on the page however we please: around the basic elements such as numbers, strings, variable names, and brackets of different types, Perl will ignore extra whitespace. Perl will even supply implied spacing in many cases where elements are run together – thus ++ $a can alternatively be written ++$a. Because Perl does not enforce layout conventions (as some languages do), you need to choose some system and use it consistently – so that you can grasp the overall structure of your program listings at a glance. The main question is about how to indent blocks; different people use different conventions. First, you need to decide how much space you are going to use for one level of indentation (common choices are one tab, or two spaces). But then, where exactly should the indents go? Perl manuals often put the opening curly bracket on the line which introduces it, indent the contents of the block, and then place the closing curly bracket level with the beginning of that first line:  This takes fewer lines than other conventions, but it is not particularly easy to read, and it is perhaps illogical in placing the pair of brackets at unrelated positions. Alternatively, one can give both curly brackets lines of their own – in which case they either both line up under the start of the introducing line, or are both indented to align with their contents:  Whichever convention you choose, if you apply it consistently you can catch and correct programming errors as you type. You may have a block which is indented within a block that is itself indented within a top-level block. When you type what you thought was the final }, if it doesn’t align properly with the item which it ought to line up with in the first line, then something has gone wrong – perhaps one of your opening brackets has not been given a closing partner?  As for which of the three styles you choose, that is entirely up to you. According to Thomas Plum, a survey of programmers working with the similar language C found a slight majority favouring the last of the three conventions.9 That is the style used in this book. Indenting consistently also has an advantage when, inevitably, one’s program as first written turns out not to run correctly. A common debugging technique is to insert instructions to print out the values of particular variables at key points, so that one can check whether their values are as expected. Once the bugs are found and eliminated, we naturally want to eliminate these diagnostic lines too – we don’t want our program spewing out a lot of irrelevancies when it is running correctly. My practice is to write diagnostic lines unindented, so that they stand out visually in the middle of an indented block, making them easy to locate and delete.  The reason to adopt a consistent style for program layout is to make it easier for a human programmer to understand what is going on within a sea of program code – the computer itself does not care about the layout. Another aid to human understanding is comments: explanatory notes written by the programmer to himself (or to those who come after him and have to maintain his code) which the machine ignores. In Perl, comments begin with the hash character. A comment can be:  or it can be added to a line to the right of code intended for the computer:  Either way, everything from the hash symbol to the end of the line is ignored by the machine.  Earlier, we saw that Perl has various “operators” represented by mathematical-type symbols. Sometimes these are the same symbols used in familiar school maths, such as + for addition and - for subtraction; sometimes they are slightly different symbols adapted to the constraints of computer keyboards, such as * for multiplication and ** for raising to a power; and sometimes the symbols represent operations that we do not usually come across in maths lessons, e.g. “.” for concatenation.  Perl has many more built-in functions that could conveniently be represented by special symbols, though.10 Most are represented by alphabetic codes. For instance, taking the square root of a number is a standard arithmetic operation, but the usual mathematical symbol, √, is nothing like any character in the ASCII character-set, so instead Perl represents it as sqrt.  ﻿Sometimes we want to repeat an action, perhaps with variations. One way to do this is with the word for. Suppose we want to print out a hundred lines containing the messages:  Here is a code snippet which does that:  The brackets following for contain: a variable created for the purpose of this for loop and given an initial value; a condition for repeating the loop; and an action to be executed after each pass. The variable $i begins with the value 1, ++$i increments it by one on each pass, and the instruction within the curly brackets is executed for each value of $i until $i reaches 101, when control moves on to whatever follows the closing curly bracket. We saw earlier that, within double quotation marks, a symbol like \n is translated into what it stands for (newline, in this case), rather than being taken literally as the two characters \ followed by n. Similarly, a variable name such as $i is translated into its current value; the lines displayed by the code above read e.g. Next number is 3, not Next number is $i. If you really wanted the latter, you would need to “escape” the dollar sign:  The little examples in earlier chapters often ended with statements such as  In practice, it would usually be far preferable to write  so that the result appears on a line of its own, rather than jammed together with the next system prompt. Within the output of the above code snippet, 1 is not a “next” number but the first number. So we might want the message on the first line to read differently. By now, we know various ways to achieve that. Here are two – a straightforward, plodding way, and a more concise way:  or (quicker to type, though less clear when you come back to it weeks later):  Another way to set up a repeating loop is the while construction. Here is another code snippet which achieves the same as the two we have just looked at:  Here, $i is incremented within the loop body, and control falls out of the loop after the pass in which $i begins with the value 99. The while condition reads $i < 100, not $i <= 100: within the curly brackets, $i is incremented before its value is displayed, so if <= had been used in the while line, the lines displayed would have reached 101. The while construction is often used for reading input lines in from a text file, so the next chapter will show us how that is done.  In general, a file you want to get data into your program from will not necessarily be in the same directory as the program itself; it may have to be located by a pathname which could be long and complicated. The structure of pathnames differs between operating systems; if you are working in a Unix environment, for instance, the pathname might be something like:  Whatever pathnames look like in your computing environment, to read data into a Perl program you have to begin by defining a convenient handle which the program will use to stand for that pathname. For instance, if your program will be using only one input file, you might choose the handle INFILE (it is usual to use capitals for filehandles). The code:  says that, from now until we hit a line close(INFILE), any reference to INFILE in the program will be reading in data from the annualRecords file specified in the pathname.  Having “opened” a file for input, we use the symbol <> to actually read a line in. Thus:  will read in a line from the annualRecords file and assign that string of characters as the value of $a. A line from a multi-line file will terminate in one or more line-end characters, and the identity of these may depend on the system which created the file (different operating systems use different line-end characters). Commonly, before doing anything else with the line we will want to convert it into an ordinary string by removing the line-end characters, and the built-in function chomp() does that. This is an example of a function whose main purpose is to change its argument rather than to return a value; chomp() does in fact return a value, namely the number of line-end characters found and removed, but programs will often ignore that value – they will say e.g. chomp($line), rather than saying e.g. $n = chomp($line), with follow-up code using the value of $n. (If no filehandle is specified, $a = <> will read in from the keyboard – the program will wait for the user to type a sequence of characters ending in a newline, and will assign that sequence to $a.11) Assuming that we are reading data from a file rather than from the keyboard, what we often want to do is to read in the whole of the input file, line by line, doing something or other with each successive line. An easy way to achieve that is like this:  The word while tests for the truth of a condition; in this case, it tests whether the assignment statement, and hence the expression <INFILE>, is true or false. So long as lines are being read in from the input file, <INFILE> counts as “true”, but when the file is exhausted <INFILE> will give the value “false”. Hence while ($a = <INFILE>) assigns each line of the input file in turn to $a, and ceases reading when there is nothing more to read. (It is a good idea then to include an explicit close(INFILE) statement, though that is not strictly necessary.) Our open … statement assumed that the annualRecords file was waiting ready to be opened at the place identified by the pathname. But, of course, that kind of assumption is liable to be confounded! Even supposing we copied the pathname accurately when we typed out the program, if that was a while ago then perhaps the annualRecords file has subsequently been moved, or even deleted. In practice it is virtually mandatory, whenever we try to open a file, to provide for the possibility that it does not get opened – normally, by using a die statement, which causes the program to terminate after printing a message about the problem encountered. A good way to code the open statement will be: ﻿Constructing a user interface (UI) is one of the most rewarding features of application development using Java; it is highly creative and gives the developer the opportunity to work very closely with users to develop the UI that suits their needs. In this chapter, we will find out how user interfaces are constructed. The themed application is referred to from time to time in previous chapters. It is not the intention to appraise the reader about all of the details of the themed application; rather, it is used to illustrate programming concepts in the context of a realistic application. However, we haven’t yet explained how the classes of the themed application are used in an actual application, although the purpose of using main methods to test classes is mentioned and illustrated in previous chapters where relevant or appropriate. In general, when classes and their methods associated with an application have been thoroughly tested, often with main methods specifically written for testing purposes, the process of constructing the UI can proceed. The UI is the component of an application that users use to interact with it in order to carry out the work provided by the business logic of the application in the form of objects that, collectively, provide a service to users. The objects that implement the logic of the application are the business objects of the application in that they meet the business requirements of the application as determined by analysis of the business domain under consideration. The notion that the business objects of an application provide a service to users means that we can regard the business objects as the server-side or server component of an application. Consider, for example, the classes of the themed application shown in Figure 3.1 on the next page. The classes shown in Figure 3.1 encapsulate the business requirements of a realistic application that is referred to in this guide as the ‘themed application’. The ‘has a’ and ‘is a’ associations shown in the figure imply that the class that represents the Media Store has a number of Member objects and Item objects of either the Dvd or Game type. Each member is provided with two virtual membership cards, one for recording the details about DVDs on loan (from the Media Store) and another for recording the details about games. Only the DvdMembershipCard class is defined in the version of the themed application shown in the figure; hence, the ‘has a’ link between this class and the Item class. For the purposes of the present discussion, the DVD membership card object implements methods to borrow and return DVDs. The state of each Member object, along with the graph of objects associated with Member – DvdMembershipCard and Dvd – are serialized to a data file. The classes shown in the figure comprise the server-side of the application. The various methods of the classes of the application were tested (by the author of this guide) with, in the first instance, a number of main methods that test the correctness of card transactions when taking items out on loan and returning them. When testing was complete, the next stage in the development of the application was to construct the UI. The classes associated with the UI comprise the client-side of the application. The client-side of the themed application is used – as we will see later in this chapter – to interact with the server-side. For example, there are buttons on the UI that call methods of the DvdMembershipCard class to borrow and return DVDs. Clearly, and rather obviously, there is a close coupling between the UI and the business objects with which it interacts. In other words, the server-side of the application is of no value without the client-side of the application and vice versa. In general, whilst the development and testing of the client and server sides of an application may proceed independently of one another, both sides of an application will eventually be brought together to comprise the complete application. Final testing will test that the UI operates correctly in its interaction with server-side objects. The combination of the server and client ‘sides’ of the themed application into a single application results in what is known, in this case, as a standalone application in the sense that it runs on one computer: i.e. a standalone application runs in a single address space using a single Java Virtual Machine. Whilst it can readily be argued that the standalone version of the themed application is not realistic in that it is not designed to run on a (computer) network, its purpose in this guide is to provide a source of examples that illustrate how a number of fundamental Java programming concepts can be applied in the context of an application that is appropriate for learners. In short, the development of standalone applications is relevant from a learning point of view. The combination of the client and server sides of the themed application (discussed in the previous section) suggests that we can regard any standalone application as comprising client and server components that interact with one another. Perhaps not surprisingly, such applications are labelled with the term client/server. Figure 3.2, shown on the next page, illustrates the architecture of a typical client/server application. Tier 1, the client tier, comprises the UI; tier 2, the server tier, comprises the business objects of the application; tier 3, the persistent data tier, comprises data storage components such as data files and database tables. In the case of the themed application, the third tier comprises a data file that stores an array of members of the Media Store. In a typical client/server application, the UI interacts with the server-side objects which, in turn, interact with the data tier. In other words, the UI interacts with the server-tier; the UI does not access the data tier directly. The double-headed arrow (in the figure) that implies that the client and server tiers are connected is, in the case of a standalone application, an indication that the client and server tiers are deployed in the same address space. In the case of a networked application, the connection between the client and server tiers will either be an intranet or the Internet. Now that we have placed user interfaces in the context of a client/server architecture, the next section moves on to find out how a UI is constructed.$$$﻿Constructing a user interface (UI) is one of the most rewarding features of application development using Java; it is highly creative and gives the developer the opportunity to work very closely with users to develop the UI that suits their needs. In this chapter, we will find out how user interfaces are constructed. The themed application is referred to from time to time in previous chapters. It is not the intention to appraise the reader about all of the details of the themed application; rather, it is used to illustrate programming concepts in the context of a realistic application. However, we haven’t yet explained how the classes of the themed application are used in an actual application, although the purpose of using main methods to test classes is mentioned and illustrated in previous chapters where relevant or appropriate. In general, when classes and their methods associated with an application have been thoroughly tested, often with main methods specifically written for testing purposes, the process of constructing the UI can proceed. The UI is the component of an application that users use to interact with it in order to carry out the work provided by the business logic of the application in the form of objects that, collectively, provide a service to users. The objects that implement the logic of the application are the business objects of the application in that they meet the business requirements of the application as determined by analysis of the business domain under consideration. The notion that the business objects of an application provide a service to users means that we can regard the business objects as the server-side or server component of an application. Consider, for example, the classes of the themed application shown in Figure 3.1 on the next page. The classes shown in Figure 3.1 encapsulate the business requirements of a realistic application that is referred to in this guide as the ‘themed application’. The ‘has a’ and ‘is a’ associations shown in the figure imply that the class that represents the Media Store has a number of Member objects and Item objects of either the Dvd or Game type. Each member is provided with two virtual membership cards, one for recording the details about DVDs on loan (from the Media Store) and another for recording the details about games. Only the DvdMembershipCard class is defined in the version of the themed application shown in the figure; hence, the ‘has a’ link between this class and the Item class. For the purposes of the present discussion, the DVD membership card object implements methods to borrow and return DVDs. The state of each Member object, along with the graph of objects associated with Member – DvdMembershipCard and Dvd – are serialized to a data file. The classes shown in the figure comprise the server-side of the application. The various methods of the classes of the application were tested (by the author of this guide) with, in the first instance, a number of main methods that test the correctness of card transactions when taking items out on loan and returning them. When testing was complete, the next stage in the development of the application was to construct the UI. The classes associated with the UI comprise the client-side of the application. The client-side of the themed application is used – as we will see later in this chapter – to interact with the server-side. For example, there are buttons on the UI that call methods of the DvdMembershipCard class to borrow and return DVDs. Clearly, and rather obviously, there is a close coupling between the UI and the business objects with which it interacts. In other words, the server-side of the application is of no value without the client-side of the application and vice versa. In general, whilst the development and testing of the client and server sides of an application may proceed independently of one another, both sides of an application will eventually be brought together to comprise the complete application. Final testing will test that the UI operates correctly in its interaction with server-side objects. The combination of the server and client ‘sides’ of the themed application into a single application results in what is known, in this case, as a standalone application in the sense that it runs on one computer: i.e. a standalone application runs in a single address space using a single Java Virtual Machine. Whilst it can readily be argued that the standalone version of the themed application is not realistic in that it is not designed to run on a (computer) network, its purpose in this guide is to provide a source of examples that illustrate how a number of fundamental Java programming concepts can be applied in the context of an application that is appropriate for learners. In short, the development of standalone applications is relevant from a learning point of view. The combination of the client and server sides of the themed application (discussed in the previous section) suggests that we can regard any standalone application as comprising client and server components that interact with one another. Perhaps not surprisingly, such applications are labelled with the term client/server. Figure 3.2, shown on the next page, illustrates the architecture of a typical client/server application. Tier 1, the client tier, comprises the UI; tier 2, the server tier, comprises the business objects of the application; tier 3, the persistent data tier, comprises data storage components such as data files and database tables. In the case of the themed application, the third tier comprises a data file that stores an array of members of the Media Store. In a typical client/server application, the UI interacts with the server-side objects which, in turn, interact with the data tier. In other words, the UI interacts with the server-tier; the UI does not access the data tier directly. The double-headed arrow (in the figure) that implies that the client and server tiers are connected is, in the case of a standalone application, an indication that the client and server tiers are deployed in the same address space. In the case of a networked application, the connection between the client and server tiers will either be an intranet or the Internet. Now that we have placed user interfaces in the context of a client/server architecture, the next section moves on to find out how a UI is constructed.
EN25	1	﻿The risk of computer crime has become a global issue affecting almost all countries. Salifu (2008) argues that the Internet is a "double-edged sword" providing many opportunities for individuals and organizations to develop and prosper, but at the same time has brought with it new opportunities to commit crime. For example, Nigeria-related financial crime is extensive and 122 out of 138 countries at an Interpol meeting complained about Nigerian involvement in financial fraud in their countries. The most notorious type attempted daily on office workers all over the world, is the so-called advance fee fraud. The sender will seek to involve the recipient in a scheme to earn millions of dollars if the recipient pays an advance fee (Ampratwum, 2009). Computer crime is an overwhelming problem worldwide. It has brought an array of new crime activities and actors and, consequently, a series of new challenges in the fight against this new threat (Picard, 2009). Policing computer crime is a knowledge-intensive challenge indeed because of the innovative aspect of many kinds of computer crime. Cyberspace presents a challenging new frontier for criminology, police science, law enforcement and policing. Virtual reality and computer-mediated communications challenge the traditional discourse of criminology and police work, introducing new forms of deviance, crime, and social control. Since the 1990s, academics and practitioners have observed how cyberspace has emerged as a new field of criminal activity. Cyberspace is changing the nature and scope of offending and victimization. A new discipline named cyber criminology is emerging. Jaishankar (2007) defines cyber criminology as the study of causation of crimes that occur in the cyberspace and its impact in the physical space.  Employees of the organization commit most computer crime, and the crime occurs inside company walls (Hagen et al., 2008: Nykodym et al, 2005). However, in our perspective of financial crime introduced in this chapter, we will define computer crime as a profit-oriented crime rather than a damage-oriented crime, thereby excluding the traditional focus of dissatisfied and frustrated employees wanting to harm their own employers.  Computer crime is defined as any violations of criminal law that involve knowledge of computer technology for their perpetration, investigation, or prosecution (Laudon and Laudon, 2010). The initial role of information and communication technology was to improve the efficiency and effectiveness of organizations. However, the quest of efficiency and effectiveness serves more obscure goals as fraudsters exploit the electronic dimension for personal profits. Computer crime is an overwhelming problem that has brought an array of new crime types (Picard, 2009). Examples of computer-related crimes include sabotage, software piracy, and stealing personal data (Pickett and Pickett, 2002). In computer crime terminology, the term cracker is typically used to denote a hacker with a criminal intent. No one knows the magnitude of the computer crime problem – how many systems are invaded, how many people engage in the practice, or the total economic damage. According to Laudon and Laudon (2010), the most economically damaging kinds of computer crime are denial-of-service attacks, where customer orders might be rerouted to another supplier. Eleven men in five countries carried out one of the worst data thefts for credit card fraud ever (Laudon and Laudon, 2010: 326): In early August 2008, U.S. federal prosecutors charged 11 men in five countries, including the United States, Ukraine, and China, with stealing more than 41 million credit and debit card numbers. This is now the biggest known theft of credit card numbers in history. The thieves focused on major retail chains such as OfficeMax, Barnes & Noble, BJ’s Wholesale Club, the Sports Authority, and T.J. Marxx. The thieves drove around and scanned the wireless networks of these retailers to identify network vulnerabilities and then installed sniffer programs obtained from overseas collaborators. The sniffer programs tapped into the retailers’ networks for processing credit cards, intercepting customers’ debit and credit card numbers and PINs (personal identification numbers). The thieves then sent that information to computers in the Ukraine, Latvia, and the United States. They sold the credit card numbers online and imprinted other stolen numbers on the magnetic stripes of blank cards so they could withdraw thousands of dollars from ATM machines. Albert Gonzales of Miami was identified as a principal organizer of the ring.  The conspirators began their largest theft in July 2005, when they identified a vulnerable network at a Marshall’s department store in Miami and used it to install a sniffer program on the computers of the chain’s parent company, TJX. They were able to access the central TJX database, which stored customer transactions for T.J. Marxx, Marshalls, HomeGoods, and A.J. Wright stores in the United States and Puerto Rico, and for Winners and HomeSense stores in Canada. Fifteen months later, TJX reported that the intruders had stolen records with up to 45 million credit and debit card numbers. TJX was still using the old Wired Equivalent Privacy (WEP) encryption system, which is relatively easy for hackers to crack. Other companies had switched to the more secure Wi-Fi Protected Access (WPA) standard with more complex encryption, but TJX did not make the change. An auditor later found that TJX had also neglected to install firewalls and data encryption on many of the computers using the wireless network, and did not properly install another layer of security software it had purchased. TJX acknowledged in a Securities and Exchange Commission filing that it transmitted credit card data to banks without encryption, violating credit card company guidelines. Computer crime, often used synonymous with cyber crime, refers to any crime that involves a computer and a network, where the computer has played a part in the commission of a crime. Internet crime, as the third crime label, refers to criminal exploitation of the Internet. In our perspective of profit-oriented crime, crime is facilitated by computer networks or devices, where the primary target is not computer networks and devices, but rather independent of the computer network or device.  Cyber crime is a term used for attacks on the cyber security infrastructure of business organizations that can have several goals. One goal pursued by criminals is to gain unauthorized access to the target’s sensitive information. Most businesses are vitally dependent on their proprietary information, including new product information, employment records, price lists and sales figures. According to Gallaher et al. (2008), an attacker may derive direct economic benefits from gaining access to and/or selling such information, or may inflict damage on an organization by impacting upon it. ﻿Fake websites have become increasingly pervasive and trustworthy in their appearance, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. Abbasi et al. (2010) found that the growth in profitable fake websites is attributable to several factors, including their authentic appearance, a lack of user awareness regarding them, and the ability of fraudsters to undermine many existing mechanisms for protecting against them. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Distinctions can be made between spoof sites and concocted sites. A spoof site is an imitation of an existing commercial website such as eBay or PayPal. A concocted site is a deceptive website attempting to create the impression of a legitimate, unique and trustworthy entity. Detecting fake websites is difficult. There is a need for both fraud cues as well as problem-specific knowledge. Fraud cues are important design elements of fake websites that may serve as indicators of their lack of authenticity. First, fake websites often use automatic content generation techniques to mass-produce fake web pages. Next, fraud cues include information, navigation, and visual design. Information in terms of web page text often contains fraud cues stemming from information design elements. Navigation in terms of linkage information and URL names for a website can provide relevant fraud cues relating to navigation design characteristics. For example, it is argued that 70 percent of ".biz" domain pages are fake sites. Fake websites frequently use images from existing legitimate or prior fake websites. For example spoof sites copy company logos from the websites they are mimicking. The fact that it is copied can be detected in the system (Abbasi et al., 2010). In addition to fraud cues, there is a need for problem-specific knowledge. Problem-specific knowledge regarding the unique properties of fake websites includes stylistic similarities and content duplication (Abbasi et al., 2010). Abbasi et al. (2010) developed a prototype system for fake website detection. The system is based on statistical learning theory. Statistical learning theory is a computational learning theory that attempts to explain the learning process from a statistical point of view. The researchers conducted a series of experiments, comparing the prototype system against several existing fake website detection systems on a test sample encompassing 900 websites. The results indicate that systems grounded in statistical learning theory can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. A variation of fake websites is fraudulent email solicitation where the sender of an email claims an association with known and reputable corporations or organizational entities. For example, one email from the "Microsoft/AOL Award Team" notified its winners of a sweepstake by stating, "The prestigious Microsoft and AOL has set out and successfully organized a Sweepstakes marking the end of year anniversary we rolled out over 100,000.000.00 for our new year Anniversary Draw" (Nhan et al., 2009). The email proceeded to ask for the potential victim's personal information.  Nhan et al. (2009) examined 476 fraudulent email solicitations, and found that the three most frequently alleged organizational associations were Microsoft, America Online, and PayPal. Fraudsters also attempt to establish trust through associating with credit-issuing financial corporations and authoritative organizations and groups.  Money laundering is an important activity for most criminal activity (Abramova, 2007; Council of Europe, 2007; Elvins, 2003). Money laundering means the securing of the proceeds of a criminal act. The proceeds must be integrated into the legal economy before the perpetrators can use it. The purpose of laundering is to make it appear as if the proceeds were acquired legally, as well as disguises its illegal origins (Financial Intelligence Unit, 2008). Money laundering takes place within all types of profit-motivated crime, such as embezzlement, fraud, misappropriation, corruption, robbery, distribution of narcotic drugs and trafficking in human beings (Økokrim, 2008). Money laundering has often been characterized as a three-stage process that requires (1) moving the funds from direct association with the crime, (2) disguising the trail to foil pursuit, and (3) making them available to the criminal once again with their occupational and geographic origins hidden from view. The first stage is the most risky one for the criminals, since money from crime is introduced into the financial system. Stage 1 is often called the placement stage. Stage 2 is often called the layering stage, in which money is moved in order to disguise or remove direct links to the offence committed. The money may be channeled through several transactions, which could involve a number of accounts, financial institutions, companies and funs as well as the use of professionals such as lawyers, brokers and consultants as intermediaries. Stage 3 is often called the integration stage, where a legitimate basis for asset origin has been created. The money is made available to the criminal and can be used freely for private consumption, luxury purchases, real estate investment or investment in legal businesses. Money laundering has also been described as a five-stage process: placement, layering, integration, justification, and embedding (Stedje, 2004). It has also been suggested that money laundering falls outside of the category of financial crime. Since money-laundering activities may use the same financial system that is used for the perpetration of core financial crime, its overlap with the latter is apparent (Stedje, 2004). According to Joyce (2005), criminal money is frequently removed from the country in which the crime occurred to be cycled through the international payment system to obscure any audit trail. The third stage of money laundering is done in different ways. For example, a credit card might be issued by offshore banks, casino 'winning' can be cashed out, capital gains on option and stock trading might occur, and real estate sale might cause profit.  The proceeds of criminal acts could be generated from organized crime such as drug trafficking, people smuggling, people trafficking, proceeds from robberies or money acquired by embezzlement, tax evasion, fraud, abuse of company structures, insider trading or corruption. The Financial Intelligence Unit (2008) in Norway argues that most criminal acts are motivated by profit. When crime generates significant proceeds, the perpetrators need to find a way to control the assets without attracting attention to them selves or the offence committed. Thus, the money laundering process is decisive in order to enjoy the proceeds without arousing suspicion. ﻿While we focus on white-collar financial crime in this book on computer crime, we must not forget that there are a number of other types of crime that are typical for cyber crime and Internet crime as well. Typical examples are hacking, child pornography and online child grooming. In this chapter, we present the case of child grooming as computer crime. Internet use has grown considerably in the last decade. Information technology now forms a core part of the formal education system in many countries, ensuring that each new generation of Internet users is more adept than the last. Research studies in the UK suggest that the majority of young people aged 9-19 accessed the Internet at least once a day. The Internet provides the opportunity to interact with friends on social networking sites such as Myspace and Bebo and enables young people to access information in a way that previous generations would not have thought possible. The medium also allows users to post detailed personal information, which may be accessed by any site visitor and provides a platform for peer communication hitherto unknown (Davidson and Martellozzo, 2008). There is, however, increasing evidence that the Internet is used by some adults to access children and young people in order to groom them for the purposes of sexual abuse. Myspace have recently expelled 29,000 suspected sex offenders and is being sued in the United States by parents who claim that their children were contacted by sex offenders on the site and consequently abused (BBC, 2007). The Internet also plays a role in facilitating the production and distribution of indecent illegal images of children, which may encourage and complement online grooming.  Recent advances in computer technology have been aiding sexual sex offenders, stalkers, child pornographers, child traffickers, and others with the intent of exploiting children (Kierkegaard, 2008: 41): Internet bulletin boards, chat rooms, private websites, and peer-to-peer networks are being used daily by pedophiles to meet unsuspecting children. Compounding the problem is the lack of direct governance by an international body, which will curb the illegal content and activity. Most countries already have laws protecting children, but what is needed is a concerted law enforcement and international legislation to combat child sex abuse. Men who target young people online for sex are pedophiles (Kierkegaard, 2008; Wolak et al., 2008). According to Dunaigre (2001), the pedophile is an emblematic figure, made into a caricature and imbued with all the fears, anxieties and apprehensions rocking our society today. Pedophile acts are - according to the World Health Organization (WHO) - sexual behavior that an adult major (16 years or over), overwhelmingly of the male sex, acts out towards prepubescent children (13 years or under). According to the WHO, there must normally be a five-year age difference between the two, except in the case of pedophilic practices at the end of adolescence where what counts is more the difference in sexual maturity. However, the definition of criminal behavior varies among countries. As will become evident from reading this article, pedophile acts in Norway are sexual behavior that a person acts out towards children of 16 years or under. There is no minimum age definition for the grooming person in Norwegian criminal law, but age difference and difference in sexual maturity is included as criteria for criminal liability.  Wolak et al. (2009: 4) present two case examples of crimes by online sex offenders in the United States: • Police in West Coast state found child pornography in the possession of the 22-year-old offender. The offender, who was from a North-eastern state, confessed to befriending a 13-year-old local boy online, traveling to the West Coast, and meeting him for sex. Prior to the meeting, the offender and victim had corresponded online for about six months. The offender had sent the victim nude images via web cam and e-mail and they had called and texted each other hundreds of times. When they met for sex, the offender took graphic pictures of the encounter. The victim believed he was in love with the offender. He lived alone with his father and was struggling to fit in and come to terms with being gay. The offender possessed large quantities of child pornography that he had downloaded from the Internet. He was sentenced to 10 years in prison. • A 24-year-old man met a 14-year-old girl at a social networking site. He claimed to be 19. Their online conversation became romantic and sexual and the victim believed she was in love. They met several times for sex over a period of weeks. The offender took nude pictures of the victim and gave her alcohol and drugs. Her mother and stepfather found out and reported the crime to the police. The victim was lonely, had issues with drugs and alcohol, and problems at school and with her parents. She had posted provocative pictures of herself on her social networking site. She had met other men online and had sex with them. The offender was a suspect in another online enticement case. He was found guilty but had not been sentenced at time of the interview.  According to Davidson and Martellozzo (2008: 277), Internet sex offender behavior can include: "the construction of sites to be used for the exchange of information, experiences, and indecent images of children; the organization of criminal activities that seek to use children for prostitution purposes and that produce indecent images of children at a professional level; the organization of criminal activities that promote sexual tourism". Child grooming is a process that commences with sexual sex offenders choosing a target area that is likely to attract children. In the physical world, this could be venues visited by children such as schools, shopping malls or playgrounds. A process of grooming then commences when offenders take a particular interest in the child and make them feel special with the intention of forming a bond. The Internet has greatly facilitated this process in the virtual world. Offenders now seek out their victims by visiting Internet relay chat (IRC) rooms from their home or Internet cafés at any time. Once a child victim is identified, the offender can invite it into a private area of the IRC to engage in private conversations on intimate personal details including the predator's sex life (Australian, 2008). ﻿When a business enterprise is the potential victim of computer crime, there are a number of measures that can be implemented to protect the business. In the survey by Hagen et al. (2008), they addressed both breath and depth in defense strategies. Depth is concerned with technological as well as organizational measures, while depth is concerned with dimensions of prevention, emergency preparedness and detection. The survey addressed the use of a broad range of technical security measures relating to access control and protection of data. Technical security measures include prevention (password, physical zones, biometric authentication, and software update), emergency (backup), and detection (intrusion detection and antivirus software). Organizational security measures include prevention (access rights and user guidelines), emergency (management plans), detection (log reviews), and incident response (management reports). The survey showed that the use of personal passwords is widespread among all enterprises, even the smallest ones (Hagen et al., 2008: 364): The trend is that the use of a variety of access control mechanisms increases with enterprise size. There is also a clear tendency that large enterprises implement more and a wider range of emergency preparedness and detection measures. The findings show that small enterprises should strengthen their access control and data protection measures, in addition to security routines. Hagen et al. (2008) found it surprising that large enterprises did not perform better than small enterprises when it comes to awareness raising and education of users as organizational security measures.  Profiling of criminals is based on the idea that an individual committing crime in cyberspace using a computer can fit a certain outline or profile. A profile consists of offender characteristics that represent assumptions of the offender’s personality and behavioral appearance. Characteristics can include physical build, offender sex, work ethic, mode of transportation, criminal history, skill level, race, marital status, passiveness/aggressiveness, medical history, and offender residence in relation to the crime (Nykodym et al., 2005). Nykodym et al. (2005: 413) make distinctions between four main categories of cyber crime: espionage, theft, sabotage, and personal abuse of the organizational network: Unlike saboteurs and spies, the thief is guided only by mercantile motives for his own gain. The only goal in front of the cyber thief is to steal valuable information from an organization and use it or sell it afterwards for money. In terms of criminal profiling, Nykodym et al. (2005) found that there is a strong pattern in the age of these cyber robbers. If the crime is for less than one hundred thousand dollars, then most likely the attacker is young 20-25 years old, male or female, still in the low hierarchy of the organization. If the crime involves more money, then the committer is probably an older male from a management level in the organization. His crime is not driven by hate or revenge but by greed and hunger for money.  Computer crime is defined as financial crime in this book. White-collar criminals commit financial crime. Characteristics of white-collar criminals include: • Wealthy yet greedy person • Highly educated yet practical person • Socially connected yet anti-social person • Talks ethics yet acts immoral • Employed by and in a legitimate organization • A person of respectability with high social status • Member of the privileged socioeconomic class • Commit crime within the occupation based on competence • On the slippery slope from legitimate to illegitimate behavior • Often charismatic, convincing and socially skilled • So desperate to succeed that they are willing to use criminal means • Sometimes excited about the thrill of not being uncovered • Often in a position where the police is reluctant to start investigation • Applies resources to hide tracks and crime outcome • Behaves in court in a manner creating sympathy and understanding  These kinds of characteristics are organized according to criteria in criminal profiling. For example, some of them are individual factors that are grounded in psychology, while others are environmental factors grounded in sociology. In terms of psychological factors, criminal profiling may ask question such as: • What kind of personality types become more easily white-collar criminals? • What are their typical background, life style and development? • What are their values, ideas and ambitions? In terms of sociological factors, criminal profiling may ask questions such as: • How do white-collar criminals look at society and their own role in society? • How do they perceive laws, and what do they consider to be crime and criminals? • How do they participate in networks, and what is associated with status and power? Not all computer criminals are white-collar criminals, but most of them are committing crime for financial gain. Cyber offenders are likely to share a broader range of social characteristics, and the cases of hacking and other Internet-related offences that have been reported in the media would suggest they are likely to be young, clever and fairly lonely individuals who are of middle-class origin, often without prior criminal records, often processing expert knowledge and often motivated by a variety of financial and non-financial goals. Some degree of technical competence is required to commit many computer-related types of crime (Salifu, 2008).  Some theorists believe that crime can be reduced through the use of deterrents. The goal of deterrence, crime prevention, is based on the assumption that criminals or potential criminals will think carefully before committing a crime if the likelihood of getting caught and/or the fear of swift and severe punishment are present. Based on such belief, general deterrence theory holds that crime can be thwarted by the threat of punishment, while special deterrence theory holds that penalties for criminal acts should be sufficiently severe that convicted criminals will never repeat their acts (Lyman and Potter, 2007). Threat is an external stimulus that exists whether or not an individual perceives it (Johnson and Warkentin, 2010). If an individual perceives the threat, then is has deterrent potential. Deterrence theory postulates that people commit such crimes on the basis of rational calculations about perceived personal benefits, and that the threat of legal sanctions will deter people for fear of punishment (Yusuf and Babalola, 2009). In more recent years when executives have been seen arrested and handcuffed for the purposes of public humiliation, it sets in motion a deterrence model of crime prevention or at the very least, a shaming policy. The purpose of these public arrests are often symbolic and say more about the regulatory agencies need to appear to be legitimately prosecuting corporate wrongdoers. As such, with regulation so closely tied to the political climate, there has been no consistency in the prosecution of corporate criminals, as compared with drug war policies of the past couple of decades (Hansen, 2009). ﻿Cyber crime investigations have both similarities and differences when compared to traditional crime such as burglary and robbery. Traditional crime generally concern personal or property offences that law enforcement has continued to combat for centuries. Cyber crime is characterized by being technologically advanced, it can occur almost instantaneously, and it is extremely difficult to observe, detect, or track. These problems are compounded by the relative anonymity afforded by the Internet as well as the transcendence of geographical and physical limitations in cyberspace. Criminals are able to take advantage of a virtually limitless pool of potential victims (Hinduja, 2007). Policing financial crime generally – according to Pickett and Pickett (2002) – is concerned with whistle blowing and detection, roles of shareholders and main board and chief executive officer and senior executives, investigations, forensics. Policing financial crime – according to Levi (2007) – is concerned with the organization of policing deception, the contexts of police undercover work, covert investigations of white-collar crime, prosecution and relationship to policing fraud. Covert activity is restricted mainly to the informal obtaining of financial information or the official obtaining of information about suspected bank accounts without the knowledge of the account-holder. Policing cyber crime is concerned with all these issues as well as a tight surveillance of relevant activities on the Internet. Within crime investigations, IT forensics and cyber crime investigations are an extremely complicated field (Callanan and Jones, 2009). Kao and Wang (2009) suggest an approach to improving cyber crime investigation consisting of three stages: independent verification of digital clues, corresponding information from different sources, and preparation of a valid argument. Furthermore, covert investigations in the workplace represent a debated practice when investigating financial crime (Tackett, 2008).  Investigation and prevention of cyber crime and building corporate reputation have the value configuration of a value shop. As can be seen in Figure 1, the five activities of a value shop are interlocking and while they follow a logical sequence, much like the management of any project, the difference from a knowledge management perspective is the way in which knowledge is used as a resource to create value in terms of results for the organization. Hence, the logic of the five interlocking value shop activities in this example is of a policing unit and how it engages in its core business of conducting reactive and proactive investigations.  The sequence of activities starts with problem understanding, moves into alternative investigation approaches, investigation decision, and investigation implementation, and ends up with criminal investigation evaluation (Sheehan and Stabell, 2007). However, these five sequential activities tend to overlap and link back to earlier activities, especially in relation to activity 5 (control and evaluation) in policing units when the need for control and command structures are a daily necessity because of the legal obligations that policing unit authority entails. Hence, the diagram is meant to illustrate the reiterative and cyclical nature of these five primary activities for managing the knowledge collected during and applied to a specific investigation in a value shop manner. Furthermore, Figure 1 illustrates the expanding domain of the knowledge work performed in financial crime investigations, starting in the centre with problem understanding and ending at the edge with evaluation of all parts of the investigation process. These five primary activities of the value shop in relation to a financial crime investigation and prevention unit can be outlined as (Sheehan and Stabell, 2007): 1. Problem Definition. This involves working with parties to determine the exact nature of the crime and hence how it will be defined. For example, a physical assault in a domestic violence situation depending on how the responding officers choose and/or perceive to define it can be either upgraded to the status of grievous bodily harm to the female spouse victim or it may be downgraded to a less serious common, garden variety assault where a bit of rough handing took place towards the spouse. This concept of making crime, a term used on how detectives choose to make incidents into a crime or not, is highly relevant here and is why this first activity has been changed from the original problem finding term used in the business management realm to a problem definition process here in relation to policing work. Moreover, this first investigative activity involves deciding on the overall investigative approach for the case not only in terms of information acquisition but also as indicated on Figure 1 in undertaking the key task, usually by a senior investigative officer in a serious or major incident, of forming an appropriate investigative team to handle the case. 2. Investigation Approaches. This second activity of identifying problem solving approaches involves the actual generation of ideas and action plans for the investigation. As such it is a key process for it sets the direction and tone of the investigation and is very much influenced by the composition of the members of the investigative team. For example, the experience level of investigators and their preferred investigative thinking style might be a critical success factor in this second primary activity of the value shop.  3. Approach Decision. This solution choice activity represents the decision of choosing between alternatives generated in the second activity. While the least important primary activity of the value shop in terms of time and effort, it might be the most important in terms of value. In this case, trying to ensure as far as is possible that what is decided on to do is the best option to follow to get an effective investigative result. A successful solution choice is dependent on two requirements. First, alternative investigation steps were identified in the problem solving approaches activity. It is important to think in terms of alternatives. Otherwise, no choices can be made. Next, criteria for decision-making have to be known and applied to the specific investigation. 4. Investigation Implementation. As the name implies, solution execution represents communicating, organizing, investigating, and implementing decisions. This is an equally important process or phase in an investigation as it involves sorting out from the mass of information coming into the incident room about a case and directing the lines of enquiry as well as establishing the criteria used to eliminate a possible suspect from further scrutiny in the investigation. A miscalculation here can stall or even ruin the whole investigation. Most of the resources spent on an investigation are used here in this fourth activity of the value shop.$$$﻿Cyber crime investigations have both similarities and differences when compared to traditional crime such as burglary and robbery. Traditional crime generally concern personal or property offences that law enforcement has continued to combat for centuries. Cyber crime is characterized by being technologically advanced, it can occur almost instantaneously, and it is extremely difficult to observe, detect, or track. These problems are compounded by the relative anonymity afforded by the Internet as well as the transcendence of geographical and physical limitations in cyberspace. Criminals are able to take advantage of a virtually limitless pool of potential victims (Hinduja, 2007). Policing financial crime generally – according to Pickett and Pickett (2002) – is concerned with whistle blowing and detection, roles of shareholders and main board and chief executive officer and senior executives, investigations, forensics. Policing financial crime – according to Levi (2007) – is concerned with the organization of policing deception, the contexts of police undercover work, covert investigations of white-collar crime, prosecution and relationship to policing fraud. Covert activity is restricted mainly to the informal obtaining of financial information or the official obtaining of information about suspected bank accounts without the knowledge of the account-holder. Policing cyber crime is concerned with all these issues as well as a tight surveillance of relevant activities on the Internet. Within crime investigations, IT forensics and cyber crime investigations are an extremely complicated field (Callanan and Jones, 2009). Kao and Wang (2009) suggest an approach to improving cyber crime investigation consisting of three stages: independent verification of digital clues, corresponding information from different sources, and preparation of a valid argument. Furthermore, covert investigations in the workplace represent a debated practice when investigating financial crime (Tackett, 2008).  Investigation and prevention of cyber crime and building corporate reputation have the value configuration of a value shop. As can be seen in Figure 1, the five activities of a value shop are interlocking and while they follow a logical sequence, much like the management of any project, the difference from a knowledge management perspective is the way in which knowledge is used as a resource to create value in terms of results for the organization. Hence, the logic of the five interlocking value shop activities in this example is of a policing unit and how it engages in its core business of conducting reactive and proactive investigations.  The sequence of activities starts with problem understanding, moves into alternative investigation approaches, investigation decision, and investigation implementation, and ends up with criminal investigation evaluation (Sheehan and Stabell, 2007). However, these five sequential activities tend to overlap and link back to earlier activities, especially in relation to activity 5 (control and evaluation) in policing units when the need for control and command structures are a daily necessity because of the legal obligations that policing unit authority entails. Hence, the diagram is meant to illustrate the reiterative and cyclical nature of these five primary activities for managing the knowledge collected during and applied to a specific investigation in a value shop manner. Furthermore, Figure 1 illustrates the expanding domain of the knowledge work performed in financial crime investigations, starting in the centre with problem understanding and ending at the edge with evaluation of all parts of the investigation process. These five primary activities of the value shop in relation to a financial crime investigation and prevention unit can be outlined as (Sheehan and Stabell, 2007): 1. Problem Definition. This involves working with parties to determine the exact nature of the crime and hence how it will be defined. For example, a physical assault in a domestic violence situation depending on how the responding officers choose and/or perceive to define it can be either upgraded to the status of grievous bodily harm to the female spouse victim or it may be downgraded to a less serious common, garden variety assault where a bit of rough handing took place towards the spouse. This concept of making crime, a term used on how detectives choose to make incidents into a crime or not, is highly relevant here and is why this first activity has been changed from the original problem finding term used in the business management realm to a problem definition process here in relation to policing work. Moreover, this first investigative activity involves deciding on the overall investigative approach for the case not only in terms of information acquisition but also as indicated on Figure 1 in undertaking the key task, usually by a senior investigative officer in a serious or major incident, of forming an appropriate investigative team to handle the case. 2. Investigation Approaches. This second activity of identifying problem solving approaches involves the actual generation of ideas and action plans for the investigation. As such it is a key process for it sets the direction and tone of the investigation and is very much influenced by the composition of the members of the investigative team. For example, the experience level of investigators and their preferred investigative thinking style might be a critical success factor in this second primary activity of the value shop.  3. Approach Decision. This solution choice activity represents the decision of choosing between alternatives generated in the second activity. While the least important primary activity of the value shop in terms of time and effort, it might be the most important in terms of value. In this case, trying to ensure as far as is possible that what is decided on to do is the best option to follow to get an effective investigative result. A successful solution choice is dependent on two requirements. First, alternative investigation steps were identified in the problem solving approaches activity. It is important to think in terms of alternatives. Otherwise, no choices can be made. Next, criteria for decision-making have to be known and applied to the specific investigation. 4. Investigation Implementation. As the name implies, solution execution represents communicating, organizing, investigating, and implementing decisions. This is an equally important process or phase in an investigation as it involves sorting out from the mass of information coming into the incident room about a case and directing the lines of enquiry as well as establishing the criteria used to eliminate a possible suspect from further scrutiny in the investigation. A miscalculation here can stall or even ruin the whole investigation. Most of the resources spent on an investigation are used here in this fourth activity of the value shop.
EN09	1	﻿This guide explains the processes used to make Keynote documents. Keynote is Apple Inc.’s equivalent to Microsoft’s Powerpoint. Keynote’s strength is its ease of use and its ability to handle a variety of media types, including HD Video. This guide to Keynote is one of three books I have written for Bookboon on iWork. My books on Pages and Numbers complement this one, with some areas of repetition, each guide is designed to stand alone. A great way to learn is to experiment and play. Use this guide to focus your learning on specific areas of Keynote before taking a broad view of the myriad of possibilities for this software. This guide describes ways to assemble and edit content. It does not seek to give advice on presentation methodology, too often business presentations suffer from densely packed slides, with too much text and statistical information squeezed into them; information that is best left for a report. Rather than cram the contents of a report onto a handful of slides, judicious planning will make for compelling presentations. Keynote is not a place to copy and paste all the text of a report. It is far better to highlight and illustrate important points using Keynote’s excellent graphic and animation capabilities. That said, animation and graphic elements should be used sparingly to aid the communication, and not as a distraction to it. Keynote can be used to create dynamic and engaging presentations. Text, images and charts can be arranged with ease. Keynote makes it easy to add audio and video, add transitions between slides, animate data, and then share presentations in a variety of ways. The guide describes software functions and outlines generic examples of the software in use. Further information can be found on Apple’s web pages, or via Apple’s Certified Training Scheme. Regarding keyboard shortcuts. The keyboard shortcuts mentioned in this book will work on International English QWERTY keyboards. For US keyboards the only difference is that Alt key ( ) is called Option ( ). For AZERTY and other language keywords please try the shortcuts, they will probably work. For seasoned Mac users please note that the Apple key is now referred to as the Command key. It is labelled cmd , not . There are three ways to launch Keynote. • Go to your Applications folder. In Finder choose Go > Applications. Open the iWork ’09 folder and double– click the Keynote icon. (Unless you have done a customized iWork installation the iWork folder will be in the Application folder found in the root of your primary hard drive.) • In the Dock, click the Keynote icon. (In Apple’s latest operating system named Lion, a Keynote icon will appear in Launchpad.) • Double–click any Keynote document. Every time you launch Keynote or try to create a new document, Keynote’s Template Chooser appears. The Template Chooser contains Apple designed templates and any Templates created by you. Apple’s Templates can be customized to suit your tastes or to comply with a business’s graphic identity. There will be more on Themes later. The White and Black Themes are good options to experiment with. Keynote has features that are shared with, or are similar to, features found in other Apple applications such as Numbers and Pages. This section lists these features. Understanding features including Inspectors and the Media Browser are essential when learning about any Apple software. To explore the features described in Section 1. launch Keynote and open any Template. When launching Keynote the following message may appear. By default Keynote’s window contains a customizable Tool Bar, a Format Bar, a Slides Pane, and the Slide Canvas. Other panes may be opened, including Master Slides Pane, and Presenter Notes Pane. The Tool Bar contains several icons. These control common functions and will be described later. Note that some Tool Bar Icons are greyed-out meaning they cannot be used. They become active once an Object is selected. Also note that the Tool Bar can be customized to display buttons for commands based on user preference. From left to right: • New – This adds a new slide. The keyboard shortcut is Command – Shift – N. • Play – This starts the presentation in Slideshow, at the selected slide, the keyboard shortcut is Command–Alt–P. To start a Slideshow from the first slide hold down the Alt key and click the Play button in the Tool Bar. • View – changes the View Mode. Options include Navigator, Outline, Slide Only and Light Table. Of these Navigator and Light Table are the most useful when constructing documents. Outline Mode will be discussed further in Section 5.4. View also controls the display of Rulers, Format Bat, Presenter Notes and Master Slides. When developing a presentation to support a speech or lecture using Presenter Notes can prove a great aid to memory. Keynote can be configured to display on two displays simultaneously, one showing slides to an audience and a comprehensive display of Presenter Notes, Next Slide and Time Elapsed or Remaining on the other. Comments can be temporarily hidden from the View Menu. Comments are like virtual sticky notes and can be found in all the iWork applications. • Guides – By default Keynote displays alignment guides to help layout. These appear as yellow lines that indicate whether an object is aligned to the top, bottom, centre or side of another object, or where it is in relation to the canvas. Choosing all four options from Show Guides at Object Center to Show Relative Sizes will give maximum layout feedback help. • Themes – On launching Keynote a Theme Chooser appears. Users can create their own Themes. Having a Theme button in the Tool Bar allows Themes to be swapped in an open document. This function is useful for companies wishing to update presentations to meet their latest visual identity guidelines. A presentation using their properly applied 2008 theme can be updated to their 2012 theme in a single click. • Masters – Themes contain several Master Slides; Title & Subtitle, Title & Bullets, Title & Bullets – 2 Column… et cetera. Use this menu to change slides to different Masters. • Text Box – This adds a simple text box to the slide canvas. It is better to use a slide Master with a Text Box and modify it. This makes the Theme change function work speedily. It isn’t wrong, nor does it expose weaknesses in the Keynote software, to add Text Boxes. Additional Text Boxes cannot be automatically changed when changing Themes.  ﻿When the View is set to Navigator, the pane to the left of the Slide Canvas is labelled Slides. Change the View to Outline and the pane is labelled Outline. The other view modes include Slide Only and Light Table. The Navigator View is probably the best option for creating and editing presentations, as thumbnails of the slide appear in the Slides Pane. This allows thumbnails to be click–dragged into new positions and slides to be duplicated and deleted with ease. Click-dragging a slide in Navigator View will reposition it in the running order. Holding down the Alt key whilst doing this will copy the slide to a new location. As with nearly all keyboard short–cuts the modifier key, in this case the Alt Key, should only be released after the mouse button is released. Other operations can be accessed using right–click on the Slides Pane. Other operations include the ability to skip slides. This is useful when shorter versions of a presentation are required; perhaps a thirty minute slot has been cut to twenty. The Skip function is a safe way to trim a presentation without deleting any data. Skipped slides won’t display as part of a Slide Show, but can be easily un–skipped too; the context menu command for this is Don’t Skip Slide. In teaching scenarios a Keynote can be prepared with extra sections that might be useful in class, but can be set to Skip by default, and only revealed if the teacher thinks it prudent to explore such areas. Skipping slides avoids the need to have several similar Keynote documents on the same subject. So rather than having to update several Keynotes as data changes, just the one document needs updating and the Skip function helps tailor the presentation for a variety of audiences and durations. Only revealed when the View is set to Show Master Slides, the Master Slide Pane appears above the Slides or Outline Pane to the left of the Slide Canvas. From here the current slide’s format can be changed from one Master to another. Simply click the slide or slides to be changed and click on the desired Master. This operation can also be achieved using the Master button in the Tool Bar. The Presenter Notes Pane appears under the Slide Canvas when set from the View menu in the Tool Bar. Presenter notes can be copied and pasted from other applications such as Pages or typed directly into the Presenter Notes Pane. They can be a great aid to memory when making presentations. Presenter Notes can also be included in handouts printed from Keynote. The Inspector is a key feature in Apple’s iLife and iWork software. Users new to Apple Mac software need to embrace the use of the Inspector. Inspectors control nearly all the parameters in iWork software. Numbers, Pages and Keynote have some unique Inspectors and some in common. For instance all the applications have a Document Inspector containing slightly different parameters in each. The Inspector is launched by either clicking the Inspector icon in the Toolbar or View > Show Inspector. Keynote has ten inspector tabs. They are: • Document Inspector – The Document Inspector should be used when first setting up a document. Divided into three tabs. The first tab contains slideshow preferences with options for Looping the slideshow. From here the Presentation mode can be changed, along with the Slide Size. The bottom section is Require Password To Open. For additional security a password can be entered to lock a Keynote document. The password would then be required every-time the document is opened. The next tab is Audio. There are several ways to add audio to a presentation, and using this tab this may not be the most flexible option. The last tab is Spotlight. Completing the fields for Author, Title, Keywords, and Comments is recommended practice. It makes documents easier to search for, especially when using the Mac Spotlight search engine. All of these fields are for metadata. Author and Title are self-explanatory. Adding Keywords helps classify a document. For example a Keynote document designed for a seminar on Apple’s Aperture could have the keywords, ‘Apple’, ‘Aperture’, ‘photography’, ‘digital imaging’ and ’45 minutes’. The list could go on. There are five keywords here. Keywords are denoted by commas, ‘Apple, Aperture, photography’ ‘digital imaging’ and ‘45 minutes’ are all single keywords. Searches can be case-sensitive, but using ‘apple’ and ‘Apple’ as keywords is not strictly necessary. When choosing keywords it is helpful to invoke the spirit of the librarian. A less rigorous form of applying metadata is using the Comments field. Here paragraphs of descriptive text can add be added. In the example of the Apple Aperture Keynote document, the comments field could read, ‘Seminar Presentation for Apple’s Aperture, aimed at the serious amateur and professional audience…’ • Slide Inspector – The Slide Inspector has two tabs, Transition and Appearance. Transitions can be applied to one or more slides at a time. There are currently forty-eight transition effects and something known as Magic Move, located in the Effect menu of the Transition tab. Experiment with the transitions to learn more, try changing the transition duration as this may enhance the effect. Some transitions have a direction, moving from left to right, or top to bottom. When this is the case the Direction menu becomes active. The Start Transition menu is set to On Click by default, meaning that to move the the next slide a mouse click or an alternative to click has to be made†. Transitions can be triggered in other ways, as listed in the Start Transition menu. † Alternatives to a mouse click, include clicking a trackpad, tapping the Spacebar, using a keyboard’s forward arrow key, using a remote control device ﻿The Media Browser appears in many of Apple’s application software. It is a system wide utility that is accessed from within programs such as Keynote. It has a distinct icon displaying a frame of film, a picture frame and two musical notes. This icon appears in different places in different applications. The Media Browser can be launched from the Toolbar or View > Media Browser. It contains three tabs, Audio, Photos and Movies. Audio – This contains tracks from the current user’s iTune account. Garage band projects, these might include voice–overs or podcasts and audio from other applications such as Aperture or Final Cut. Photos – This contains images stored in iPhoto or Aperture is it is installed Movies – This contains video files stored in iMovie, the current user’s Movies Folder, or in iTunes. If Final Cut or Aperture are installed video files from those applications will also appear here. The bottom pane of the Media Browser contains thumbnails of the respective media files. Double–clicking the thumbnail previews the file. In the case of Photos it enlarges the thumbnail to fill the Media Browser pane. As with all iWork applications, Keynote stores graphics and other media within the file itself, with the exception of Fonts. This makes for easy transfer of files from workstation to laptop et cetera. A consequence of this is that iWork documents may have large files sizes though there is an easy remedy in the Reduce File Size command. This will be illustrated in Section 16 — Sharing Your Work. To insert a media file or files select them in the Media Browser and drag them onto the Keynote Slide Canvas. When images are placed into Keynote they may look too dark or lack contrast. Like all iWork applications there is an Adjust Image window that can be used to adjust the brightness, contrast, saturation and other image parameters directly within Keynote. There are several image parameters that can be used to finesse imported photographs though often just clicking the Enhance button will improve a picture. Image Adjust is an icon in the Toolbar, or can be launched from View > Adjust. When you have taken the time to format a picture frame or text box there is no need to repeat all the steps taken when you wish to apply that style to subsequent objects. Styles can be copied and pasted. Start this process by creating two text boxes. Use the Graphics Inspector to stylize one box. Ensure that Text Box is selected then go to Format > Copy Style. Select the second text box and go to Format > Paste Style. The second Text Box will take on the general appearance of the stylized one. There are several methods for preparing presentations. With presentations planning and rehearsal are essential. As previously stated this guide does not aim to explain the art of presentation. Information on presentation creation and delivery can be found on the web with several sites offering Keynote tips and techniques. Suffice to say planning is required before opening Keynote. Keynote can help structure planning processes, once you have a concept in mind. Outline mode is a particularly useful way to list bullet points as hierarchies and sequences when creating a presentation from scratch. There are five stages in the life of a presentation: 1. Planning – What you need to do before opening Keynote and using Keynote in Outline mode. 2. Construction – Using Outline, Navigator and Light Table modes to build and sequence the presentation. 3. Rehearsal — For presentations that are to be delivered to an audience rehearsal is very important. Keynote has a rehearsal mode that can be used to check timings. 4. Delivery — After rehearsals comes the main event often in front of an audience. Being able to stop and jump to different slides is a useful skill. Keynote presentations don’t have to be delivered in person. Using Kiosk mode allows users to navigate through presentation material on their own. 5. Legacy – Making a presentation may be sufficient, though legacy items can be created, such as handouts or narrated versions of the presentation in video form that can be accessed on-demand, by whoever. Keynote ships with forty-four Apple designed themes. Themes are collections of Master Slides, for example, titles slides, bullets, bullets with photos, and so on. Themes have a coordinated design for fonts, colours and style across their respective Master Slides. An Apple Designed Theme may be a perfect starting point for a presentation, though users can generate and save their own themes, or purchase extra themes online. Defining Master Slides and creating custom themes will be explored later. Some Keynote Themes only have two resolutions, 800 x 600 pixels and 1024 x 768 pixels. Newer Themes come in three additional sizes; 1280 x 720, 1920 x 1080†, and 1680 x 1050. Although presentations can be set to playback so that they fill the screen, choosing an option with sufficient resolution at an appropriate aspect ratio is very important. The Theme Chooser contains all the themes currently available including any custom ones you might make. To see previews of all the Master Slides in a Theme, skim the cursor over the Theme’s thumbnail. Before selecting a slide select the correct slide resolution from the Slide SIze Pop–up menu. Master Slides are preset layouts for title slides, bullet point slides, bullets with a picture and so on. Themes are a collection of Master Slides. The Master icon in the toolbar can be used to change one or more slides to a different master layout. The View icon in the toolbar can be used to Show Master Slides. Click dragging Master Slide thumbnails onto existing slides thumbnails changes their master layout. Although many users begin making their Keynote, or Powerpoint, presentations in slide view, Outline view is a great way to structure ideas. iWork’s Pages has several Outline templates to help with this task. Outline view is one of several options that can be found under the View menu. If the default Outline view is not suitable it can be adjusted. To make the Slide pane areas bigger move the cursor over the resize handle and click drag to adjust its size. ﻿Spreadsheet tables can be created, complete with calculations, directly in Keynote. Clicking the Table icon in the Toolbar places a basic three column by three row table onto a slide. The Table Inspector can be used to modify that Table adding extra rows and columns as needed. The Table Inspector be used to format the Table. However in most circumstances spreadsheet data is best copied and pasted into slides from iWork’s Numbers. As Numbers is a dedicated spreadsheet package it offers far more data editing and function control than Keynote. Having Table creation control in Keynote is great though the primary reason for having this capability is to allow table and chart data to be modified within Keynote. For instance, if sales figures change immediately before a business presentation, the table and chart data can be changed in Keynote without the need to locate the original spreadsheet data. Clicking on any Table Cell that contains a formula invokes the Formula Editor. Once active the Formula Editor can be used to modify equations. If Numbers is not available, create a simple table, select a cell and press the equals (=) key. In common with the other iWork applications Keynote runs automatic spell checking. To switch this off go to Edit > Spelling > Check Spelling As You Type and mark sure it is un–ticked. When on potentially misspelt words are automatically underlined with a red dotted line. Using the context menu spelling suggestions are offered. The context menu also allows words to be learnt. To invoke the Context Menu use Right Click or Control Click. This context menu also launches the Mac’s dictionary and thesaurus, and even links to Wikipedia, or the Google search engine. To run spell check across a document go to Edit > Spelling >Spelling… The spell check dialogue box appears. Automatic Corrections can be controlled from Keynotes Preferences. For example typing ‘teh’ will automatically change to ‘the’. Also, scientific terms or business names can be abbreviated. Here typing ‘mwp’ will automatically change to Mark Wood | Photography. Genuine fractions can be achieved but only with certain font types. Once enabled Automatic Corrections, Symbol and Text Substitution will replace 1/3 with ⅓, but only if the chosen font contains glyphs for fractions. If the chosen font does not contain the required glyph an alternative font is used. Photographs, Movies and Sound can be added to a Keynote document in one of four different ways. 1. The Media Browser 2. The Insert menu and Choose… 3. Dragging files from Finder 4. Copying and Pasting from other applications The hassle free method is to use Media Browser, as it connects to other Apple software including iTunes. The available media will be in a Keynote compatible format. To include files from the Media Browser locate the correct tab for the media type, either Photos, Movies, Audio, then find the desired file and drag it on to a Keynote slide. Using the Insert command from the application menu bypasses the need to have files included in the Media Browser, but be aware that not all picture, movie and audio formats are supported by iWork and Keynote. If a file appears greyed out in the Finder Import window it means that file format is not supported. Supported file formats are: For Pictures – all QuickTime-supported formats, including TIFF, GIF, JPEG, PDF, PSD, EPS, PICT For Movies and Audio - any QuickTime or iTunes file type, including MOV, MP3, MPEG-4, AIFF and AAC Supported files can be simply dragged from a Finder window onto a Slide. If the contents of an entire folder are required, drag the folder from its Finder window onto the Media Browser. For a folder of mixed media, being Pictures, Movies and Audio this operation will have to be executed three times. Once for each tab of the Media Browser.  If images placed onto slides look too dark or lack contrast Keynote has an Adjust Image control. This is a window that can be used to adjust the brightness, contrast, saturation and other image parameters directly within Keynote. There are several image parameters that can be used to finesse imported photographs, though often just clicking the Enhance button will improve a picture. Adjust Image is an icon in the Toolbar, or it can be launched from View > Adjust. By default Keynote’s preferences are set to include audio and movies in the document file. This option, found in the General tab of Keynote’s Preferences, should be left ticked; its default. This ensures media is saved within Keynote documents. There is no mechanism for excluding picture data from automatically being saved in Keynote. Because Keynote embeds media files, presentations can become rather bloated, filling hard drive space. On modern workstations this is not a big problem but for the fast transmission of a Keynote via a network, file size can jeopardize smooth playback. All iWork applications have a Reduce File Size command found in the File menu. Use this to resample media files contained in Keynote. Caution: do not overwrite, that is save over, the original Keynote file. Using Reduce File Size may reduce the fidelity of media during resampling. This may not be an issue, but keeping the higher quality Keynote is a sensible precaution. Global Transitions can be added to slides. They help to indicate a change is taking place. The Dissolve Effect set to two or three seconds makes a pleasing transition from one picture slide to another. With so many transitions it is all too easy to overload a presentation with effects, and so obscure the intended message. Often transitions are triggered by a click, though they can also be set to change after a time delay. Mixing click-triggered transitions with time-delayed transitions can be confusing to a presenter. It is probably best to use click-triggering and no delays because often when a computer appears unresponsive users start clicking freely and frustratedly. ﻿There are six table presets; each providing an insight into the variety of Cell Formats. Controls for Cells can be explored by right–clicking or Control–Clicking a Cell or Cells. The six table types are: • Headers – This includes one header column and one header row. Headers are used to label columns and rows. Headers are automatically formatted so that they stand out from other cells in a Table. They are always the topmost row or the first column on the left of a Table. Numbers supports up to five header rows and columns. Multiple headers are useful when assigning names to two or more header columns or rows. • Basic – is similar to the Headers preset except that it does not contain a row header. • Sums – like Basic, this has columns headers, no row headers, but includes a Footer Row. The Footer Row cells contain a SUM formula which totals any number values entered into the columns. • Plain – is a simple grid of Cells with no headers or formula added. • Checklist – is a useful example of Cell Formatting. Column A contains Checkboxes. So if making, for instance, a Do List once a task has been completed it can be simply ticked to indicate its completion. Other types of Cell Format are Stepper, Slider and Pop-up Menu. Cells can be formatted for Numbers, Currency, Percentage, Date & Time, Duration, Fraction, Numeral System, Scientific and Text. • Sums Checklist – is identical to Checklist save that the first column contains checkboxes. Cells are formatted as Automatic when they are created. This means that data can be entered without the user having to consider which format to use. Numbers allows users to change cell formatting depending on their project requirements. The types of Cell Format are Numbers, Currency, Percentage, Date & Time, Duration, Fraction, Numeral System, Scientific, and Text. There are also, Stepper, Slider and Pop-up Menu formats. To manually format a cell, column or row, select it, then in the Cells Inspector click on the drop down menu and change the formatting accordingly. In addition users can make custom formats. This option is also found in the drop down menu of the Cells Inspector. If a date is entered into a Cell, for example 04/04/2012, Numbers will automatically read this as a date and will display it as such; 4th April 2012. If this is not the desired date format, select that cell, column or row and in the Cells Inspector use the Date menu to change the formatting. The options found in the Cells Inspector can also be found on the Format Bar. When entering data into Cells, to move the insertion point to the next Cell in the row press TAB. To move the insertion point to the cell below press Return. Use the Arrow keys to move the insertion point freely. A series is a range of linked values. For example, months or weekdays where February follows January, Tuesday follows Monday. Numbers understands such series. To prove this point try typing January into a column. Select that cell and click-drag on the bottom right hand corner of the cell and, moving the cursor down the column, the column automatically sequences the rows with all the months of the year. The same process can be applied to weekdays. If a date is entered into the top body cell of a Column, for example 01/01/12, a date sequence can be created. By clickdragging on the bottom right hand corner of the cell and moving the cursor down the column, the column automatically sequences the rows as following days; 2nd, 3rd, 4th January and so on. Brilliant, but this can be frustrating if, for instance, a series based on weeks is required. The way to achieve this is to manually enter the date information for the first two months of your series, say 04/04/2012 and 04/05/12. Then select the row for April, hold down the Shift key and select May, both rows are now selected. Click–drag the bottom right hand corner of the May cell and the date series now runs in months. In the illustration Column A was created using consecutive days, 04/04/12 was typed into Cell A1 then dragged to fill Column A. In Column B 04/04/12 was typed into Cell B1, then 11/04/12 into Cell B2 before both these Cells were selected and dragged to fill Column B. Column C is a series based on the first day of the month. Column D illustrates the drag operation, and is a series based on years. Use the Cell Inspector to change how the dates are displayed. Other series might include a dinner menu. Typing Aperitif, Starters, Main, Deserts, Coffee into a successive Rows of a Column then performing the Shift–Clicking and Dragging operation described above will repeat those five elements down the column ad nauseam. And there’s more. Hovering the cursor over the reference tab for the date column reveals a triangle, clicking on this invokes a menu. That menu has an option ‘Categorize by This Column’. If this is selected the months are automatically divided into years. If the sequence was stepping-up in days then ‘Categorize by This Column’ would divide the column into months. Numbers is full of features like this. There is not space to explore all the permutations here. The key to understanding lies in understanding how Cells are automatically formatted and how this formatting can be ‘trained’ to behave in new ways. Using the menus that appear on the reference tabs, users can sort columns and rows. Typically data is sorted by Column, for example by date ascending. Sometimes more sophisticated sort options are required, these can be accessed via the reference tab menus and are labelled, ‘Show More Options’. In the example Column B, Fruit Type is being sorted to display in alphabetical order. The following example shows a fictitious Table for a green-grocer’s fruit order. This and the previous illustration contain the same data, but the second Table has been categorized by Column.$$$﻿There are six table presets; each providing an insight into the variety of Cell Formats. Controls for Cells can be explored by right–clicking or Control–Clicking a Cell or Cells. The six table types are: • Headers – This includes one header column and one header row. Headers are used to label columns and rows. Headers are automatically formatted so that they stand out from other cells in a Table. They are always the topmost row or the first column on the left of a Table. Numbers supports up to five header rows and columns. Multiple headers are useful when assigning names to two or more header columns or rows. • Basic – is similar to the Headers preset except that it does not contain a row header. • Sums – like Basic, this has columns headers, no row headers, but includes a Footer Row. The Footer Row cells contain a SUM formula which totals any number values entered into the columns. • Plain – is a simple grid of Cells with no headers or formula added. • Checklist – is a useful example of Cell Formatting. Column A contains Checkboxes. So if making, for instance, a Do List once a task has been completed it can be simply ticked to indicate its completion. Other types of Cell Format are Stepper, Slider and Pop-up Menu. Cells can be formatted for Numbers, Currency, Percentage, Date & Time, Duration, Fraction, Numeral System, Scientific and Text. • Sums Checklist – is identical to Checklist save that the first column contains checkboxes. Cells are formatted as Automatic when they are created. This means that data can be entered without the user having to consider which format to use. Numbers allows users to change cell formatting depending on their project requirements. The types of Cell Format are Numbers, Currency, Percentage, Date & Time, Duration, Fraction, Numeral System, Scientific, and Text. There are also, Stepper, Slider and Pop-up Menu formats. To manually format a cell, column or row, select it, then in the Cells Inspector click on the drop down menu and change the formatting accordingly. In addition users can make custom formats. This option is also found in the drop down menu of the Cells Inspector. If a date is entered into a Cell, for example 04/04/2012, Numbers will automatically read this as a date and will display it as such; 4th April 2012. If this is not the desired date format, select that cell, column or row and in the Cells Inspector use the Date menu to change the formatting. The options found in the Cells Inspector can also be found on the Format Bar. When entering data into Cells, to move the insertion point to the next Cell in the row press TAB. To move the insertion point to the cell below press Return. Use the Arrow keys to move the insertion point freely. A series is a range of linked values. For example, months or weekdays where February follows January, Tuesday follows Monday. Numbers understands such series. To prove this point try typing January into a column. Select that cell and click-drag on the bottom right hand corner of the cell and, moving the cursor down the column, the column automatically sequences the rows with all the months of the year. The same process can be applied to weekdays. If a date is entered into the top body cell of a Column, for example 01/01/12, a date sequence can be created. By clickdragging on the bottom right hand corner of the cell and moving the cursor down the column, the column automatically sequences the rows as following days; 2nd, 3rd, 4th January and so on. Brilliant, but this can be frustrating if, for instance, a series based on weeks is required. The way to achieve this is to manually enter the date information for the first two months of your series, say 04/04/2012 and 04/05/12. Then select the row for April, hold down the Shift key and select May, both rows are now selected. Click–drag the bottom right hand corner of the May cell and the date series now runs in months. In the illustration Column A was created using consecutive days, 04/04/12 was typed into Cell A1 then dragged to fill Column A. In Column B 04/04/12 was typed into Cell B1, then 11/04/12 into Cell B2 before both these Cells were selected and dragged to fill Column B. Column C is a series based on the first day of the month. Column D illustrates the drag operation, and is a series based on years. Use the Cell Inspector to change how the dates are displayed. Other series might include a dinner menu. Typing Aperitif, Starters, Main, Deserts, Coffee into a successive Rows of a Column then performing the Shift–Clicking and Dragging operation described above will repeat those five elements down the column ad nauseam. And there’s more. Hovering the cursor over the reference tab for the date column reveals a triangle, clicking on this invokes a menu. That menu has an option ‘Categorize by This Column’. If this is selected the months are automatically divided into years. If the sequence was stepping-up in days then ‘Categorize by This Column’ would divide the column into months. Numbers is full of features like this. There is not space to explore all the permutations here. The key to understanding lies in understanding how Cells are automatically formatted and how this formatting can be ‘trained’ to behave in new ways. Using the menus that appear on the reference tabs, users can sort columns and rows. Typically data is sorted by Column, for example by date ascending. Sometimes more sophisticated sort options are required, these can be accessed via the reference tab menus and are labelled, ‘Show More Options’. In the example Column B, Fruit Type is being sorted to display in alphabetical order. The following example shows a fictitious Table for a green-grocer’s fruit order. This and the previous illustration contain the same data, but the second Table has been categorized by Column.
EN08	0	﻿There’s an old joke, well known in database circles, to the effect that what users really want (and always have wanted, ever since database systems were first invented) is for somebody to implement the go faster! command. Well, I’m glad to be able to tell you that, as of now, somebody finally has ... This book is all about a radically new database implementation technology, a technology that lets us build database management systems (DBMSs) that are “blindingly fast”—certainly orders of magnitude faster than any previous system. As explained in the preface, that technology is known as The TransRelationaltm Model, or the TR model for short (the terms TR technology and, frequently, just TR are also used). As also explained in the preface, the technology is the subject of a United States patent (U.S. Patent No. 6,009,432, dated December 28th, 1999), listed as reference [63] in Appendix B at the back of this book; however, that reference is usually known more specifically as the Initial Patent, because several follow-on patent applications have been applied for at the time of writing. This book covers material from the Initial Patent and from certain of those follow-on patents as well. The TR model really is a breakthrough. To say it again, it allows us to build DBMSs that are orders of magnitude faster than any previous system. And when I say “any previous system,” I don’t just mean previous relational systems. It’s an unfortunate fact that many people still believe that the fastest relational system will never perform as well as the fastest nonrelational system. Indeed, it’s exactly that belief that accounts in large part for the continued existence and use of older, nonrelational systems such as IMS [25,57] and IDMS [14,25], despite the fact that—as is well known—relational systems are far superior from the point of view of usability, productivity, and the like. However, a relational system implemented using TR technology should dramatically outperform even the fastest of those older nonrelational systems, finally giving the lie to those old performance arguments and making them obsolete (not before time, either). I must also make it clear that I don’t just mean that queries should be faster under TR (despite the traditional emphasis in relational systems on queries in particular)—updates should be faster as well. Nor do I mean that TR is suitable only for decision support systems—it’s eminently suitable for transaction processing systems, too (though it’s probably fair to say that TR is particularly suitable for systems in which read-only operations predominate, such as data warehouse and data mining systems). And one last preliminary remark: You’re probably thinking that the performance advantages I’m claiming must surely come at a cost: perhaps poor usability, or less functionality, or something (there’s no free lunch, right?). Well, I’m pleased to be able to tell you that such is not the case. The fact is, TR actually provides numerous additional benefits, over and above the performance benefit—for example, in the areas of database and system administration. Thus, I certainly don’t want you to think that performance is the only argument in favor of TR. We’ll take a look at some of those additional benefits in Chapters 2 and 15, and elsewhere in passing. (In fact, a detailed summary of all of the TR benefits appears in Chapter 15, in Section 15.4. You might like to take a quick look at that section right now, just to get an idea of how much of a breakthrough the TR model truly is.) As I said in the preface, I believe TR technology is one of the most significant advances—quite possibly the most significant advance—in the data management field since E. F. Codd first invented the relational model (which is to say, since the late 1960s and early 1970s; see references [5 7], also reference [35]). As I also said in the preface, TR represents among other things a highly effective way to implement the relational model, as I hope to show in this book. In fact, the TR model—or, rather, the more general technology of which the TR model is just one specific but important manifestation—represents an effective way to implement data management systems of many different kinds, including but not limited to the following: ■■SQL DBMSs ■■Data warehouse systems ■■Information access tools ■■Data mining tools ■■Object/relational DBMSs ■■Web search engines ■■Main-memory DBMSs ■■Temporal DBMSs ■■Business rule systems ■■Repository managers ■■XML document storage and retrieval systems ■■Enterprise resource planning tools as well as relational DBMSs in particular. Informally, we could say we’re talking about a backend technology that’s suitable for use with many different frontends. In planning this book, however, I quickly decided that my principal focus should be on the application of the technology to implementing the relational model specifically. Here are some of my reasons for that decision: ■Concentrating on one particular application should make the discussions and examples more concrete and therefore, I hope, easier to follow and understand. ■■More significantly, the relational model is of fundamental importance; it’s rock solid, and it will endure. After all, it really is the best contender, so far as we know, for the role of “proper theoretical foundation” for the entire data management field. One hundred years from now, I fully expect database systems still to be firmly based on Codd’s relational model—even if they’re advertised as “object/relational,” or “temporal,” or “spatial,” or whatever. See Chapter 15 for further discussion of such matters. ■■If your work involves data management in any of its aspects, then you should already have at least a nodding acquaintance with the basic ideas of the relational model. Though I feel bound to add that if that “nodding acquaintance” is based on a familiarity with SQL specifically, then you might not know as much as you should about the model as such, and you might know “some things that ain’t so.” I’ll come back to this point in a few moments. ■■The relational model is an especially good fit with TR ideas; I mean, it’s a very obvious candidate for implementation using those ideas. Why? Because the relational model is at a uniform, and high, level of abstraction; it’s concerned purely with what a database system is supposed to look like to the user, and has absolutely nothing to say about what the system might look like internally. As many people would put it, the relational model is logical, not physical. ﻿The main purpose of this chapter is to explain in more detail some of the problems that arise in connection with what the lawyers call “prior art”—meaning, in the case at hand, systems that use the traditional direct-image approach to implementation. Of course, you can skip this material if you’re already familiar with conventional implementation technology. However, this first section does also introduce a few simple relational ideas, and you might at least want to make sure you’re familiar with those and fully understand them. Consider Fig. 2.1, which depicts a relation called S (“suppliers”). Observe that each supplier has a supplier number (S#), unique to that supplier;1 a supplier name (SNAME), not necessarily unique (though in fact the sample names shown in the figure do happen to be unique); a rating or status value (STATUS); and a location (CITY). I’ll use this example to remind you of a few of the most fundamental relational terms and concepts. ■First of all, a relation can, obviously enough, be pictured as a table. However, a relation is not a table.2 A picture of a thing isn’t the same as the thing! In fact, the difference between a thing and a picture of that thing is another of the great logical differences (see the remarks on this latter notion in Chapter 1, near the beginning of Section 1.3). One problem with thinking of a relation as a table is that it suggests that certain properties of tables—for example, the property that the rows are in a certain top-to-bottom order—apply to relations too, when in fact they don’t (see below). ■■Each of the five suppliers is represented by a tuple (pronounced as noted in Chapter 1 to rhyme with “couple”). Tuples are depicted as rows in figures like Fig. 2.1, but tuples aren’t rows. ■■Each supplier tuple contains four values, called attribute values; that is, the suppliers relation involves four attributes, called S#, SNAME, STATUS, and CITY. Attributes are depicted as columns in figures like Fig. 2.1, but attributes aren’t columns. ■■Attributes are defined over data types (types for short, also known as domains), meaning that every value of the attribute in question is required to be a value of the type in question. Types can be either system-defined (built in) or user-defined. For example, attribute STATUS might be defined over the system-defined type INTEGER (STATUS values are integers), while attribute SNAME might be defined over the user-defined type NAME (SNAME values are names). Note: For definiteness, I’ll assume these specific types throughout what follows, where it makes any difference. I’ll also assume that attribute S# is defined over a user-defined type with the same name (that is, S#), and attribute CITY is defined over the system-defined type CHAR (meaning character strings of arbitrary length). ■■The tuples of a relation are all distinct. In fact, relations never contain duplicate tuples—the tuples of a relation form a mathematical set, and sets in mathematics don’t contain duplicate elements. Note: People often complain about this aspect of the relational model, but in fact there are good practical reasons for not permitting duplicate tuples. A detailed discussion of the point is beyond the scope of this book; see any of references [13], [20], or [33] if you want to pursue the matter. ■■There’s no top-to-bottom ordering to the tuples of a relation. Although figures like Fig. 2.1 clearly suggest there is such an ordering, there really isn’t—to say it again, the tuples of a relation form a mathematical set, and sets in mathematics have no ordering to their elements. Note: It follows from this point that we could draw several different pictures that would all represent the same relation. An analogous remark applies to the point immediately following. ■There’s no left-to-right ordering to the attributes of a relation. Again, figures like Fig. 2.1 clearly suggest there is such an ordering, but there really isn’t; like the tuples, the attributes of a relation form a set, and thus have no ordering. (By the same token, there’s no left-to-right ordering to the components of a tuple, either.) No relation can have two or more attributes with the same name. ■■The suppliers relation is in fact a base relation specifically. In general, we distinguish between base and derived relations; a derived relation is one that is derived from, or defined in terms of, other relations, and a base relation is one that isn’t derived in this sense. Loosely speaking, in other words, the base relations are the “given” ones—they’re the ones that make up the actual database—while the derived ones are views, snapshots, query results, and the like [33]. For example, given the base relation of Fig. 2.1, the result of the query “Get suppliers in London” is a derived relation that looks like this: Another way to think about the distinction is that base relations exist in their own right, while derived ones don’t—they’re existence-dependent on the base relations. ■■Every relation has at least one candidate key (or just key for short), which serves as a unique identifier for the tuples of that relation. In the case of the suppliers relation (and the derived relation just shown as well), there’s just one key, namely {S#}, but relations can have any number of keys, in general. Note: It’s important to understand that keys are always sets of attributes (though the set in question might well contain just a single attribute). For this reason, in this book I’ll always show key attributes enclosed in braces, as in the case at hand—braces being used by convention to bracket the elements that make up a set. ■■As you probably know, it’s customary (though not obligatory) to choose, for any given relation, one of that relation’s candidate keys—possibly its sole candidate key—as primary; thus, for example, we might say in the case of the suppliers relation that {S#} is not just a key but the “primary” key. In figures like Fig. 2.1, I’ll follow the convention of identifying primary key attributes by double underlining. ■■Finally, relations can be operated on by a variety of relational operators. In general, a relational operator is an operator that takes zero or more relations as input and produces a relation as output. Examples include the well-known operators restrict, project, join, and so on. ﻿In order to understand the TR approach to implementing the relational model, it’s necessary to be very clear over three distinct levels of the system, which I’ll refer to as the three levels of abstraction (since each level is an abstraction of the one below, loosely speaking). The three levels, or layers, are: 1. The relational (or user) level 2. The file level 3. The TR level ■Level 1, which corresponds to the database as seen by the user, is the relational level. At this level, the data is perceived as relations, including, perhaps, the suppliers relation S discussed in Section 2.1 (and illustrated in Fig. 2.1) in the previous chapter. ■■Level 3 is the fundamental TR implementation level. At this level, data is represented by means of a variety of internal structures called tables. Please note immediately that those TR tables are NOT tables in the SQL sense and do NOT correspond directly to relations at the user level. ■Level 2 is a level of indirection between the other two. Relations at the user or relational level are mapped to files at this level, and those files are then mapped to tables at the TR level. Of course, the mappings go both ways; that is, tables at the TR level map to files at the next level up, and those files then map to relations at the top level. Note: As I’m sure you know, map is a synonym for transform (and I’ll be using the term in that sense throughout this book); thus, we’re already beginning to touch on the TR transforms that were mentioned in Chapter 1. However, there’s a great deal more to it, as we’ll soon see. Please now observe that each level has its own terminology: relational terms at the user level, file terms at the file level, and table terms at the TR level. Using different terms should, I hope, help you keep the three levels distinct and separate in your mind; for that reason, I plan to use the three sets of terms consistently and systematically throughout the rest of this book. Having said that, I now need to say too that I’m well aware that some readers might object to my choice of terms—perhaps even find them confusing—for at least the following two reasons: ■■First, the industry typically uses the terminology of tables, not relations, at the user level—almost exclusively so, in fact. But I’ve already explained some of my rationale for wanting to use relational terms at that level (see the previous chapter, Section 2.1), and I’m going to give some additional reasons in the next section. ■Second, the industry also typically tends to think of files as a fairly “physical” construct. In fact, I did the same thing myself in the previous chapter, somewhat, though I was careful in that chapter always to be quite clear that the files I was talking about were indeed physically stored files specifically. By contrast, the files I’ll be talking about in the rest of the book are not physically stored; instead, they’re an abstraction of what’s physically stored, and hence a “logical” construct, not a physical one. (Though it wouldn’t be wrong to think of them as “slightly more physical” than the user-level relations, if you like.) If you still think my terms are confusing, then I’m sorry, but for better or worse they’re the terms I’m going to use. One final point: When I talk of three levels, or layers, of abstraction, I don’t mean that each of those levels is physically materialized in any concrete sense—of course not. The relational level is only a way of looking at the file level, a way in which certain details are ignored (that’s what “level of abstraction” means). Likewise, the file level in turn is only a way of looking at the TR level. Come to that, the TR level in turn is only a way of looking at the bits and bytes that are physically stored; that is, the TR level is itself—as already noted in Chapter 1, Section 1.2—still somewhat abstract. In a sense, the bits-and-bytes level is the only level that’s physically materialized.1 Since the focus of this book is on the use of TR technology to implement the relational model specifically, the topmost (user) level is relational by definition. In other words, the user sees the database as a set of relations, made up of attributes and tuples as explained in Chapter 2. For simplicity, I’m going to assume those relations are all base relations specifically (again, see Chapter 2); that is, I’ll simply assume, barring explicit statements to the contrary, that any relation that’s named and is included in the database is in fact a base relation specifically, and I won’t usually bother to use the “base” qualifier. Also, of course, the user at the relational level has available a set of relational operators—restrict, project, join, and so forth—for querying the relations in the database, as well as the usual INSERT, DELETE, and UPDATE operators for updating them. Note: If I wanted to be more precise here, I’d have to get into the important distinction between relation values and relation variables. Relational operators like join operate on relation values, while update operators like INSERT operate on relation variables. Informally, however, it’s usual to call them all just relations, and—somewhat against my better judgment—I’ve decided to follow that common usage (for the most part) in the present book. For further discussion of such matters, see either reference [32] or reference [40]. Now, given the current state of the IT industry, the user level in a real database system will almost certainly be based on SQL, not on the relational model. As a consequence, users will typically tend to think, not in terms of relational concepts as such, but rather in terms of SQL analogs of those concepts. For example, there isn’t any explicit project operator, as such, in SQL; instead, such an operation has to be formulated in terms of SQL’s SELECT and FROM operators, and the user has to think in terms of those SQL operators, as in this example (“Project suppliers over supplier number and city name”): ﻿Now (at last) I can begin to explain the TR model in detail. As I mentioned several times in Part I, TR is indeed still a model, and thus, like the relational model, still somewhat abstract. At the same time, however, it’s at a much lower level of abstraction than the relational model; it can be thought of as being closer to the physical implementation level (“closer to the metal”), and accordingly more oriented toward issues of performance. In particular, it relies heavily on the use of pointers—a concept deliberately excluded from the relational model, of course, for reasons discussed in references [9], [30], [40], and many other places—and its operators are much more procedural in nature than those of the relational model. (What I mean by this latter remark is that code that makes use of those operators is much more procedural than relational code is, or is supposed to be.) What’s more, reference [63] includes detailed, albeit still somewhat abstract, algorithms for implementing those operators. Note: These remarks aren’t meant to be taken as criticisms, of course; I’m just trying to capture the essence of the TR model by highlighting some of its key features. Despite its comparatively low-level nature, the fact remains that, to say it again, TR is indeed a model, and thus capable of many different physical realizations. In what follows, I’ll talk for much of the time in terms of just one possible realization—it’s easier on the reader to be concrete and definite—but I’ll also mention some alternative implementation schemes on occasion. Note that the alternatives in question have to do with the implementation of both data structures and corresponding access algorithms. In particular, bear in mind that both main-memory and secondary-storage implementations are possible. Now, this book is meant to be a tutorial; accordingly, I want to focus on showing the TR model in action (as it were)—that is, showing how it works in terms of concrete examples—rather than on describing the abstract model as such. Also, many TR features are optional, in the sense that they might or might not be present in any given implementation or application of the model, and it’s certainly not worth getting into all of those optional features in a book of this kind. Nor for the most part is it worth getting into the optionality or otherwise of those features that are discussed—though I should perhaps at least point out that options do imply a need for decisions: Given some particular option X, some agency, at some time, has to decide whether or not X should be exercised. For obvious reasons, I don’t want to get into a lot of detail on this issue here, either. Suffice it to say that I don’t think many of those decisions, if any at all, should have to be made at database design time (by some human being) or at run time (by the system itself); in fact, I would expect most of them to be made during the process of designing the DBMS that is the specific TR implementation in question. In other words, I don’t think the fact that those decisions do have to be made implies that a TR implementation will therefore suffer from the same kinds of problems that arise in connection with direct-image systems, as discussed in Chapter 2. It follows from all of the above that this book is meant as an introduction only; many topics are omitted and others are simplified, and I make no claims of completeness of any kind. Now let’s get down to business. In this chapter and the next,1 we’ll be looking at what are clearly the most basic TR constructs of all: namely, the Field Values Table and the Record Reconstruction Table, both of which were mentioned briefly in the final section of the previous chapter. These two constructs are absolutely fundamental—everything else builds on them, and I recommend as strongly as I can that you familiarize yourself with their names and basic purpose before you read much further. Just to remind you: ■■The Field Values Table contains the field values from a given file, rearranged in a way to be explained in Section 4.3. ■■The Record Reconstruction Table contains information that allows records of the given file to be reconstructed from the Field Values Table, in a way to be explained in Section 4.4. In subsequent chapters I’ll consider various possible refinements of those core concepts. Note: Those refinements might be regarded in some respects as “optional extras” or “frills,” but some of them are very important—so much so, that they’ll almost certainly be included in any concrete realization of the TR model, as we’ll see. Let r be some given record within some given file at the file level. Then the crucial insight underlying the TR model can be characterized as follows: The stored form of r involves two logically distinct pieces, a set of field values and a set of “linkage” information that ties those field values together, and there’s a wide range of possibilities for physically storing each piece. In direct-image systems, the two pieces (the field values and the linkage information) are kept together, of course; in other words, the linkage information in such systems is represented by physical contiguity. In TR, by contrast, the two pieces are kept separate; to be specific, the field values are kept in the Field Values Table, and the linkage information is kept in the Record Reconstruction Table. That separation makes TR strikingly different from virtually all previous approaches to implementing the relational model (see Chapters 1 and 2), and is the fundamental source of the numerous benefits that TR technology is capable of providing. In particular, it means that TR data representations are categorically not a direct image of what the user sees at the relational level. Note: One immediate advantage of the separation is that the Field Values Table and the Record Reconstruction Table can both be physically stored in a way that is highly efficient in terms of storage space and access time requirements. However, we’ll see many additional advantages as well, both in this chapter and in subsequent ones. ﻿This chapter continues our examination of the core constructs of the TR model (principally the Field Values and Record Reconstruction Tables). However, the chapter is rather more of a potpourri than the previous one. Its structure is as follows. Following this short introductory section, Section 5.2 offers some general observations regarding performance. Section 5.3 then briefly surveys the TR operators, and Sections 5.4 and 5.5 take another look at how the Record Reconstruction Table is built and how record reconstruction is done. Sections 5.6 and 5.7 describe some alternative perspectives on certain of the TR constructs introduced in Chapter 4. Finally, Section 5.6 takes a look at some alternative ways of implementing some of the TR structures and algorithms also first described in that previous chapter. It seems to me undeniable that the mechanisms described in the previous chapter for representing and reconstructing records and files are vastly different from those found in conventional DBMSs, and I presume you agree with this assessment. At the same time, however, they certainly look pretty complicated ... How does all of that complexity square with the claims I made in Chapter 1 regarding good performance? Let me remind you of some of the things I said there: Well, let me say a little more now regarding query performance specifically (I haven’t really discussed updates yet, so I’ll have to come back to the question of update performance later—actually in the next chapter). Now, any given query involves two logically distinct processes: a) Finding the data that’s required, and then b) Retrieving that data. TR is designed to exploit this fact. Precisely because it separates field value information and linkage information, it can treat these two processes more or less independently. To find the data, it uses the Field Values Table; to retrieve it, it uses the Record Reconstruction Table. (These characterizations aren’t 100 percent accurate, but they’re good to a first approximation—good enough for present purposes, at any rate.) And the Field Values Table in particular is designed to make the finding of data very efficient (for example, via binary search), as we saw in Chapter 4. Of course, it’s true that subsequent retrieval of that data then involves the record reconstruction process, and this latter process in turn involves a lot of pointer chasing, but: ■■Even in a disk-based implementation, the system will do its best to ensure that pertinent portions of both the Field Values Table and the Record Reconstruction Table are kept in main memory at run time, as we’ll see in Part III. Assuming this goal is met, the reconstruction will be done at main-memory speeds. ■■The “frills” to be discussed in Chapters 7 9 (as well as others that are beyond the scope of this book) have the effect, among other things, of dramatically improving the performance of various aspects of the reconstruction process. ■■Most important of all: Almost always, finding the data that’s wanted is a much bigger issue than returning that data to the user is. In a sense, the design of the TR internal structures is biased in favor of the first of these issues at the expense of the second. Observe the implication: The more complex the query, the better TR will perform—in comparison with traditional approaches, that is. (Of course, I don’t mean to suggest by these remarks that record reconstruction is slow or inefficient—it isn’t—nor that TR performs well on complex queries but not on simple ones. I just want to stress the relative importance of finding the data in the first place, that’s all.) I’d like to say more on this question of query performance. In 1969, in his very first paper on the relational model [5], Codd had this to say: Once aware that a certain relation exists, the user will expect to be able to exploit that relation using any combination of its attributes as “knowns” and the remaining attributes as “unknowns,” because the information (like Everest) is there. This is a system feature (missing from many current information systems) which we shall call (logically) symmetric exploitation of relations. Naturally, symmetry in performance is not to be expected. —E. F. Codd Note: I’ve reworded Codd’s remarks just slightly here. In particular, the final sentence (the caveat concerning performance) didn’t appear in the original 1969 paper [5] but was added in the expanded 1970 version [6]. Anyway, the point I want to make is that the TR approach gives us symmetry in performance, too—or, at least, it comes much closer to doing so than previous approaches ever did. This is because, as we saw in Chapter 4, the separation of field values from linkage information effectively allows the data to be physically stored in several different sort orders simultaneously. When Codd said “symmetry in performance is not to be expected,” he was tacitly assuming a direct-image style of implementation, one involving auxiliary structures like those described in Chapter 2. However, as I said in that chapter: [Auxiliary structures such as pointer chains and] indexes can be used to impose different orderings on a given file and thus (in a sense) “level the playing field” with respect to different processing sequences; all of those sequences are equally good from a logical point of view. But they certainly aren’t equally good from a performance point of view. For example, even if there’s a city index, processing suppliers in city name sequence will involve (in effect) random accesses to storage, precisely because the supplier records aren’t physically stored in city name sequence but are scattered all over the disk. —from Chapter 2 As we’ve seen, however, these remarks simply don’t apply to the TR data representation. And now I can address another issue that might possibly have been bothering you. We’ve seen that the TR model relies heavily on pointers. Now, the CODASYL “network model” [14,25] also relies heavily on pointers—as the “object model” [3,4,28,29] and “hierarchic model” [25,56] both do also, as a matter of fact—and I and many other writers have criticized it vigorously in the past on exactly that score (see, for example, references [10], [21], and [37]). So am I arguing out of both sides of my mouth here? How can TR pointers be good while CODASYL pointers are bad? ﻿You have already met constraints, in type definitions (Chapter 2), where they are used to define the set of values constituting a type. The major part of this chapter is about database constraints. Database constraints express the integrity rules that apply to the database. They express these rules to the DBMS. By enforcing them, the DBMS ensures that the database is at all times consistent with respect to those rules. In Chapter 1, Example 1.3, you saw a simple example of a database constraint declaration expressed in Tutorial D, repeated here as Example 6.1 (though now referencing IS_ENROLLED_ON rather than ENROLMENT). The first line tells the DBMS that a constraint named MAX_ENROLMENTS is being declared. The second line gives the expression to be evaluated whenever the DBMS decides to check that constraint. This particular constraint expresses a rule to the effect that there can never be more than 20000 enrolments altogether. It is perhaps an unrealistic rule and it was chosen in Chapter 1 for its simplicity. Now that you have learned the operators described in Chapters 4 and 5 you have all the equipment you need to express more complicated constraints and more typical ones. This chapter explains how to use those operators for that purpose. Now, if a database is currently consistent with its declared constraints, then there is clearly no need for the DBMS to test its consistency again until either some new constraint is declared to the DBMS, or, more likely, the database is updated. For that reason, it is also appropriate in this chapter to deal with methods of updating the database, for it is not a bad idea to think about which kinds of constraints might be violated by which kinds of updating operations, as we shall see. A constraint is defined by a truth-valued expression, such as a comparison. A database constraint is defined by a truth-valued expression that references the database. To be precise, the expression defines a condition that must be satisfied by the database at all times. We have previously used such terminology in connection with tuples in relational restriction for example, which yields a relation containing just those tuples of a given relation that satisfy the given condition. We can justify the use of the terminology in connection with database constraints by considering the database valuexv at any particular point in time to be a tuple. The attributes of this tuple take their names and declared types from the variables constituting the database and their values are the values of those variables. Taking this view, the database itself is a tuple variable and every successful update operation conceptually assigns a tuple value to that variable, even if it actually assigns just one relation value to one relation variable, leaving the other relvars unchanged. What do we really mean when we say that the DBMS must ensure that the database is consistent at all times? Internally, the DBMS might have to perform several disk writes to complete what is perceived by the user as a single update operation, but intermediate states arising during this process are visible to nobody.xvi Because those intermediate states are invisible, we can state that if the database is guaranteed to be consistent immediately following completion of each single statement that updates it, then it will be consistent whenever it is visible. We say therefore that, conceptually at least, constraints are checked at all statement boundaries, and only at statement boundaries we don’t care about the consistency of intermediate states arising during the DBMS’s processing of a statement because those states aren’t visible to us in any case. To clarify “all statement boundaries”, first, note that this includes statements that are contained inside other statements, such as IF … THEN … ELSE … constructs for example. Secondly, the conceptual checking need not take place at all for a statement that does no updating, but no harm is done to our model if we think of constraints as being checked at every statement boundary. In Tutorial D, as in many computer languages, a statement boundary is denoted by a semicolon, so we can usefully think of constraints as being effectively checked at every semicolon. If all the constraints are satisfied, then the updates brought about by the statement just completed are accepted and made visible; on the other hand, if some constraint is not satisfied, then the updates are rejected and the database reverts to the value it had immediately after the most recent successful statement execution. We can usually expect a database to be subject to quite a few separately declared constraints. To say that the database must satisfy all of the conditions specified by these constraints is equivalent to saying that it must satisfy the single condition that is the conjunction of those individually specified conditions the condition formed by connecting them all together using logical AND. We can conveniently refer to the resulting condition as the database constraint. Now we can state the principle governing correct maintenance of database integrity by the DBMS quite succinctly: the database constraint is guaranteed to be satisfied at every statement boundary. The condition for a database constraint must reference the database and therefore must mention at least one variable in that database. In the case of relational databases, that means that at least one relvar must be mentioned. Moreover, as the condition is specified by a single expression (a truth-valued expression), it must use relational operators if it involves more than one relvar and, as we shall soon see, is likely to use them even when it involves just one relvar. However, a relation isn’t a truth value, so we need some of the non-relational operators described in Chapter 5, in addition to the relational operators, to express conditions for declared constraints. In particular, the expression itself must denote an invocation of some truth-valued operator. In Example 6.1 that operator is “=”. No relational operators are used in that example, because the only relation we need to operate on is the one that is the value of the relvar IS_ENROLLED_ON when the constraint is checked. The aggregate operator COUNT operates on that relation to give its cardinality, an integer.$$$﻿You have already met constraints, in type definitions (Chapter 2), where they are used to define the set of values constituting a type. The major part of this chapter is about database constraints. Database constraints express the integrity rules that apply to the database. They express these rules to the DBMS. By enforcing them, the DBMS ensures that the database is at all times consistent with respect to those rules. In Chapter 1, Example 1.3, you saw a simple example of a database constraint declaration expressed in Tutorial D, repeated here as Example 6.1 (though now referencing IS_ENROLLED_ON rather than ENROLMENT). The first line tells the DBMS that a constraint named MAX_ENROLMENTS is being declared. The second line gives the expression to be evaluated whenever the DBMS decides to check that constraint. This particular constraint expresses a rule to the effect that there can never be more than 20000 enrolments altogether. It is perhaps an unrealistic rule and it was chosen in Chapter 1 for its simplicity. Now that you have learned the operators described in Chapters 4 and 5 you have all the equipment you need to express more complicated constraints and more typical ones. This chapter explains how to use those operators for that purpose. Now, if a database is currently consistent with its declared constraints, then there is clearly no need for the DBMS to test its consistency again until either some new constraint is declared to the DBMS, or, more likely, the database is updated. For that reason, it is also appropriate in this chapter to deal with methods of updating the database, for it is not a bad idea to think about which kinds of constraints might be violated by which kinds of updating operations, as we shall see. A constraint is defined by a truth-valued expression, such as a comparison. A database constraint is defined by a truth-valued expression that references the database. To be precise, the expression defines a condition that must be satisfied by the database at all times. We have previously used such terminology in connection with tuples in relational restriction for example, which yields a relation containing just those tuples of a given relation that satisfy the given condition. We can justify the use of the terminology in connection with database constraints by considering the database valuexv at any particular point in time to be a tuple. The attributes of this tuple take their names and declared types from the variables constituting the database and their values are the values of those variables. Taking this view, the database itself is a tuple variable and every successful update operation conceptually assigns a tuple value to that variable, even if it actually assigns just one relation value to one relation variable, leaving the other relvars unchanged. What do we really mean when we say that the DBMS must ensure that the database is consistent at all times? Internally, the DBMS might have to perform several disk writes to complete what is perceived by the user as a single update operation, but intermediate states arising during this process are visible to nobody.xvi Because those intermediate states are invisible, we can state that if the database is guaranteed to be consistent immediately following completion of each single statement that updates it, then it will be consistent whenever it is visible. We say therefore that, conceptually at least, constraints are checked at all statement boundaries, and only at statement boundaries we don’t care about the consistency of intermediate states arising during the DBMS’s processing of a statement because those states aren’t visible to us in any case. To clarify “all statement boundaries”, first, note that this includes statements that are contained inside other statements, such as IF … THEN … ELSE … constructs for example. Secondly, the conceptual checking need not take place at all for a statement that does no updating, but no harm is done to our model if we think of constraints as being checked at every statement boundary. In Tutorial D, as in many computer languages, a statement boundary is denoted by a semicolon, so we can usefully think of constraints as being effectively checked at every semicolon. If all the constraints are satisfied, then the updates brought about by the statement just completed are accepted and made visible; on the other hand, if some constraint is not satisfied, then the updates are rejected and the database reverts to the value it had immediately after the most recent successful statement execution. We can usually expect a database to be subject to quite a few separately declared constraints. To say that the database must satisfy all of the conditions specified by these constraints is equivalent to saying that it must satisfy the single condition that is the conjunction of those individually specified conditions the condition formed by connecting them all together using logical AND. We can conveniently refer to the resulting condition as the database constraint. Now we can state the principle governing correct maintenance of database integrity by the DBMS quite succinctly: the database constraint is guaranteed to be satisfied at every statement boundary. The condition for a database constraint must reference the database and therefore must mention at least one variable in that database. In the case of relational databases, that means that at least one relvar must be mentioned. Moreover, as the condition is specified by a single expression (a truth-valued expression), it must use relational operators if it involves more than one relvar and, as we shall soon see, is likely to use them even when it involves just one relvar. However, a relation isn’t a truth value, so we need some of the non-relational operators described in Chapter 5, in addition to the relational operators, to express conditions for declared constraints. In particular, the expression itself must denote an invocation of some truth-valued operator. In Example 6.1 that operator is “=”. No relational operators are used in that example, because the only relation we need to operate on is the one that is the value of the relvar IS_ENROLLED_ON when the constraint is checked. The aggregate operator COUNT operates on that relation to give its cardinality, an integer.
EN20	0	﻿The way an agent behaves is often used to tell them apart and to distinguish what and who they are, whether animal, human or artificial. Behaviour can also be associated with groups of agents, not just a single agent. For example, human cultural behaviour relates to behaviour that is associated with a particular nation, people or social group, and is distinct from the behaviour of an individual human being or the human body. Behaviour also has an important role to play in the survival of different species and subspecies. It has been suggested, for example, that music and art formed part of a suite of behaviours displayed by our own species that provided us with the evolutionary edge over the Neanderthals.  In the two preceding chapters, we have talked about various aspects concerning behaviours of embodied, situated agents, such as how an agent’s behaviour from a design perspective can be characterised in terms of its movement it exhibits in an environment, and how agents exhibit a range of behaviours from reactive to cognitive. We have not, however, provided a more concrete definition of what behaviour is. From the perspective of designing embodied, situated agents, behaviour can be defined as follows. A particular behaviour of an embodied, situated agent is a series of actions it performs when interacting with an environment. The specific order or manner in which the actions’ movements are made and the overall outcome that occurs as a result of the actions defines the type of behaviour. We can define an action as a series of movements performed by an agent in relation to a specific outcome, either by volition (for cognitive-based actions) or by instinct (for reactive-based actions). With this definition, movement is being treated as a fundamental part of the components that characterise each type of behaviour – in other words, the actions and reactions the agent executes as it is performing the behaviour. The distinction between a movement and an action is that an action comprises one or more movements performed by an agent, and also that there is a specific outcome that occurs as a result of the action. For example, a human agent might wish to perform the action of turning a light switch on. The outcome of the action is that the light gets switched on. This action requires a series of movements to be performed such as raising the hand up to the light switch, moving a specific finger up out of the hand, then using that finger to touch the top of the switch, then applying pressure downwards until the switch moves. The distinction between an action and a particular behaviour is that a behaviour comprises one or more actions performed by an agent in a particular order or manner. For example, an agent may prefer an energy saving type of behaviour by only switching lights on when necessary (this is an example of a cognitive type of behaviour as it involves a conscious choice). Another agent may always switch on the light through habit as it enters a room (this is an example of a mostly reactive type of behaviour). Behaviour is the way an agent acts in a given situation or set of situations. The situation is defined by the environmental conditions, its own circumstances and the knowledge the agent currently has available to it. If the agent has insufficient knowledge for a given situation, then it may choose to search for further knowledge about the situation. Behaviours can be made up of sub-behaviours. The search for further knowledge is itself a behaviour, for example, and may be a component of the original behaviour. There are also various aspects to behaviour, including the following: sensing and movement (sensory-motor co-ordination); recognition of the current situation (classification); decision-making (selection of an appropriate response); performance (execution of the response).  Behaviours range from the fully conscious (cognitive) to the unconscious (reactive), from overt (done in an open way) to covert (done in a secretive way), and from voluntary (the agent acts according to its own free will) to involuntary (done without conscious control or done against the will of the agent). The term ‘behaviour’ also has different meanings depending on the context (Reynolds, 1987). The above definition is applicable when the term is being used in relation to the actions of a human or animal, but it is also applicable in describing the actions of a mechanical system, or the complex actions of a chaotic system, if the agent-oriented perspective is considered (here the agents are humans, animals, mechanical systems or complex systems). However, in virtual reality and multimedia applications, the term can sometimes be used as a synonym for computer animation. In the believable agents and artificial life fields, behaviour is used “to refer to the improvisational and life-like actions of an autonomous character” (Reynolds, 1987). We also often anthropomorphically attribute human behavioural characteristics with how a computer operates when we say that a computer system or computer program is behaving in a certain way based on responses to our interaction with the system or program. Similarly, we often (usually erroneously) attribute human behavioural characteristics with animals and inanimate objects such as cars.  In this section, we will further explore the important distinction between reactive and cognitive behaviour that was first highlighted in the previous chapter. Agents can be characterised by where they sit on a continuum as shown in Figure 6.1. This continuum ranges from purely reactive agents that exhibit no cognitive abilities (such as ants and termites), to agents that exhibit cognitive behaviour or have an ability to think. Table 6.1 details the differences between the two types of agents. In reality, many agents exhibit both reactive and cognitive behaviours to varying degrees, and the distinction between reactive and cognitive can be arbitrary.  Comparing the abilities of reactive agents with cognitive agents listed in Table 6.1, it is clear that reactive agents are very limited in what they can do as they do not have the ability to plan, co-ordinate between themselves or set and understand specific goals; they simply react to events when they occur. This does not preclude them from having a role to play in producing intelligent behaviour. The reactive school of thought is that it is not necessary for agents to be individually intelligent. However, they can work together collectively to solve complex problems. ﻿Communication may be defined as the process of sharing or exchanging of information between agents. An agent exhibits communicating behaviour when it attempts to transmit information to another agent. A sender agent or agents transmits a message through some medium to a receiver agent or agents. The term communication in common English usage can also refer to interactions between people that involve the sharing of information, ideas and feelings. Communication is not unique to humans, though, since animals and even plants also have the ability to communicate with each other. Language can be defined as the set of symbols that agents communicate with in order to convey information. In Artificial Intelligence, human language is often called ‘natural language’ in order to distinguish it from computer programming languages. Communicating using language is often considered to be a uniquely human behavioural trait. Human language, such as spoken, written or sign, is distinguished from animal communication systems in that it is learned rather than inherited biologically. Although various animals exhibit the ability to communicate, and some animals such as orangutans and chimpanzees even have the ability to use certain features of human language, it is the degree of sophistication and complexity in human language that distinguishes it from animal communication systems. Human language is based on the unique ability of humans to think abstractly, using symbols to represent concepts and ideas. Language is defined by a set of socially shared rules that define the commonly accepted symbols, their meaning and their structural relationships specified by rules of grammar. These rules describe how the symbols can be manipulated to create a potentially infinite number of grammatically correct symbol sequences. The specific symbols chosen are arbitrary and can be associated with any particular phoneme, grapheme or sign.  Linguistics is the scientific study of language which can be split into separate areas of study: grammar is the study of language structure; morphology is the study of how words are formed and put together; phonology is the study of systems of sounds; syntax concerns the rules governing how words combine into phrases and sentences; semantics is the study of meaning; and pragmatics concerns the study of language and use and the contexts in which it is used.  In all natural languages, there is a wide variation in usage as well as frequent lack of agreement amongst language users. For example, Table 7.1 lists some examples of acceptable ‘English’ sentences from various regions of the world (Newbrook, 2009). Each of the sentences is regarded as ‘normal’ English for the region shown on the right, and yet most people outside those regions would argue differently, and in many cases have difficulty in understanding their meaning.  Concerning the English language, David Crystal (1988) states: “The English language stretches around the world: from Australia to Zimbabwe, over 300 million people speak it as their mother tongue alone… And yet, despite its astonishingly widespread use as a medium of communication, every profession and every province – indeed, every individual person – uses a slightly different variant.” English has many different regional dialects (such as American, British, Australian and New Zealand English), as well as many sub-dialects within those regions. There are also dialects that cut across regional lines, for example, “Public School English” in Britain, Black English in America and Maori English in New Zealand. And in every country, there are countless social variations that “possess their own bewildering variety of cants, jargons and lingoes” (Claiborne 1990, page 20). One of the more colourful examples is a dictionary on Wall Street slang entitled High steppers, fallen angels, and lollipops (Odean, 1989). It illustrates how such language can become almost unintelligible to the uninitiated. (For example, what is a ‘high stepper’ or a ‘fallen angel’?) Hudson (1983, page 69) writes the following in The language of the teenage revolution about the resentment of older people to the language used by contemporary teenagers : “… perhaps, they dislike the fact that teenagers speak another kind of language, using a considerable number of expressions which they themselves find either incomprehensible or repulsive.” As well as language being diverse, there are many different ways that language is expressed and used. Spoken language is markedly different from written language, as illustrated from the following example taken from Crystal (1981): “This is part of a lecture, and I chose it because it shows that even a professional speaker uses structure that would rarely if ever occur in written English, and displays a ‘disjointedness’ of speech that would be altogether lacking there. (Everyday conversation provides even more striking differences.) The dot (.) indicates a short pause, the dash a longer pause, and the erm is an attempt to represent the noises the speaker made when he was audibly hesitating. … – and I want . very arbitrarily if I may to divide this into three headings --- and to ask . erm . three questions . assessment why – assessment of what – and assessment how . so this is really . means I want to talk about . first of all the purposes of assessment – why we are assessing at all – erm secondly the kind of functions and processes that are being assessed – and thirdly I want to talk about techniques – …” Baugh (1957, page 17) reminds us that language is not just “the printed page” relatively uniform and fixed, as many people think it to be. Language is “primarily speech” and writing “only a conventional device for recoding sounds.” He further states that as the repeated muscular movements which generate speech are subject to gradual alteration on the part of the speaker: “each individual is constantly and quite unconsciously introducing slight changes in his speech. There is no such thing as uniformity in language. Not only does the speech of one community differ from that of another, but the speech of different individuals of a single community, even different members of the same family, is marked by individual peculiarities.”  Some other distinctive forms of language are, for example, poetry, legal documents, newspaper reporting, advertising, letter writing, office correspondence, telegrams, telephone conversations, electronic mail, Usenet news articles, scientific papers and political speeches. Each has their own flavour, quirks and style. And within each form there are individual authors who have their own distinctive styles of language. The plays of Shakespeare and the science fiction novels of H. G. Wells are two examples of very distinct literary styles. In fact, every single person has their own style of language, or idiolect with its own unique characteristics that can be readily discerned by other people (Fromkin et al., 1990). ﻿It is generally acknowledged in Artificial Intelligence research that search is crucial to building intelligent systems and for the design of intelligent agents. For example, Newell (1994) has stated that search is fundamental for intelligent behaviour (see the quote at the beginning of this chapter). From a behavioural perspective, search can be considered to be a meta-behaviour where the agent is making a decision on which behaviour amongst a set of possible behaviours to execute in a given situation. In other words, it can be defined as the behavioural process that the agent employs in order to make decisions about which choice of actions it should perform in order to carry out a specific task. The task may include higher-level cognitive behaviours such as learning, strategy, goal-setting, planning, and modelling (these were called Action Selection in Reynold’s Boids model). If there are no decisions to be made, then searching is not required. If the agent already knows that a particular set of actions, or behaviour, is appropriate for a given situation, then there is no need to search for the appropriate behaviour, and therefore the actions can be applied without coming to any decision. Searching can be considered to be a behaviour that an agent exhibits when it has insufficient knowledge in order to solve a problem. The problem is defined by the current situation of the agent, which is determined by the environmental conditions, its own circumstances and the knowledge the agent currently has available to it. If the agent has insufficient knowledge in order to solve a given problem, then it may choose to search for further knowledge about the problem. If it already has sufficient knowledge, it will not need to employ searching behaviour in order to solve the problem. An agent uses search behaviour in order to answer a question it does not already know the answer to or complete a task it does not know how to complete. The agent must explore an environment by following more than one path in order to obtain that knowledge.  The exploration of the environment is carried out in a manner analogous to early explorers of the American or Australian continents, or people exploring a garden maze such as the Hampton Court Palace Maze or the Chevening House Maze. For the two garden mazes, the people trying to get to the centre of the maze must employ search behaviour if they do not have the knowledge about where it is. If they take a map with them, however, they no longer have to use search behaviour as the map provides them with the knowledge of which paths to take to get to the centre. The early American explorers Lewis and Clark were instructed by Thomas Jefferson to explore the Missouri and find a water route across the continent to the Pacific. They did not already know how large the continent was or what was out there and needed to physically explore the land. They chose to head in a westerly then north-westerly direction following a path along the Missouri river. Similarly, the Australian explorers Burke and Wills led an expedition starting from Melbourne with the goal of reaching the Gulf of Carpentaria. The land at the time had yet to be explored by European settlers. The expedition were prevented from reaching their ultimate goal just three miles short of the northern coastline due to mangrove swamps and, worse still, the expedition leaders died on the return journey. When performing a search, an agent can adopt different behaviours that determine the way the search is performed. The search behaviour adopted can have a significant impact on the effectiveness of the search. For example, poor leadership was blamed for the unsuccessful Burke and Wills expedition. The behaviour can be thought of as a strategy the agent adopts when performing the search. We can consider these search behaviours from an embodied agent perspective. The type of search behaviour is determined both by the embodiment of the agent, and whether the agent employs a reactive or more cognitive behaviour when executing the search. We have already seen many examples of how an agent can perform a search of an environment as a side effect of employing purely reactive behaviour (see Section 5.4 and Figures 5.2, 5.7 to 5.10 and 6.9). We can term these types of search behaviours as reactive search. We can also use the term cognitive search for cases when an agent has an ability to recognize that there is a choice to be made when a choice presents itself in the environment (such as when the agent reaches a junction in the maze). As stated in section 5.5, this act of recognition is a fundamental part of cognitive-based searching behaviour, and it is related to the situation that the agent finds itself in, the way its body is moving and interacting with the environment. It is also related to what is happening in the environment externally, and/or what is happening with other agents in the same environment if there are any. In addition, a single agent can be used to perform any given search, but there is nothing stopping us from using more than one agent to perform the search. We can adopt a multi-agent perspective to describe how each particular search can be performed in order to clarify how the searches differ. In this case, the effectiveness of a particular search can be evaluated by comparing the number of agents that are needed to perform the search and the amount of information that is communicated between them.  There are many problems that require search with varying degrees of difficulty. Simplified or ‘toy’ problems are problems that are to the most part synthetic and unrelated to real life. However, these problems can be useful to designers in gaining insight into the problem of search. Once the properties of the different search behaviours have been investigated on toy problems, then they can be adapted and/or scaled up to real life problems. To illustrate different search problems, this section describes three problems in particular that have been simulated in NetLogo – these are the Searching Mazes model, the Missionaries and Cannibals model, and the Searching for Kevin Bacon model. ﻿Knowledge is essential for intelligent behaviour. Without knowledge, an intelligent agent cannot make informed decisions, and instead must rely on using some form of searching type behaviour involving exploration and/or communication in order to gain the missing knowledge. Humans rely on knowledge every moment of their life – knowledge of how to communicate with other humans, knowledge of where they and other people live and work, knowledge of where things are, knowledge of how to behave in different situations, knowledge of how to perform different tasks and so on. Without the ability to store and process knowledge, the cognitive abilities of a human is seriously curtailed. An illness such as Alzheimer’s, for example, can be debilitating when memory loss occurs such as the difficulty in remembering recently learned facts. We all know (or think we know) what we mean when we use the term ‘knowledge’. But what exactly is knowledge? Bertrand Russell (1926) acknowledged the difficult question of how to define the meaning of ‘knowledge’: “It is perhaps unwise to begin with a definition of the subject, since, as elsewhere in philosophical discussions, definitions are controversial, and will necessarily differ for different schools”.  A definition of knowledge is the subject of ongoing philosophical debate and presently there are many competing theories with no single definition universally agreed upon. Consequently, treatment of knowledge from an Artificial Intelligence perspective has often consciously avoided the definition of what knowledge is. However, this avoidance of providing a definition of knowledge upfront results in a lack of preciseness in the literature and research. The following argument will illustrate why. A knowledge-based system is a term used in Artificial Intelligence to refer to a system that processes knowledge in some manner. We can make the analogy of a knowledge-based system as being a repository of knowledge, whereas a database is a repository of data. However, in this definition, we have neglected to define the meaning of the term ‘knowledge’ and how it is different to data. For example, we can ask ourselves the following question – “What constitutes a knowledge-based system, and how does it differ from a database system?” This is a difficult question that cannot readily be answered in a straightforward way. A common approach taken in the literature is that a knowledge-based system can perform reasoning using some form of inferencing (whether rule-based, frame-based; see below). Modern database systems, however, now employ most of these standard ‘knowledge-based’ techniques and more. Clearly, the addition of inferencing capabilities alone is not sufficient to define what a knowledge-based system is. However, a great deal of A.I. literature makes such an assumption. By avoiding a definition of knowledge, the problem becomes that it is no longer clear that what we are building really is in fact ‘knowledge-based’. In Chapter 1, it was stated that early A.I. systems in the 1970s and 1980s suffered from a lack of evaluation – there was a rush to build new systems, but often very little evaluation was undertaken of how well the systems worked. Without a working definition of knowledge, the same problem occurs now with current knowledge-based systems – how can we evaluate how effective our knowledge-base system might be if we do not have a definition of what it should be (or even achieve or do)? We can, however, avoid the philosophical pitfalls, and rather than attempting to define knowledge, and making a claim that this definition is the “right” one, instead we can propose design principles for our knowledge-based system. Hence, we can decide what principles we wish our knowledge-based system to adhere to, and we, as designers, are free to change them as we see fit based on knowledge we gain during the design process. Also, we are no longer standing on shaky ground in the sense that we do not have to provide one particular definition of knowledge which is open to philosophical debate, although we are still open to criticism about whether our principles are worthwhile from an engineering perspective (i.e. whether they produce “good” programs, or aren’t as good as other approaches). But evaluation becomes much simpler – all we need to do is evaluate whether our design principles are met.  The following are some design principles for knowledge-based systems.  The argument for this design principle is that if we design from an embodied, situated agent perspective, then all knowledge cannot exist independently of the agents. That is, knowledge cannot exist by itself – it can only be found in the ‘minds’ of the agents that are embodied and situated in an environment. We also wish to define and use the term ‘knowledge’ in a way similar to the way the term is used in natural language. The root of the word ‘knowledge’ comes from the verb “to know”. From a natural language perspective, ‘knowing’ and ‘knowledge’ are related. A rock, for example, does not ‘know’ anything. But a dog can ‘know’ where it has buried a bone; and it makes sense to say in natural language that the dog has ‘knowledge’ of where the bone is buried. The dog, in this case, is the agent, and the rock is an object in the environment. In other words, knowing behaviour is associated with an agent who has knowledge. A knowledge-based system can then be thought of as an agent whose role is to convey the knowledge that it contains to the users of the system. The interaction between the user agent and the system agent can be characterised by the actions that determine each agent’s behaviour, and whether the user agent perceives the system agent to be acting in a knowledgeable way. This leads to the next design principle.  The following design principles are based on properties of ‘good’ knowledge-base systems proposed by Russell and Norvig (2002):  A behavioural approach to knowledge places the emphasis not on building a specific independent system, but on building agents that exhibit behaviour that demonstrates they have knowledge of their environment and of other agents. In this approach, the act of ‘knowing’ occurs when an agent has information that might potentially aid the performance of an action taken by itself or by another agent. Further, an agent can be considered to have ‘knowledge’ if it knows what the likely outcomes will be of an action it may perform, or of an action another agent is performing, or what is likely to happen to an object in the environment. ﻿What is the nature of intelligence? That is a question that has been pondered, and debated for thousands of years. Many people over the centuries have offered their own view on the matter, as illustrated by the quotes provided in Table 10.1.  It seems that everyone has their own opinion on what intelligence is or isn’t. Intelligence is a concept that everyone knows about, but understands differently. As we have seen in the previous chapter, the way each person understands a particular concept will have its own unique ‘flavour’. Perhaps one of the most interesting quotes above is by Susan Sontag that uses an analogy between taste and intelligence. Taste is a complex sensation in four dimensions – sweetness, sourness, bitterness and saltiness. Similarly, intelligence is a complex concept, with multiple dimensions.  Intelligence is multi-faceted – its nature cannot be defined using one of these quotes alone; it requires all of them. As an analogy, try describing the Mona Lisa. One person’s description of the painting may be anathema to another person. To imagine that we can distil the Mona Lisa down to a few written words, and then naïvely believe other people will agree with us that it is the one and only definitive description, is like believing that people should only ever eat one type of food, or enjoy looking at one type of painting, or read one type of book. The Mona Lisa painting continues to inspire people to write more and more words about it. Similarly, intelligence is not something we can elucidate definitively. But that will not stop people from continuing to do so, since in so doing further insights can be gained into its nature. Although definitions of intelligence are fraught with problems, we can look for desirable properties of intelligence that we can help us to describe the nature of intelligence. In other words, we can help define the nature of intelligence by describing what it ‘looks’ like or what it ‘tastes’ like. Using the taste analogy, we can think of these properties as being ‘ingredients’ in a recipe for intelligence – we need to mix them together in order to make a particular taste, which some people will like, while others may not, preferring alternative tastes. For example, we can use the analogy of African and Australian explorers trying to describe what a giraffe or platypus looks like to someone who has never seen it. These explorers will use words (concepts) that they are familiar with, such as ‘long neck’ and ‘fish-like tail’, but their description will be ‘flavoured’ by their own unique perspective. Whatever words they come up with, they will have over-emphasized certain features and ignored other important ingredients.  Similarly, AI researchers with a background in knowledge engineering and the symbolic approach to AI will describe intelligence using ingredients such as the following: • the capacity to acquire and apply knowledge; • the ability to perform reasoning; and • the ability to make decisions and plan in order to achieve a specific goal. AI researchers who prefer a behavioural-based approach will describe the intelligent behaviour of embodied, situated agents using ingredients such as: • the ability to perform an action that an external intelligent agent would deem to be intelligent; • the ability to demonstrate knowledge of the consequences of its actions; and • the ability to demonstrate knowledge of how to influence or change its environment in order to affect outcomes and achieve its goals. If we think of intelligence using an analogy of mapping, as discussed in the previous chapter, then we might use the following ingredients to describe intelligence: • the ability of an embodied, situated agent to map environments, both real and abstract (i.e. recognize patterns to provide useful simplifications and/or characterizations of its environments); • the ability to use maps to navigate around its environments; • the ability to update its maps when it finds they do not fit reality; and • the ability to communicate details of its maps to other agents. It is important to realise, however, that these are not definitive descriptions, just ingredients in alternative recipes for intelligence. In the previous chapters, we have seen various examples (implemented as models in NetLogo) that have demonstrated some of these ingredients. In some respects, these models have exhibited a small degree of intelligence in the sense that if we observed a human agent with the same behaviour, we would deem that to be a sign of intelligence. In the next volume of this book series, we will also see other models that will demonstrate more advanced technologies. It can be argued, however, that these examples show no true intelligence – but of course that depends on your own perspective, and the ingredients with which you choose for your own recipe for intelligence.  In the last chapter, a question was asked about whether it was possible to have knowledge without representation. Similarly, we can ask ourselves the following question: “Is it possible to have intelligence without representation?” In a seminal paper, Rodney Brooks (1991) considered exactly this same question. In another paper (Brooks, 1991), he also considered the related question: “Is it possible to have intelligence without reasoning?” As discussed in the previous chapter, Brooks favours the embodied, situated approach to AI – the sub-symbolic paradigm rather than the classical symbolic paradigm. When he talks about the possibility of intelligence without ‘representation’, he means that an embodied, situated agent does not need to explicitly represent its environment – it can simply react to it. There is no need for the agent to have an explicit knowledge base about the world it is situated in since the agent can directly ‘consult’ it by interacting with it. Brooks goes further and states that intelligence is an emergent property of certain complex systems (see quote at the beginning of this chapter). Brook’s ideas are interesting in that it raises the possibility that, in designing AI systems, we may not have to do all the work ourselves. If we can find the right way of setting up the initial conditions of the system, the system itself will do the work for us, and through self-organisation, intelligence will emerge as a result. Unfortunately, although this idea is very intriguing, no one as yet has figured out how to set up the necessary initial conditions. ﻿Conciseness and accessibility of source code through declarative reading are Prolog’s major strengths. It is therefore relatively easy to appreciate the workings of someone else’s implementation, while it is much harder independently to arrive at one’s own solution to the same problem. In this chapter, we illustrate a practical methodology which is intended to overcome this discrepancy: it is a software development style that is interactive, incremental, exploratory and allows Prolog code to be arrived at in a relatively effortless manner. The task is to write a Prolog predicate rhyme/0 which displays on the screen the well-known nursery rhyme This is the House that Jack Built ([11]): In our implementation of rhyme/0 we want to exploit the rhyme’s repetitive structure and the fact that all essential information is contained in its last verse. We record the last verse in the database by verse/1 as shown in (P-4.1). The rhyme is seen roughly to match the simplified pattern shown in Fig. 4.1. Knowing the rhyme’s last verse and the above structure will allow (up to some finer detail) the rhyme to be fully reconstructed. With a view to a simplified preliminary Prolog implementation, we therefore define the following Prolog fact in the database The first task is now to define a predicate rhyme prel/2 which should enable us to obtain the skeleton rhyme’s structure in the following manner. Taking this as an informal specification of rhyme prel/2 , we want to arrive at its definition by a series of interactive experiments. What could be the least ambitious first step in implementing rhyme prel/2 ? We may for example create a list whose only entry is the last entry of the above list-of-lists. (This will correspond to reproducing the last verse.) This we do by Still interactively, a list comprising the last two entries of the target list-of-lists may be generated by Here we unify T1 with the tail of V and position it in front of V to form the new list (of lists). How do we now generate the next larger list (comprising the last three entries of the target list-of-lists)? We proceed as before except that we assemble R from the entries T2 , T1 and V (in that order!) where T2 is unified with the tail of T1 . One more such step should suffice to appreciate the underlying pattern of interactively generating instances of R . Since our aim is to identify a recursive pattern in the above interactive session, we recast the inputs slightly by observing that [a1, • • • , an−1, an] and [a1|[a2|[a3| • • • |[an−1|[an]] • • • ]] are equivalent representations of the same list. Let’s have a look at the last two queries again. The annotated lists suggest the following pseudocode (using Prolog’s list-notation) for one single recursive step. Notice that by equations (4.1) and (4.2) we may replace the latter by The base case for the recursion is given by A straightforward implementation of the recursive step is by the (auxiliary) predicate rhyme aux/3 in (P-4.2). In the first argument of rhyme aux/3 the most recent version of the rhyme is accumulated; its second argument is a counter which is decremented from an initial value until it reaches unity at which point the third argument is instantiated to the first. It is noteworthy in the definition of rhyme aux/3 that, as a consequence of using the accumulator technique, reference to the more complex case in the recursive step is found in the rule’s body. (In this sense, as opposed to the familiar situation from imperative programming, progression is from right to left.) We find out by an experiment what the counter should be initialized to. It is seen that the second argument of rhyme aux/3 (the counter) will have to be initialized to the length of (what stands for) the last verse. This gives rise to the following first version of the predicate rhyme prel/2 which then behaves as specified on p. 119. Even though the solution thus obtained is perfectly acceptable, there is scope for improvement. Counters are commonly used in imperative programming for verifying a stopping criterion. The corresponding task in declarative programming is best achieved by pattern matching. There is indeed no need for a counter here since the information for when not to apply the recursive step (any more) can be gleaned from the pattern of the first argument of rhyme aux/3 : For the recursion to stop, the head of the list-of-lists (in the first argument) should itself be a list with exactly one entry. (The complete rhyme will have been arrived at when the first verse comprises a single line!) This idea gives rise in (P-4.3) to a new, improved (and more concise) version of the auxiliary predicate, now called rhyme aux/3 . rhyme aux 2/3 behaves as intended: The definition of a second, improved version of the preliminary rhyme predicate now simplifies to To complete the ‘skeleton version’ of the rhyme, we display the above by with the predicate show rhyme/1 defined by There is still scope for further improvement leading to an even more concise version of the auxiliary predicate. We may replace in the definition of rhyme aux 2/2 all occurrences of Head Old by [H|T], say, accounting for the fact that Head Old will be unified with a list. But then, by virtue of the first goal in the body of this rule we may replace all occurrences of Head by T. Subsequently, the first goal may be dropped. Overall, we obtain in (P-4.4) a third, even more concise version of the auxiliary predicate. There is hardly any room for improvement left save perhaps a minor simplification of the first clause. We derive an alternative boundary case by first completing the interactive session from p. 120 and then carrying out one more step: The first query suggests that we are finished if the (partially) completed skeleton rhyme’s head is a single element list; this condition gave rise to the earlier boundary case. On the other hand, in the second query the variable R is unified with a list whose head is empty and whose tail is the full skeleton rhyme.$$$﻿Conciseness and accessibility of source code through declarative reading are Prolog’s major strengths. It is therefore relatively easy to appreciate the workings of someone else’s implementation, while it is much harder independently to arrive at one’s own solution to the same problem. In this chapter, we illustrate a practical methodology which is intended to overcome this discrepancy: it is a software development style that is interactive, incremental, exploratory and allows Prolog code to be arrived at in a relatively effortless manner. The task is to write a Prolog predicate rhyme/0 which displays on the screen the well-known nursery rhyme This is the House that Jack Built ([11]): In our implementation of rhyme/0 we want to exploit the rhyme’s repetitive structure and the fact that all essential information is contained in its last verse. We record the last verse in the database by verse/1 as shown in (P-4.1). The rhyme is seen roughly to match the simplified pattern shown in Fig. 4.1. Knowing the rhyme’s last verse and the above structure will allow (up to some finer detail) the rhyme to be fully reconstructed. With a view to a simplified preliminary Prolog implementation, we therefore define the following Prolog fact in the database The first task is now to define a predicate rhyme prel/2 which should enable us to obtain the skeleton rhyme’s structure in the following manner. Taking this as an informal specification of rhyme prel/2 , we want to arrive at its definition by a series of interactive experiments. What could be the least ambitious first step in implementing rhyme prel/2 ? We may for example create a list whose only entry is the last entry of the above list-of-lists. (This will correspond to reproducing the last verse.) This we do by Still interactively, a list comprising the last two entries of the target list-of-lists may be generated by Here we unify T1 with the tail of V and position it in front of V to form the new list (of lists). How do we now generate the next larger list (comprising the last three entries of the target list-of-lists)? We proceed as before except that we assemble R from the entries T2 , T1 and V (in that order!) where T2 is unified with the tail of T1 . One more such step should suffice to appreciate the underlying pattern of interactively generating instances of R . Since our aim is to identify a recursive pattern in the above interactive session, we recast the inputs slightly by observing that [a1, • • • , an−1, an] and [a1|[a2|[a3| • • • |[an−1|[an]] • • • ]] are equivalent representations of the same list. Let’s have a look at the last two queries again. The annotated lists suggest the following pseudocode (using Prolog’s list-notation) for one single recursive step. Notice that by equations (4.1) and (4.2) we may replace the latter by The base case for the recursion is given by A straightforward implementation of the recursive step is by the (auxiliary) predicate rhyme aux/3 in (P-4.2). In the first argument of rhyme aux/3 the most recent version of the rhyme is accumulated; its second argument is a counter which is decremented from an initial value until it reaches unity at which point the third argument is instantiated to the first. It is noteworthy in the definition of rhyme aux/3 that, as a consequence of using the accumulator technique, reference to the more complex case in the recursive step is found in the rule’s body. (In this sense, as opposed to the familiar situation from imperative programming, progression is from right to left.) We find out by an experiment what the counter should be initialized to. It is seen that the second argument of rhyme aux/3 (the counter) will have to be initialized to the length of (what stands for) the last verse. This gives rise to the following first version of the predicate rhyme prel/2 which then behaves as specified on p. 119. Even though the solution thus obtained is perfectly acceptable, there is scope for improvement. Counters are commonly used in imperative programming for verifying a stopping criterion. The corresponding task in declarative programming is best achieved by pattern matching. There is indeed no need for a counter here since the information for when not to apply the recursive step (any more) can be gleaned from the pattern of the first argument of rhyme aux/3 : For the recursion to stop, the head of the list-of-lists (in the first argument) should itself be a list with exactly one entry. (The complete rhyme will have been arrived at when the first verse comprises a single line!) This idea gives rise in (P-4.3) to a new, improved (and more concise) version of the auxiliary predicate, now called rhyme aux/3 . rhyme aux 2/3 behaves as intended: The definition of a second, improved version of the preliminary rhyme predicate now simplifies to To complete the ‘skeleton version’ of the rhyme, we display the above by with the predicate show rhyme/1 defined by There is still scope for further improvement leading to an even more concise version of the auxiliary predicate. We may replace in the definition of rhyme aux 2/2 all occurrences of Head Old by [H|T], say, accounting for the fact that Head Old will be unified with a list. But then, by virtue of the first goal in the body of this rule we may replace all occurrences of Head by T. Subsequently, the first goal may be dropped. Overall, we obtain in (P-4.4) a third, even more concise version of the auxiliary predicate. There is hardly any room for improvement left save perhaps a minor simplification of the first clause. We derive an alternative boundary case by first completing the interactive session from p. 120 and then carrying out one more step: The first query suggests that we are finished if the (partially) completed skeleton rhyme’s head is a single element list; this condition gave rise to the earlier boundary case. On the other hand, in the second query the variable R is unified with a list whose head is empty and whose tail is the full skeleton rhyme.
EN05	0	﻿This guide explains the processes used to make Keynote documents. Keynote is Apple Inc.’s equivalent to Microsoft’s Powerpoint. Keynote’s strength is its ease of use and its ability to handle a variety of media types, including HD Video. This guide to Keynote is one of three books I have written for Bookboon on iWork. My books on Pages and Numbers complement this one, with some areas of repetition, each guide is designed to stand alone. A great way to learn is to experiment and play. Use this guide to focus your learning on specific areas of Keynote before taking a broad view of the myriad of possibilities for this software. This guide describes ways to assemble and edit content. It does not seek to give advice on presentation methodology, too often business presentations suffer from densely packed slides, with too much text and statistical information squeezed into them; information that is best left for a report. Rather than cram the contents of a report onto a handful of slides, judicious planning will make for compelling presentations. Keynote is not a place to copy and paste all the text of a report. It is far better to highlight and illustrate important points using Keynote’s excellent graphic and animation capabilities. That said, animation and graphic elements should be used sparingly to aid the communication, and not as a distraction to it. Keynote can be used to create dynamic and engaging presentations. Text, images and charts can be arranged with ease. Keynote makes it easy to add audio and video, add transitions between slides, animate data, and then share presentations in a variety of ways. The guide describes software functions and outlines generic examples of the software in use. Further information can be found on Apple’s web pages, or via Apple’s Certified Training Scheme. Regarding keyboard shortcuts. The keyboard shortcuts mentioned in this book will work on International English QWERTY keyboards. For US keyboards the only difference is that Alt key ( ) is called Option ( ). For AZERTY and other language keywords please try the shortcuts, they will probably work. For seasoned Mac users please note that the Apple key is now referred to as the Command key. It is labelled cmd , not . There are three ways to launch Keynote. • Go to your Applications folder. In Finder choose Go > Applications. Open the iWork ’09 folder and double– click the Keynote icon. (Unless you have done a customized iWork installation the iWork folder will be in the Application folder found in the root of your primary hard drive.) • In the Dock, click the Keynote icon. (In Apple’s latest operating system named Lion, a Keynote icon will appear in Launchpad.) • Double–click any Keynote document. Every time you launch Keynote or try to create a new document, Keynote’s Template Chooser appears. The Template Chooser contains Apple designed templates and any Templates created by you. Apple’s Templates can be customized to suit your tastes or to comply with a business’s graphic identity. There will be more on Themes later. The White and Black Themes are good options to experiment with. Keynote has features that are shared with, or are similar to, features found in other Apple applications such as Numbers and Pages. This section lists these features. Understanding features including Inspectors and the Media Browser are essential when learning about any Apple software. To explore the features described in Section 1. launch Keynote and open any Template. When launching Keynote the following message may appear. By default Keynote’s window contains a customizable Tool Bar, a Format Bar, a Slides Pane, and the Slide Canvas. Other panes may be opened, including Master Slides Pane, and Presenter Notes Pane. The Tool Bar contains several icons. These control common functions and will be described later. Note that some Tool Bar Icons are greyed-out meaning they cannot be used. They become active once an Object is selected. Also note that the Tool Bar can be customized to display buttons for commands based on user preference. From left to right: • New – This adds a new slide. The keyboard shortcut is Command – Shift – N. • Play – This starts the presentation in Slideshow, at the selected slide, the keyboard shortcut is Command–Alt–P. To start a Slideshow from the first slide hold down the Alt key and click the Play button in the Tool Bar. • View – changes the View Mode. Options include Navigator, Outline, Slide Only and Light Table. Of these Navigator and Light Table are the most useful when constructing documents. Outline Mode will be discussed further in Section 5.4. View also controls the display of Rulers, Format Bat, Presenter Notes and Master Slides. When developing a presentation to support a speech or lecture using Presenter Notes can prove a great aid to memory. Keynote can be configured to display on two displays simultaneously, one showing slides to an audience and a comprehensive display of Presenter Notes, Next Slide and Time Elapsed or Remaining on the other. Comments can be temporarily hidden from the View Menu. Comments are like virtual sticky notes and can be found in all the iWork applications. • Guides – By default Keynote displays alignment guides to help layout. These appear as yellow lines that indicate whether an object is aligned to the top, bottom, centre or side of another object, or where it is in relation to the canvas. Choosing all four options from Show Guides at Object Center to Show Relative Sizes will give maximum layout feedback help. • Themes – On launching Keynote a Theme Chooser appears. Users can create their own Themes. Having a Theme button in the Tool Bar allows Themes to be swapped in an open document. This function is useful for companies wishing to update presentations to meet their latest visual identity guidelines. A presentation using their properly applied 2008 theme can be updated to their 2012 theme in a single click. • Masters – Themes contain several Master Slides; Title & Subtitle, Title & Bullets, Title & Bullets – 2 Column… et cetera. Use this menu to change slides to different Masters. • Text Box – This adds a simple text box to the slide canvas. It is better to use a slide Master with a Text Box and modify it. This makes the Theme change function work speedily. It isn’t wrong, nor does it expose weaknesses in the Keynote software, to add Text Boxes. Additional Text Boxes cannot be automatically changed when changing Themes.  ﻿When the View is set to Navigator, the pane to the left of the Slide Canvas is labelled Slides. Change the View to Outline and the pane is labelled Outline. The other view modes include Slide Only and Light Table. The Navigator View is probably the best option for creating and editing presentations, as thumbnails of the slide appear in the Slides Pane. This allows thumbnails to be click–dragged into new positions and slides to be duplicated and deleted with ease. Click-dragging a slide in Navigator View will reposition it in the running order. Holding down the Alt key whilst doing this will copy the slide to a new location. As with nearly all keyboard short–cuts the modifier key, in this case the Alt Key, should only be released after the mouse button is released. Other operations can be accessed using right–click on the Slides Pane. Other operations include the ability to skip slides. This is useful when shorter versions of a presentation are required; perhaps a thirty minute slot has been cut to twenty. The Skip function is a safe way to trim a presentation without deleting any data. Skipped slides won’t display as part of a Slide Show, but can be easily un–skipped too; the context menu command for this is Don’t Skip Slide. In teaching scenarios a Keynote can be prepared with extra sections that might be useful in class, but can be set to Skip by default, and only revealed if the teacher thinks it prudent to explore such areas. Skipping slides avoids the need to have several similar Keynote documents on the same subject. So rather than having to update several Keynotes as data changes, just the one document needs updating and the Skip function helps tailor the presentation for a variety of audiences and durations. Only revealed when the View is set to Show Master Slides, the Master Slide Pane appears above the Slides or Outline Pane to the left of the Slide Canvas. From here the current slide’s format can be changed from one Master to another. Simply click the slide or slides to be changed and click on the desired Master. This operation can also be achieved using the Master button in the Tool Bar. The Presenter Notes Pane appears under the Slide Canvas when set from the View menu in the Tool Bar. Presenter notes can be copied and pasted from other applications such as Pages or typed directly into the Presenter Notes Pane. They can be a great aid to memory when making presentations. Presenter Notes can also be included in handouts printed from Keynote. The Inspector is a key feature in Apple’s iLife and iWork software. Users new to Apple Mac software need to embrace the use of the Inspector. Inspectors control nearly all the parameters in iWork software. Numbers, Pages and Keynote have some unique Inspectors and some in common. For instance all the applications have a Document Inspector containing slightly different parameters in each. The Inspector is launched by either clicking the Inspector icon in the Toolbar or View > Show Inspector. Keynote has ten inspector tabs. They are: • Document Inspector – The Document Inspector should be used when first setting up a document. Divided into three tabs. The first tab contains slideshow preferences with options for Looping the slideshow. From here the Presentation mode can be changed, along with the Slide Size. The bottom section is Require Password To Open. For additional security a password can be entered to lock a Keynote document. The password would then be required every-time the document is opened. The next tab is Audio. There are several ways to add audio to a presentation, and using this tab this may not be the most flexible option. The last tab is Spotlight. Completing the fields for Author, Title, Keywords, and Comments is recommended practice. It makes documents easier to search for, especially when using the Mac Spotlight search engine. All of these fields are for metadata. Author and Title are self-explanatory. Adding Keywords helps classify a document. For example a Keynote document designed for a seminar on Apple’s Aperture could have the keywords, ‘Apple’, ‘Aperture’, ‘photography’, ‘digital imaging’ and ’45 minutes’. The list could go on. There are five keywords here. Keywords are denoted by commas, ‘Apple, Aperture, photography’ ‘digital imaging’ and ‘45 minutes’ are all single keywords. Searches can be case-sensitive, but using ‘apple’ and ‘Apple’ as keywords is not strictly necessary. When choosing keywords it is helpful to invoke the spirit of the librarian. A less rigorous form of applying metadata is using the Comments field. Here paragraphs of descriptive text can add be added. In the example of the Apple Aperture Keynote document, the comments field could read, ‘Seminar Presentation for Apple’s Aperture, aimed at the serious amateur and professional audience…’ • Slide Inspector – The Slide Inspector has two tabs, Transition and Appearance. Transitions can be applied to one or more slides at a time. There are currently forty-eight transition effects and something known as Magic Move, located in the Effect menu of the Transition tab. Experiment with the transitions to learn more, try changing the transition duration as this may enhance the effect. Some transitions have a direction, moving from left to right, or top to bottom. When this is the case the Direction menu becomes active. The Start Transition menu is set to On Click by default, meaning that to move the the next slide a mouse click or an alternative to click has to be made†. Transitions can be triggered in other ways, as listed in the Start Transition menu. † Alternatives to a mouse click, include clicking a trackpad, tapping the Spacebar, using a keyboard’s forward arrow key, using a remote control device ﻿The Media Browser appears in many of Apple’s application software. It is a system wide utility that is accessed from within programs such as Keynote. It has a distinct icon displaying a frame of film, a picture frame and two musical notes. This icon appears in different places in different applications. The Media Browser can be launched from the Toolbar or View > Media Browser. It contains three tabs, Audio, Photos and Movies. Audio – This contains tracks from the current user’s iTune account. Garage band projects, these might include voice–overs or podcasts and audio from other applications such as Aperture or Final Cut. Photos – This contains images stored in iPhoto or Aperture is it is installed Movies – This contains video files stored in iMovie, the current user’s Movies Folder, or in iTunes. If Final Cut or Aperture are installed video files from those applications will also appear here. The bottom pane of the Media Browser contains thumbnails of the respective media files. Double–clicking the thumbnail previews the file. In the case of Photos it enlarges the thumbnail to fill the Media Browser pane. As with all iWork applications, Keynote stores graphics and other media within the file itself, with the exception of Fonts. This makes for easy transfer of files from workstation to laptop et cetera. A consequence of this is that iWork documents may have large files sizes though there is an easy remedy in the Reduce File Size command. This will be illustrated in Section 16 — Sharing Your Work. To insert a media file or files select them in the Media Browser and drag them onto the Keynote Slide Canvas. When images are placed into Keynote they may look too dark or lack contrast. Like all iWork applications there is an Adjust Image window that can be used to adjust the brightness, contrast, saturation and other image parameters directly within Keynote. There are several image parameters that can be used to finesse imported photographs though often just clicking the Enhance button will improve a picture. Image Adjust is an icon in the Toolbar, or can be launched from View > Adjust. When you have taken the time to format a picture frame or text box there is no need to repeat all the steps taken when you wish to apply that style to subsequent objects. Styles can be copied and pasted. Start this process by creating two text boxes. Use the Graphics Inspector to stylize one box. Ensure that Text Box is selected then go to Format > Copy Style. Select the second text box and go to Format > Paste Style. The second Text Box will take on the general appearance of the stylized one. There are several methods for preparing presentations. With presentations planning and rehearsal are essential. As previously stated this guide does not aim to explain the art of presentation. Information on presentation creation and delivery can be found on the web with several sites offering Keynote tips and techniques. Suffice to say planning is required before opening Keynote. Keynote can help structure planning processes, once you have a concept in mind. Outline mode is a particularly useful way to list bullet points as hierarchies and sequences when creating a presentation from scratch. There are five stages in the life of a presentation: 1. Planning – What you need to do before opening Keynote and using Keynote in Outline mode. 2. Construction – Using Outline, Navigator and Light Table modes to build and sequence the presentation. 3. Rehearsal — For presentations that are to be delivered to an audience rehearsal is very important. Keynote has a rehearsal mode that can be used to check timings. 4. Delivery — After rehearsals comes the main event often in front of an audience. Being able to stop and jump to different slides is a useful skill. Keynote presentations don’t have to be delivered in person. Using Kiosk mode allows users to navigate through presentation material on their own. 5. Legacy – Making a presentation may be sufficient, though legacy items can be created, such as handouts or narrated versions of the presentation in video form that can be accessed on-demand, by whoever. Keynote ships with forty-four Apple designed themes. Themes are collections of Master Slides, for example, titles slides, bullets, bullets with photos, and so on. Themes have a coordinated design for fonts, colours and style across their respective Master Slides. An Apple Designed Theme may be a perfect starting point for a presentation, though users can generate and save their own themes, or purchase extra themes online. Defining Master Slides and creating custom themes will be explored later. Some Keynote Themes only have two resolutions, 800 x 600 pixels and 1024 x 768 pixels. Newer Themes come in three additional sizes; 1280 x 720, 1920 x 1080†, and 1680 x 1050. Although presentations can be set to playback so that they fill the screen, choosing an option with sufficient resolution at an appropriate aspect ratio is very important. The Theme Chooser contains all the themes currently available including any custom ones you might make. To see previews of all the Master Slides in a Theme, skim the cursor over the Theme’s thumbnail. Before selecting a slide select the correct slide resolution from the Slide SIze Pop–up menu. Master Slides are preset layouts for title slides, bullet point slides, bullets with a picture and so on. Themes are a collection of Master Slides. The Master icon in the toolbar can be used to change one or more slides to a different master layout. The View icon in the toolbar can be used to Show Master Slides. Click dragging Master Slide thumbnails onto existing slides thumbnails changes their master layout. Although many users begin making their Keynote, or Powerpoint, presentations in slide view, Outline view is a great way to structure ideas. iWork’s Pages has several Outline templates to help with this task. Outline view is one of several options that can be found under the View menu. If the default Outline view is not suitable it can be adjusted. To make the Slide pane areas bigger move the cursor over the resize handle and click drag to adjust its size. ﻿Spreadsheet tables can be created, complete with calculations, directly in Keynote. Clicking the Table icon in the Toolbar places a basic three column by three row table onto a slide. The Table Inspector can be used to modify that Table adding extra rows and columns as needed. The Table Inspector be used to format the Table. However in most circumstances spreadsheet data is best copied and pasted into slides from iWork’s Numbers. As Numbers is a dedicated spreadsheet package it offers far more data editing and function control than Keynote. Having Table creation control in Keynote is great though the primary reason for having this capability is to allow table and chart data to be modified within Keynote. For instance, if sales figures change immediately before a business presentation, the table and chart data can be changed in Keynote without the need to locate the original spreadsheet data. Clicking on any Table Cell that contains a formula invokes the Formula Editor. Once active the Formula Editor can be used to modify equations. If Numbers is not available, create a simple table, select a cell and press the equals (=) key. In common with the other iWork applications Keynote runs automatic spell checking. To switch this off go to Edit > Spelling > Check Spelling As You Type and mark sure it is un–ticked. When on potentially misspelt words are automatically underlined with a red dotted line. Using the context menu spelling suggestions are offered. The context menu also allows words to be learnt. To invoke the Context Menu use Right Click or Control Click. This context menu also launches the Mac’s dictionary and thesaurus, and even links to Wikipedia, or the Google search engine. To run spell check across a document go to Edit > Spelling >Spelling… The spell check dialogue box appears. Automatic Corrections can be controlled from Keynotes Preferences. For example typing ‘teh’ will automatically change to ‘the’. Also, scientific terms or business names can be abbreviated. Here typing ‘mwp’ will automatically change to Mark Wood | Photography. Genuine fractions can be achieved but only with certain font types. Once enabled Automatic Corrections, Symbol and Text Substitution will replace 1/3 with ⅓, but only if the chosen font contains glyphs for fractions. If the chosen font does not contain the required glyph an alternative font is used. Photographs, Movies and Sound can be added to a Keynote document in one of four different ways. 1. The Media Browser 2. The Insert menu and Choose… 3. Dragging files from Finder 4. Copying and Pasting from other applications The hassle free method is to use Media Browser, as it connects to other Apple software including iTunes. The available media will be in a Keynote compatible format. To include files from the Media Browser locate the correct tab for the media type, either Photos, Movies, Audio, then find the desired file and drag it on to a Keynote slide. Using the Insert command from the application menu bypasses the need to have files included in the Media Browser, but be aware that not all picture, movie and audio formats are supported by iWork and Keynote. If a file appears greyed out in the Finder Import window it means that file format is not supported. Supported file formats are: For Pictures – all QuickTime-supported formats, including TIFF, GIF, JPEG, PDF, PSD, EPS, PICT For Movies and Audio - any QuickTime or iTunes file type, including MOV, MP3, MPEG-4, AIFF and AAC Supported files can be simply dragged from a Finder window onto a Slide. If the contents of an entire folder are required, drag the folder from its Finder window onto the Media Browser. For a folder of mixed media, being Pictures, Movies and Audio this operation will have to be executed three times. Once for each tab of the Media Browser.  If images placed onto slides look too dark or lack contrast Keynote has an Adjust Image control. This is a window that can be used to adjust the brightness, contrast, saturation and other image parameters directly within Keynote. There are several image parameters that can be used to finesse imported photographs, though often just clicking the Enhance button will improve a picture. Adjust Image is an icon in the Toolbar, or it can be launched from View > Adjust. By default Keynote’s preferences are set to include audio and movies in the document file. This option, found in the General tab of Keynote’s Preferences, should be left ticked; its default. This ensures media is saved within Keynote documents. There is no mechanism for excluding picture data from automatically being saved in Keynote. Because Keynote embeds media files, presentations can become rather bloated, filling hard drive space. On modern workstations this is not a big problem but for the fast transmission of a Keynote via a network, file size can jeopardize smooth playback. All iWork applications have a Reduce File Size command found in the File menu. Use this to resample media files contained in Keynote. Caution: do not overwrite, that is save over, the original Keynote file. Using Reduce File Size may reduce the fidelity of media during resampling. This may not be an issue, but keeping the higher quality Keynote is a sensible precaution. Global Transitions can be added to slides. They help to indicate a change is taking place. The Dissolve Effect set to two or three seconds makes a pleasing transition from one picture slide to another. With so many transitions it is all too easy to overload a presentation with effects, and so obscure the intended message. Often transitions are triggered by a click, though they can also be set to change after a time delay. Mixing click-triggered transitions with time-delayed transitions can be confusing to a presenter. It is probably best to use click-triggering and no delays because often when a computer appears unresponsive users start clicking freely and frustratedly. ﻿ An intelligence strategy is needed for business intelligence. Business intelligence is a process of taking large amounts of data, analyzing that data, and presenting a high-level set of reports that condense the essence of that data into the basis of business actions. Business intelligence can enable management to gain new insights and thereby contributing to their business decisions to prevent computer crime and to strengthen corporate reputation.  Traditionally, intelligence was understood to mean information from criminals about criminal activity by a covert source. Today, intelligence is a systematic approach to collecting information with the purpose, for example, of tracking and predicting crime to improve law enforcement (Brown et al., 2004). Intelligence analysts investigate who is committing crimes, how, when, where and why. They then provide recommendations on how to stop or curb the offences. As part of this, analysts produce profiles of crime problems and individual targets, and produce both strategic (overall, long-term) and tactical (specific, short-term) assessments within the confines set by the policing unit.  The aim of intelligence strategy is to continue to develop intelligence led policing in all parts of an organization, a nation or in all regions of the world. An intelligence strategy provides a framework for a structured problem solving and partnership enhanced approach, based around a common model. For example, the National Intelligence Model in the UK is a structured approach to improve intelligence led policing both centrally and locally in policing districts such as the South Yorkshire Police (SYPIS, 2007). Intelligence-led policing is carried out in many law enforcement areas. For example, intelligence-led vehicle crime reduction was carried out in the West Surrey police area in the UK. Analysis of vehicle crime included identifying (Brown et al., 2004): • Locations (hotspots, streets, car parks, postcodes, wards, etc.) of vehicle crime, • Sites where vehicles were dumped, • Times of offences, • Prolific vehicle crime offenders, • Areas where prolific offenders were identified as offending, • Models of vehicles targeted for vehicle crime, • Type of property stolen in theft from vehicle offences. The analysis resulted in problem profiles, which identified emerging patterns of crime. These patterns included vehicle crime occurring in beauty spot car parks and the theft of badges from cars. Such information was disseminated to local officers to act on. Intelligence-led policing is defined as a business model and a management philosophy according to Ratcliffe (2008: 89): Intelligence-led policing is a business model and managerial philosophy where data analysis and crime intelligence are pivotal to an objective, decision-making framework that facilitates crime and problem reduction, disruption and prevention through both strategic management and effective enforcement strategies that target prolific and serious offenders.  An interesting case of intelligence-led policing in the UK was the project called "Operation Gallant" that lead to a reduction of 17% in car thefts. Operation Gallant involved all Basic Command Unit (BCU) in the collection and analysis of information (Brown et al., 2004: 2): In the case of Operation Gallant, the intelligence-led vehicle crime reduction approach involved the activity of officers from across a BCU. A crime analyst, dedicated solely to examine vehicle crime patterns and trends, developed a detailed picture of vehicle crime in the area, including analysis of time, location, vehicle type and known offenders. As a result of this strategic analysis, a number of interventions were planned, drawing heavily upon the Operation Igneous tactical menu. The most significant, in terms of resources devoted to the operation, involved a program of prolific offender targeting and crime prevention advice targeted towards the owners of high-risk vehicles. The substantial decline in car crimes were explained by the increased attention paid to this crime sector (Brown et al., 2004: 16): Given the fact that the first reduction coincides with the commencement of the planning process for Operation Gallant, this may also reflect an anticipatory effect in which the very act of planning and talking about an operation leads to a decline.  In intelligence work for investigating and preventing white-collar crime, a variety of information sources are available. Sheptycki (2007) list the following information sources in policing for general corporate social responsibility work: victim reports, witness reports, police reports, crime scene examinations, historical data held by police agencies (such as criminal records), prisoner debriefings, technical or human surveillance products, suspicious financial transactions reporting, and reports emanating from undercover police operations. Similarly, internal investigation units in business organizations can apply intelligence sources. Intelligence analysis may also refer to governmental records of other governmental departments and agencies, and other more open sources of information may be used in elaborate intelligence assessment. Most of the information used to prevent and investigate financial crime is sensitive, complex, and the result of time consuming tasks (Wilhelmsen, 2009). However, Sheptycki (2007) found that most crime analysis is organized around existing investigation and prevention sector data. Intelligence analysis is typically framed by already existing institutional ways of thinking. He argues that organized crime notification, classification and measurement schemes tend to reify pre-existing notions of traditional policing practice. In this perspective, it is important for strategic criminal analysts to be aware of the variety of information sources available. We choose to classify information sources into the following categories in this book: 1. Interview. By means of interrogation of witnesses, suspects, reference persons and experts, information is collected on crimes, criminals, times and places, organizations, criminal projects, activities, roles, etc.  2. Network. By means of informants in the criminal underworld as well as in legal businesses, information is collected on actors, plans, competitors, markets, customers, etc. Informants often have connections with persons that are an investigating colleague would not be able to approach formally. 3. Location. By analyzing potential and actual crime scenes and potential criminal scenes, information are collected on criminal procedures, preferences, crime evolution, etc. Hot spots and traces are found. Secret ransacking of suspicious places is part of this information source. Pictures in terms of crime scene photographs are important information elements. 4. Documents. By studying documents from confiscations may provide information on ownership, transactions, accounts, etc. An example is forensic accounting, which is the application of accounting tasks for an evidentiary purpose. Forensic accounting is the action of identifying, recording, settling, extracting, sorting, reporting and verifying past financial data or other accounting activities for settling current or prospective legal disputes or using such past financial data for projecting future financial data to settle legal disputes. Forensic accountants are essential to the legal system, providing expert services such as fake invoicing valuations, suspicious bankruptcy valuations, and analysis of financial documents in fraud schemes (Curtis, 2008).$$$﻿ An intelligence strategy is needed for business intelligence. Business intelligence is a process of taking large amounts of data, analyzing that data, and presenting a high-level set of reports that condense the essence of that data into the basis of business actions. Business intelligence can enable management to gain new insights and thereby contributing to their business decisions to prevent computer crime and to strengthen corporate reputation.  Traditionally, intelligence was understood to mean information from criminals about criminal activity by a covert source. Today, intelligence is a systematic approach to collecting information with the purpose, for example, of tracking and predicting crime to improve law enforcement (Brown et al., 2004). Intelligence analysts investigate who is committing crimes, how, when, where and why. They then provide recommendations on how to stop or curb the offences. As part of this, analysts produce profiles of crime problems and individual targets, and produce both strategic (overall, long-term) and tactical (specific, short-term) assessments within the confines set by the policing unit.  The aim of intelligence strategy is to continue to develop intelligence led policing in all parts of an organization, a nation or in all regions of the world. An intelligence strategy provides a framework for a structured problem solving and partnership enhanced approach, based around a common model. For example, the National Intelligence Model in the UK is a structured approach to improve intelligence led policing both centrally and locally in policing districts such as the South Yorkshire Police (SYPIS, 2007). Intelligence-led policing is carried out in many law enforcement areas. For example, intelligence-led vehicle crime reduction was carried out in the West Surrey police area in the UK. Analysis of vehicle crime included identifying (Brown et al., 2004): • Locations (hotspots, streets, car parks, postcodes, wards, etc.) of vehicle crime, • Sites where vehicles were dumped, • Times of offences, • Prolific vehicle crime offenders, • Areas where prolific offenders were identified as offending, • Models of vehicles targeted for vehicle crime, • Type of property stolen in theft from vehicle offences. The analysis resulted in problem profiles, which identified emerging patterns of crime. These patterns included vehicle crime occurring in beauty spot car parks and the theft of badges from cars. Such information was disseminated to local officers to act on. Intelligence-led policing is defined as a business model and a management philosophy according to Ratcliffe (2008: 89): Intelligence-led policing is a business model and managerial philosophy where data analysis and crime intelligence are pivotal to an objective, decision-making framework that facilitates crime and problem reduction, disruption and prevention through both strategic management and effective enforcement strategies that target prolific and serious offenders.  An interesting case of intelligence-led policing in the UK was the project called "Operation Gallant" that lead to a reduction of 17% in car thefts. Operation Gallant involved all Basic Command Unit (BCU) in the collection and analysis of information (Brown et al., 2004: 2): In the case of Operation Gallant, the intelligence-led vehicle crime reduction approach involved the activity of officers from across a BCU. A crime analyst, dedicated solely to examine vehicle crime patterns and trends, developed a detailed picture of vehicle crime in the area, including analysis of time, location, vehicle type and known offenders. As a result of this strategic analysis, a number of interventions were planned, drawing heavily upon the Operation Igneous tactical menu. The most significant, in terms of resources devoted to the operation, involved a program of prolific offender targeting and crime prevention advice targeted towards the owners of high-risk vehicles. The substantial decline in car crimes were explained by the increased attention paid to this crime sector (Brown et al., 2004: 16): Given the fact that the first reduction coincides with the commencement of the planning process for Operation Gallant, this may also reflect an anticipatory effect in which the very act of planning and talking about an operation leads to a decline.  In intelligence work for investigating and preventing white-collar crime, a variety of information sources are available. Sheptycki (2007) list the following information sources in policing for general corporate social responsibility work: victim reports, witness reports, police reports, crime scene examinations, historical data held by police agencies (such as criminal records), prisoner debriefings, technical or human surveillance products, suspicious financial transactions reporting, and reports emanating from undercover police operations. Similarly, internal investigation units in business organizations can apply intelligence sources. Intelligence analysis may also refer to governmental records of other governmental departments and agencies, and other more open sources of information may be used in elaborate intelligence assessment. Most of the information used to prevent and investigate financial crime is sensitive, complex, and the result of time consuming tasks (Wilhelmsen, 2009). However, Sheptycki (2007) found that most crime analysis is organized around existing investigation and prevention sector data. Intelligence analysis is typically framed by already existing institutional ways of thinking. He argues that organized crime notification, classification and measurement schemes tend to reify pre-existing notions of traditional policing practice. In this perspective, it is important for strategic criminal analysts to be aware of the variety of information sources available. We choose to classify information sources into the following categories in this book: 1. Interview. By means of interrogation of witnesses, suspects, reference persons and experts, information is collected on crimes, criminals, times and places, organizations, criminal projects, activities, roles, etc.  2. Network. By means of informants in the criminal underworld as well as in legal businesses, information is collected on actors, plans, competitors, markets, customers, etc. Informants often have connections with persons that are an investigating colleague would not be able to approach formally. 3. Location. By analyzing potential and actual crime scenes and potential criminal scenes, information are collected on criminal procedures, preferences, crime evolution, etc. Hot spots and traces are found. Secret ransacking of suspicious places is part of this information source. Pictures in terms of crime scene photographs are important information elements. 4. Documents. By studying documents from confiscations may provide information on ownership, transactions, accounts, etc. An example is forensic accounting, which is the application of accounting tasks for an evidentiary purpose. Forensic accounting is the action of identifying, recording, settling, extracting, sorting, reporting and verifying past financial data or other accounting activities for settling current or prospective legal disputes or using such past financial data for projecting future financial data to settle legal disputes. Forensic accountants are essential to the legal system, providing expert services such as fake invoicing valuations, suspicious bankruptcy valuations, and analysis of financial documents in fraud schemes (Curtis, 2008).
EN06	1	﻿While there is a study guide (available from Ventus) that focuses largely on objects and their characteristics, it will be instructive to the learner (of the Java programming language) to understand how the concept of an object is applied to their construction and use in Java applications. Therefore, Chapter One (of this guide) introduces the concept of an object from a language-independent point of view and examines the essential concepts associated with object-oriented programming (OOP) by briefly comparing how OOP and non-OOP approach the representation of data and information in an application. The chapter goes on to explain classes, objects and messages and concludes with an explanation of how a class is described with a special diagram known as a class diagram.  Despite the wide use of OOP languages such as Java, C++ and C#, non-OOP languages continue to be used in specific domains such as for some categories of embedded applications. In a conventional, procedural language such as C, data is sent to a procedure for processing; this paradigm of information processing is illustrated in Figure 1.1 below.  The figure shows that the number 4 is passed to the function (SQRT) which is ‘programmed’ to calculate the result and output it (to the user of the procedure). In general, we can think of each procedure in an application as ready and waiting for data items to be sent to them so that they can do whatever they are programmed to do on behalf of the user of the application. Thus an application written in C will typically comprise a number of procedures along with ways and means to pass data items to them.  The way in which OOP languages process data, on the other hand, can be thought of as the inverse of the procedural paradigm. Consider Figure 1.2 below.  In the figure, the data item – the number 4 – is represented by the box (with the label ‘4’ on its front face). This representation of the number 4 can be referred to as the object of the number 4. This simple object doesn’t merely represent the number 4, it includes a button labeled sqrt which, when pressed, produces the result that emerges from the slot labeled return. Whilst it is obvious that the object-oriented example is expected to produce the same result as that for the procedural example, it is apparent that the way in which the result is produced is entirely different when the object-oriented paradigm considered. In short, the latter approach to producing the result 2 can be expressed as follows.  A message is sent to the object to tell it what to do. Other messages might press other buttons associated with the object. However for the present purposes, the object that represents the number 4 is a very simple one in that it has only one button associated with it. The result of sending a message to the object to press its one and only button ‘returns’ another object. Hence in Figure 1.2, the result that emerges from the ‘return’ slot - the number 2 – is an object in its own right with its own set of buttons. Despite the apparent simplicity of the way in which the object works, the question remains: how does it calculate the square root of itself? The answer to this question enshrines the fundamental concept associated with objects, which is to say that objects carry their programming code around with them. Applying this concept to the object shown in Figure 1.2, it has a button which gives access to the programming code which calculates the square root (of the number represented by the object). This amalgam of data and code is further illustrated by an enhanced version of the object shown in Figure 1.3 below.  The enhanced object (representing the number 4) has two buttons: one to calculate the square root of itself – as before - and a second button that adds a number to the object. In the figure, a message is sent to the object to press the second button – the button labeled ‘+’ – to add the object that represents the number 3 to the object that represents the number 4. For the ‘+’ button to work, it requires a data item to be sent to it as part of the message to the object. This is the reason why the ‘+’ button is provided with a slot into which the object representing the number 3 is passed. The format of the message shown in the figure can be expressed as follows.  When this message is received and processed by the object, it returns an object that represents the number 7. In this case, the message has accessed the code associated with the ‘+’ button. The enhanced object can be thought of as having two buttons, each of which is associated with its own programming code that is available to users of the object.  Extrapolating the principal of sending messages to the object depicted in Figure 1.3 gives rise to the notion that an object can be thought of as comprising a set of buttons that provide access to operations which are carried out depending on the details in the messages sent to that object.  In summary: in procedural programming languages, data is sent to a procedure; in an object-oriented programming language, messages are sent to an object; an object can be thought of as an amalgam of data and programming code: this is known as encapsulation.  Whilst the concept of encapsulation is likely to appear rather strange to learners who are new to OOP, working with objects is a much more natural way of designing applications compared to designing them with procedures. Objects can be constructed to represent anything in the world around us and, as such, they can be easily re-used or modified. Given that we are surrounded by things or objects in the world around us, it seems natural and logical that we express this in our programming paradigm.  The next section takes the fundamental concepts explored in this section and applies them to a simple object.  ﻿The aim of Chapter Two is to take the simple class diagram shown at the end of Chapter One and explain how it is translated into Java source code. The code is explained in terms of its attributes, constructor and behaviour and a test class is used to explain how its constructor and behaviour elements are used. Before we embark on our first Java programme, let us recall the class diagram with which we concluded Chapter One. The class diagram is reproduced in Figure 2.1 below, with the omission of the constructor: this is to keep the code simple to begin with. We will replace the constructor in the class diagram and provide code for it later in this chapter. In Figure 2.1, let us be reminded that the qualifier ‘-‘ means private and the qualifier ‘+’ means public. The purpose of these qualifiers will be revealed when we write the code for the class. The next section explains how the information in the class diagram shown in Figure 2.1 is translated into Java source code. Remember that, in general, a class definition declares attributes and defines constructors and behaviour. The Java developer concentrates on writing types called classes, as a result of interpreting class diagrams and other elements of the OOA & D of an application’s domain. The Java developer also makes extensive use of the thousands of classes provided by the originators of the Java language (Sun Microsystems Inc.) that are documented in the Java Applications Programming Interface (API). We have established that classes typically comprise attributes and the behaviour that is used to manipulate these data. Attributes are implemented, in Java, as variables, whose value determines the condition or state of an object of that class and behaviour elements are implemented using a construct known as a method. When a method is executed, it is said to be called or invoked. As has been mentioned earlier, an instance of a class is also called an object, such that, perhaps somewhat confusingly, the terms instance and object are interchangeable in Java. The requirement to create an instance of a class from the definition of the class gives rise to a fundamental question: how do we actually create an instance of a class so that its methods can be executed? We will address this question in this section. One of the components of a class, which we haven’t explained fully so far in the discussion of the Member class, is its constructor. A constructor is used to create or construct an instance of that class. Object construction is required so that the Java run-time environment (JRE) can respond to a call to an object’s constructor to create an actual object and store it in memory. An instance does not exist in memory until its constructor is called; only its class definition is loaded by the (JRE). We will meet the constructor for the Member class later. Broadly, then, we can think of the Java developer as writing Java classes, from which objects can be constructed (by calling their constructors). Classes are to objects as an architect’s plan is to a house, i.e. we can produce many houses from a single plan and we can construct or instantiate many instances from a single template known as a class. Given that objects can communicate with other objects, this gives the developer the means to re-use classes from one application in another application. Therefore, with Java object technology, we can build software applications by combining re-useable and interchangeable objects, some of which can be standardised in terms of their interface. This is probably the single-most important advantage of object-oriented programming (OOP) compared with non-OOP in application development. We are now at the stage when we can translate the class diagram for the Member class into Java source code, often shortened to ‘code’. The code that follows is the class definition of the class named Member but includes only some of the attributes and methods that do not involve object types: this is to keep the example straightforward. The reason for this restriction is that if we were to declare attributes or parameters of the MembershipCard class type in the class Member, as required by the class diagram, the Java compiler would look for the class definition of the class MembershipCard. In order to keep the example straightforward, we will only write the class definition for the class Member for the time being; we will refer to the class definition of the class MembershipCard in a later chapter. Thus, in this section, we will work with a single class that includes only primitive data types; there are no class types included in the simplified class diagram. In order to make the example code even more straightforward, the class diagram is further simplified as shown in the next diagram. The class diagram that we will translate into Java code declares two variables and their corresponding ‘setter’ (or mutator) and ‘getter’ (or accessor) methods, as follows. The reason for the simplification (of the full class diagram) is so that the class definition can be more easily understood, compared to its full definition. In short, we well keep our first Java programme as simple as possible. In the class definition that follows below, ‘ // ‘ is a single-line comment and ‘ /** … */ ‘ is a block comment and, as such, are ignored by the Java compiler. For the purposes of the example, Java statements are written in bold and comments in normal typeface. // Class definition for the class diagram shown in Figure 2.2. Note that the name of // the class starts, by convention, with a capital letter and that it is declared as public. // The first Java statement is the class declaration. Note that the words public and // class must begin with a lower case letter. public class Member { // The class declaration. // Declare instance variables first. Things to note: // String types in Java are objects and are declared as ‘String’, not ‘string’. // The qualifier 'private' is used for variables. // 'String' is a type and 'userName' and ‘password’ are variable names, also // known as identifiers. Thus, we write the following: ﻿In Chapter Two, we see that class attributes are implemented in Java programmes as variables, whose values determine the state of an object. To some extent Chapter Two addresses the question of how we name variables; this question is explored further in this chapter. Chapter Three explores some of the basic elements of the Java language. Given the nature of this guide, it is not the intention to make this chapter exhaustive with respect to all of the basic elements of the Java language. Further details can be found in the on-line Java tutorial. We see in Chapter Two that the two broad categories of Java types are primitives and classes. There are eight of the former and a vast number of classes, including several thousand classes provided with the Java language development environment and an infinitude of classes written by the worldwide community of Java developers. This chapter examines aspects of both categories of types. An identifier is a meaningful name given to a component in a Java programme. Identifiers are used to name the class itself – where the name of the class starts with an upper case letter – and to name its instances, its methods and their parameters. While class identifiers always – by convention – start with an upper case letter, everything else is identified with a word (or compound word) that starts with a lower case letter. Identifiers should be made as meaningful as possible, in the context of the application concerned. Thus compound words or phrases are used in practice. Referring to elements of the themed application, we can use the following identifiers for variables in the Member class: because we wouldn’t name a class membershipCard and spaces are not permitted in identifiers. We could have declared other variables in the class definition as follows: We cannot use what are known as keywords for identifiers. These words are reserved and cannot be used solely as an identifier, but can be used as part of an identifier. Thus we cannot identify a variable as follows: // not permitted because int is a keyword but we could write The table below lists the keywords in the Java language. Java is case-sensitive: this means that we cannot expect the following statement to compile: if we have not previously declared the identifier newint. On the other hand, if we write as the last statement of the getNewInt method, it will compile because the identifier named newInt has been declared previously. Similarly we cannot expect the compiler to recognise identifiers such as the following if they have not been declared before we refer to them later in our code. In one of the declarations in Section 3.2, we declared a variable with the identifier newInt to be of the int type, in the following statement: Let us deconstruct this simple statement from right to left: we declare that we are going to use an identifier named newInt to refer to integer values and ensure that access to this variable is private. This kind of declaration gives rise to an obvious question: what primitive data types are there in the Java language? The list on the next page summarises the primitive data types supported in Java. Before we move on to discuss assignment of actual values to variables, it will be instructive to find out if Java can convert between types automatically or whether this is left to the developer and if compile-time and run-time rules for conversion between types are different. In some situations, the JRE implicitly changes the type without the need for the developer to do this. All conversion of primitive data types is checked at compile-time in order to establish whether or not the conversion is permissible. Consider, for example, the following code snippet: A value of 10.0 is displayed when d is output. Evidently the implicit conversion from an int to a double is permissible. Consider this code snippet: The first statement compiles; this means that the implicit conversion from an int to a double is permissible when we assign a literal integer value to a double. However the second statement does not compile: the compiler tells us that there is a possible loss of precision. This is because we are trying to squeeze, as it were, an eight byte value into a four byte value (see Table 3.2); the compiler won’t let us carry out such a narrowing conversion. On the other hand, if we write: // the cast ( int ) forces d to be an int; we will examine the concept of casting // or explicit conversion later in this section Both statements compile and a value of 10 is displayed when i is output. The general rules for implicit assignment conversion are as follows: a boolean cannot be converted to any other type; a non-boolean type can be converted to another non-boolean type provided that the conversion is a widening conversion; a non-boolean type cannot be converted to another non-boolean type if the conversion is a narrowing conversion. Another kind of conversion occurs when a value is passed as an argument to a method when the method defines a parameter of some other type. For example, consider the following method declaration: The method is expecting a value of a double to be passed to it when it is invoked. If we pass a float to the method when it is invoked, the float will be automatically converted to a double. Fortunately the rules that govern this kind of conversion are the same as those for implicit assignment conversion listed above. The previous sub-section shows that Java is willing to carry out widening conversions implicitly. On the other hand, a narrowing conversion generates a compiler error. Should we actually intend to run the risk of the possible loss of precision when carrying out a narrowing conversion, we must make what is known as an explicit cast. Let us recall the following code snippet from the previous sub-section: Casting means explicitly telling Java to force a conversion that the compiler would otherwise not carry out implicitly. To make a cast, the desired type is placed between brackets, as in the second statement above, where the type of d – a double - is said to be cast (i.e. flagged by the compiler to be converted at run-time) into an int type.  ﻿By now the learner will be familiar, to some extent, with method invocation from earlier chapters, when objects of the Member class in the themed application are used to give some examples of passing arguments to methods. Chapter Four goes into more detail about methods and gives a further explanation about how methods are defined and used. Examples from the themed application are used to illustrate the principal concepts associated with an object’s methods. Chapter Three examines an object’s variables, i.e. its state or what it knows what its values are. An object’s methods represent the behaviour of an object, or what is knows what it can do, and surround, or encapsulate, an object’s variables. This section answers the question about how we get computable values into methods. As we know from previous chapters, a method is invoked by selecting the object reference for the instance required. The general syntax of a method invocation can be summarised as follows. Referring, again, to the Member class of the themed application, we could instantiate a number of Member objects (in a main method) and call their methods as in the following code snippet. // Instantiate three members; call the no-arguments constructor for the Member class. // Call one of the set methods of these objects. // Call one of the get methods of these objects in a print statement. The screen output from executing this fragment of main is: In short, we must ensure that we know which method we are calling on which object and in which order. In the code snippet above, it is evident that setUserName expects a String argument to be passed to it; this is because its definition is written as: The single parameter is replaced by a computable value, i.e. an argument, when the method is invoked. The general syntax of a method’s declaration is modifier return_type method_name( parameter_list ) exception_list The method’s definition is its declaration, together with the body of the method’s implementation between braces, as follows: The method’s signature is its name and parameter list. It is in the body of a method where application logic is executed, using statements such as: invocations: calls to other methods; assignments: changes to the values of fields or local variables; selection: cause a branch; repetition: cause a loop; detect exceptions, i.e. error conditions. If the identifier of a parameter is the same as that of an instance variable, the former is said to hide the latter. The compiler is able to distinguish between the two identifiers by the use of the keyword ‘this’, as in the following method definition that we met in Chapter One: If, on the other hand, we wish to avoid hiding, we could write the method definition as follows: where the identifier of the parameter is deliberately chosen to be different from that of the instance variable. In this case, the keyword ‘this’ can be included but it is not necessary to do so. In both versions of the method setUserName, the value of the parameter’s argument has scope only within the body of the method. Thus, in general, arguments cease to exist when a method completes its execution. A final point to make concerning arguments is that a method cannot be passed as an argument to another method or a constructor. Instead, an object reference is passed to the method or constructor so that the object reference is made available to that method or constructor or to other members of the class that invoke that method. For example, consider the following code snippet from the graphical version of the themed application shown on the next page. The examples and discussion in this section are meant to raise a question in the mind of the learner: are arguments passed by value or by reference? This question is addressed in the next sub-section. All arguments to methods (and constructors) are, in Java, passed by value. This means that a copy of the argument is passed in to a method (or a constructor) call. The example that follows aims to illustrate what pass by value semantics means in practice: detailed code documentation is omitted for the sake of clarity. The method changeValue changes the value of the argument passed to it – a copy of x – but it does not change the original value of x, as shown by the output. Thus the integer values 1235 and 1234 are output according to the semantics of pass by value as they apply to arguments. When a parameter is an object reference, it is a copy of the object reference that is passed to the method. You can change which object the argument refers to inside the method, without affecting the original object reference that was passed. However if the body of the method calls methods of the original object – via the copy of its reference - that change the state of the object, the object’s state is changed for the duration of its scope in a programme. Thus, in the example above, the strings “Bonjour” and “Hello there!” are output according to the semantics of pass by value as they apply to object references. A common misconception about passing object references to methods or constructors is that Java uses pass by reference semantics. This is incorrect: pass by reference would mean that if used by Java, the original reference to the object would be passed to the method or constructor, rather than a copy of the reference, as is the case in Java. The Java language passes object references by value, in that a copy of the object reference is passed to the method or constructor. The statement in the box isn’t true when objects are passed amongst objects in a distributed application. However, such applications are beyond the scope of this guide. For the purposes of the present guide, the learner should use the examples above to understand the consequences of Java’s use of pass by value semantics. In previous chapters, we have encountered a number of references to a method’s return type. In the definition of a method, the return type is declared as part of the method’s declaration and its value is returned by the final statement of the method.  ﻿There are several examples in previous chapters that illustrate how constructors are used to instantiate objects of a class. Let us recall the overall technique before we bring together a number of features of constructors in this chapter. One of the constructors for Member objects in the themed application is as follows: An object’s constructors have the same name as the class they instantiate. To access an object of the class Member in an application, we first declare a variable of the Member type in a main method in a test class as follows: The statement above does not create a Member object; it merely declares a variable of the required type that can subsequently be initialised to refer to an instance of the Member type. The variable that refers to an object is known as its object reference. The object that an object reference refers to must be created explicitly, in a statement that instantiates a Member object as follows. The two statements above can be combined as follows. When the Member object is created by using ‘new’, the type of object required to be constructed is specified and the required arguments are passed to the constructor. The JRE allocates sufficient memory to store the fields of the object and initialises its state. When initialisation is complete, the JRE returns a reference to the new object. Thus, we can regard a constructor as returning an object reference to the object stored in memory. While objects are explicitly instantiated using ‘new’, as shown above for a Member object, there is no need to explicitly destroy them (as is required in some OO run-time systems). The Java Virtual Machine (JVM) manages memory on behalf of the developer so that memory for objects that is no longer used in an application is automatically reclaimed without the intervention of the developer. In general, an object’s fields can be initialised when they are declared or they can be declared without being initialised. For example, the code snippet on the next page shows part of the class declaration for a version of the Member class: The code snippet illustrates an example where some of the instance variables are initialised and some are only declared. In the case of the latter type of declaration, the instance variable is initialised to its default value when the constructor returns an object reference to the newly-created object. For example, the instance variable noOfCards is initialised to 0 when the object is created. Declaring and initialising none, some or all instance variables in this way if often sufficient to establish the initial state of an object. On the other hand, where more than simple initialisation to literals or default values is required and where other tasks are required to be performed, the body of a constructor can be used to do the work of establishing the initial state of an object. Consider the following part of the constructor for the Member class. This constructor is used when simple initialisation of Member objects is insufficient. Thus, in the code block of the constructor above, the arguments passed to the constructor are associated with four of the fields of the Member class. The effect of the four statements inside the constructor’s code block is to initialise the four fields before the constructor returns a reference to the object. Constructors can, like methods, generate or throw special objects that represent error conditions. These special objects are instances of Java’s in-built Exception class. We will explore how to throw and detect Exception objects in Chapter Four in An Introduction to Java Programming 2: Classes in Java Applications. It is worthwhile being reminded at this point in the discussion about constructors that the compiler inserts a default constructor if the developer has not defined any constructors for a class. The default constructor takes no arguments and contains no code. It is provided automatically only if the developer has not provided any constructors in a class definition. We saw in the previous chapter that methods can be overloaded. Constructors can be similarly overloaded to provide flexibility in initialising the state of objects of a class. For example, the following class definition includes more than one constructor. The example class – SetTheTime – is a simple illustration of a class which provides more than one constructor. The example also shows that a constructor can be called from the body of another constructor by using the ‘this’ invocation as the first executable statement in the constructor. Thus, in the example above, the two argument constructor is called in the first statement of the three argument constructor. Complex initialisation of fields can be achieved by using what is known as an initialisation block. An initialisation block is a block of statements, delimited by braces, that appears near the beginning of a class definition outside of any constructor definitions. The position of such a block can be generalised in the following simple template for a typical class definition: An initialisation block is executed as if it were placed at the beginning of every constructor of a class. In other words, it represents a common block of code that every constructor executes. Thus far, in this study guide, we have only been able to work with single values of primitive data types and object references. In the next chapter, we will find out how we can associate multiple values of types with a single variable so that we can work with multiple values of primitives or object references in an application. ﻿The Java language provides a number of constructs that enable the developer to control the sequence of execution of Java statements. Chapter Two provides examples of how these constructs are used to control the flow of execution through a block of code that is typically contained in the body of a method. Sequential flow of execution of statements is the execution of Java source code in a statement-bystatement sequence in the order in which they are written, with no conditions. Most of the examples of methods that are discussed in previous chapters exhibit sequential flow. In general terms, such a method is written as follows. A number of the main methods, presented in previous chapters, are structured in this sequential way in order to satisfy straightforward testing criteria. While sequential flow is useful, it is likely to be highly restrictive in terms of its logic. Executing statements conditionally gives the developer a mechanism to control the flow of execution in order to repeat the execution of one or more statements or change the normal, sequential flow of control. Constructs for conditional flow control in Java are very similar to those provided by other programming languages. Table 2.1 on the next page identifies the flow control constructs provided by the Java language. The sub-sections that follow show, by example, how these constructs are used. Using a decision-making construct allows the developer to execute a block of code only if a condition is true. The sub-sections that follow illustrate how decision-making constructs are used. The if … then construct is the most basic of the decision-making constructs provided by the Java language. If a condition is true, the block of code is executed: otherwise, control skips to the first statement after the if block. The following code snippet illustrates a simple use of the if … then construct. When the code snippet is run (in a main method), the output when age = 20 is: You can drink legally. The rest of the programme is next. and when age = 17, the output is: The rest of the programme is next. In some programming languages, the word ‘then’ is included in the then clause. As the code snippet above shows, this is not the case in Java. An example taken from the themed application shows an if … then construct in action in one of the methods of the Member class. The method adds a member to the array of members only if there is room in the array of (arbitrary) size 6. If there is no room in the array because noOfMembers is equal to or greater than 6, control skips to the print statement that outputs the message “No room for another member.” The if … else construct (sometimes known as the if … then … else construct) provides an alternative path of execution if the if condition evaluates to false. Figure 2.1 illustrates, diagrammatically, the logic of the if … else construct. Flow of control enters the if clause and the if condition is tested. The result of evaluating the if condition returns either true or false and one or other of the paths of execution are followed depending on this value. The else block is executed if the if condition is false. The next code snippet illustrates a simple use of the if … else construct by modifying the first code snippet in Section 2.4.1. When the code snippet is run (in a main method), the output when age = 20 is: You can drink legally. The rest of the programme is next. and when age = 17, the output is: You are too young to drink alcohol! The rest of the programme is next. Another example taken from the themed application shows an if … else construct in action in another of the methods of the Member class. The setCard method is used to associate a member of the Media Store with a virtual membership card. Each member may have up to two cards, so the method checks whether another card can be allocated to a member. The if … else construct in the method is used to return either true or false, depending upon the result of evaluating the if condition that determined whether or not the member has fewer than two cards. There is another form of the else part of the if .. else construct: else … if. This form of compound or cascading construct executes a code block depending on the evaluation of an if condition immediately after the initial if condition. The compound if … else construct is illustrated diagrammatically in Figure 2.2 below. The figure shows that any number of else … if statements can follow the initial if statement. The example on the next page illustrates how the if .. else construct is used to identify the classification for degrees awarded by universities in the United Kingdom, based on the average mark achieved in the final year. Running the code with an average of 30 % produces the following output: Your result is: You are going to have to tell your mother about this! and with an average of 65 %, the output is as follows: Your result is: Upper Second When the value of average is equal to 65, this satisfies more than one of the else … if statements in the code above. However, the output confirms that the first time that a condition is met – when average >= 60 – control passes out of the initial if statement without evaluating the remaining conditions. When a condition is met in the code above, the output shows that control skips to the first statement after the initial if statement, i.e. to the statement It is worthwhile alerting learners to the use of braces in compound else … if constructs. Care must be taken when coding compound else .. if constructs due to the number of pairs of brackets involved: a common error is to omit one or more of these brackets. In cases where there is only one statement in an if block, it is good practice to include braces – as shown in the example above – in anticipation of if blocks that include more than one statement. The final example in this sub-section shows a compound else … if construct in action in the Member class of the themed application. The method scans the array of (virtual) cards held by a member and outputs some information that is stored against each card. (for loops are discussed in a later section of this chapter.)$$$﻿The Java language provides a number of constructs that enable the developer to control the sequence of execution of Java statements. Chapter Two provides examples of how these constructs are used to control the flow of execution through a block of code that is typically contained in the body of a method. Sequential flow of execution of statements is the execution of Java source code in a statement-bystatement sequence in the order in which they are written, with no conditions. Most of the examples of methods that are discussed in previous chapters exhibit sequential flow. In general terms, such a method is written as follows. A number of the main methods, presented in previous chapters, are structured in this sequential way in order to satisfy straightforward testing criteria. While sequential flow is useful, it is likely to be highly restrictive in terms of its logic. Executing statements conditionally gives the developer a mechanism to control the flow of execution in order to repeat the execution of one or more statements or change the normal, sequential flow of control. Constructs for conditional flow control in Java are very similar to those provided by other programming languages. Table 2.1 on the next page identifies the flow control constructs provided by the Java language. The sub-sections that follow show, by example, how these constructs are used. Using a decision-making construct allows the developer to execute a block of code only if a condition is true. The sub-sections that follow illustrate how decision-making constructs are used. The if … then construct is the most basic of the decision-making constructs provided by the Java language. If a condition is true, the block of code is executed: otherwise, control skips to the first statement after the if block. The following code snippet illustrates a simple use of the if … then construct. When the code snippet is run (in a main method), the output when age = 20 is: You can drink legally. The rest of the programme is next. and when age = 17, the output is: The rest of the programme is next. In some programming languages, the word ‘then’ is included in the then clause. As the code snippet above shows, this is not the case in Java. An example taken from the themed application shows an if … then construct in action in one of the methods of the Member class. The method adds a member to the array of members only if there is room in the array of (arbitrary) size 6. If there is no room in the array because noOfMembers is equal to or greater than 6, control skips to the print statement that outputs the message “No room for another member.” The if … else construct (sometimes known as the if … then … else construct) provides an alternative path of execution if the if condition evaluates to false. Figure 2.1 illustrates, diagrammatically, the logic of the if … else construct. Flow of control enters the if clause and the if condition is tested. The result of evaluating the if condition returns either true or false and one or other of the paths of execution are followed depending on this value. The else block is executed if the if condition is false. The next code snippet illustrates a simple use of the if … else construct by modifying the first code snippet in Section 2.4.1. When the code snippet is run (in a main method), the output when age = 20 is: You can drink legally. The rest of the programme is next. and when age = 17, the output is: You are too young to drink alcohol! The rest of the programme is next. Another example taken from the themed application shows an if … else construct in action in another of the methods of the Member class. The setCard method is used to associate a member of the Media Store with a virtual membership card. Each member may have up to two cards, so the method checks whether another card can be allocated to a member. The if … else construct in the method is used to return either true or false, depending upon the result of evaluating the if condition that determined whether or not the member has fewer than two cards. There is another form of the else part of the if .. else construct: else … if. This form of compound or cascading construct executes a code block depending on the evaluation of an if condition immediately after the initial if condition. The compound if … else construct is illustrated diagrammatically in Figure 2.2 below. The figure shows that any number of else … if statements can follow the initial if statement. The example on the next page illustrates how the if .. else construct is used to identify the classification for degrees awarded by universities in the United Kingdom, based on the average mark achieved in the final year. Running the code with an average of 30 % produces the following output: Your result is: You are going to have to tell your mother about this! and with an average of 65 %, the output is as follows: Your result is: Upper Second When the value of average is equal to 65, this satisfies more than one of the else … if statements in the code above. However, the output confirms that the first time that a condition is met – when average >= 60 – control passes out of the initial if statement without evaluating the remaining conditions. When a condition is met in the code above, the output shows that control skips to the first statement after the initial if statement, i.e. to the statement It is worthwhile alerting learners to the use of braces in compound else … if constructs. Care must be taken when coding compound else .. if constructs due to the number of pairs of brackets involved: a common error is to omit one or more of these brackets. In cases where there is only one statement in an if block, it is good practice to include braces – as shown in the example above – in anticipation of if blocks that include more than one statement. The final example in this sub-section shows a compound else … if construct in action in the Member class of the themed application. The method scans the array of (virtual) cards held by a member and outputs some information that is stored against each card. (for loops are discussed in a later section of this chapter.)
EN01	1	﻿This chapter gives a very broad overview of •what a database is •what a relational database is, in particular •what a database management system (DBMS) is •what a DBMS does •how a relational DBMS does what a DBMS does We start to familiarise ourselves with terminology and notation used in the remainder of the book, and we get a brief introduction to each topic that is covered in more detail in later sections.  You will find many definitions of this term if you look around the literature and the Web. At one time (in 2008), Wikipedia [1] offered this: “A structured collection of records or data.” I prefer to elaborate a little:  The organized, machine-readable collection of symbols is what you “see” if you “look at” a database at a particular point in time. It is to be interpreted as a true account of the enterprise at that point in time. Of course it might happen to be incorrect, incomplete or inaccurate, so perhaps it is better to say that the account is believed to be true. The alternative view of a database as a collection of variables reflects the fact that the account of the enterprise has to change from time to time, depending on the frequency of change in the details we choose to include in that account. The suitability of a particular kind of database (such as relational, or object-oriented) might depend to some extent on the requirements of its user(s). When E.F. Codd developed his theory of relational databases (first published in 1969), he sought an approach that would satisfy the widest possible ranges of users and uses. Thus, when designing a relational database we do so without trying to anticipate specific uses to which it might be put, without building in biases that would favour particular applications. That is perhaps the distinguishing feature of the relational approach, and you should bear it in mind as we explore some of its ramifications.  For example, the table in Figure 1.1 shows an organized collection of symbols.  Can you guess what this tabular arrangement of symbols might be trying to tell us? What might it mean, for symbols to appear in the same row? In the same column? In what way might the meaning of the symbols in the very first row (shown in blue) differ from the meaning of those below them? Do you intuitively guess that the symbols below the first row in the first column are all student identifiers, those in the second column names of students, and those in the third course identifiers? Do you guess that student S1’s name is Anne? And that Anne is enrolled on courses C1 and C2? And that Cindy is enrolled on neither of those two courses? If so, what features of the organization of the symbols led you to those guesses? Remember those features. In an informal way they form the foundation of relational theory. Each of them has a formal counterpart in relational theory, and those formal counterparts are the only constituents of the organized structure that is a relational database.  Perhaps those green symbols, organized as they are with respect to the blue ones, are to be understood to mean: “Student S1, named Anne, is enrolled on course C1.” An important thing to note here is that only certain symbols from the sentence in quotes appear in the table—S1, Anne, and C1. None of the other words appear in the table. The symbols in the top row of the table (presumably column headings, though we haven’t actually been told that) might help us to guess “student”, “named”, and “course”, but nothing in the table hints at “enrolled”. And even if those assumed column headings had been A, B and C, or X, Y and Z, the given interpretation might still be the intended one. Now, we can take the sentence “Student S1, named Anne, is enrolled on course C1” and replace each of S1, Anne, and C1 by the corresponding symbols taken from some other row in the table, such as S2, Boris, and C1. In so doing, we are applying exactly the same mode of interpretation to each row. If that is indeed how the table is meant to be interpreted, then we can conclude that the following sentences are all true: Student S1, named Anne, is enrolled on course C1. Student S1, named Anne, is enrolled on course C2. Student S2, named Boris, is enrolled on course C1. Student S3, named Cindy, is enrolled on course C3.  In Chapter 3, “Predicates and Propositions”, we shall see exactly how such interpretations can be systematically formalized. In Chapter 4, “Relational AlgebraThe Foundation”, and Chapter 5, “Building on The Foundation”, we shall see how they help us to formulate correct queries to derive useful information from a relational database.  We have added the name, ENROLMENT, above the table, and we have added an extra row. ENROLMENT is a variable. Perhaps the table we saw earlier was once its value. If so, it (the variable) has been updated since thenthe row for S4 has been added. Our interpretation of Figure 1.1 now has to be revised to include the sentence represented by that additional row: Student S1, named Anne, is enrolled on course C1. Student S1, named Anne, is enrolled on course C2. Student S2, named Boris, is enrolled on course C1. Student S3, named Cindy, is enrolled on course C3. Student S4, named Devinder, is enrolled on course C1. Notice that in English we can join all these sentences together to form a single sentence, using conjunctions like “and”, “or”, “because” and so on. If we join them using “and” in particular, we get a single sentence that is logically equivalent to the given set of sentences in the sense that it is true if each one of them is true (and false if any one of them is false). A database, then, can be thought of as a representation of an account of the enterprise expressed as a single sentence! (But it’s more usual to think in terms of a collection of individual sentences.) We might also be able to conclude that the following sentences (for example) are false: Student S2, named Boris, is enrolled on course C2. Student S2, named Beth, is enrolled on course C1. ﻿In this chapter we look at the four fundamental concepts on which most computer languages are based. We acquire some useful terminology to help us talk about these concepts in a precise way, and we begin to see how the concepts apply to relational database languages in particular. It is quite possible that you are already very familiar with these conceptsindeed, if you have done any computer programming they cannot be totally new to youbut I urge you to study the chapter carefully anyway, as not everybody uses exactly the same terminology (and not everybody is as careful about their use of terminology as we need to be in the present context). And in any case I also define some special terms, introduced by C.J. Date and myself in the 1990s, which have perhaps not yet achieved wide usagefor example, selector and possrep. I wrote “most computer languages” because some languages dispense with variables. Database languages typically do not dispense with variables because it seems to be the very nature of what we call a database that it varies over time in keeping with changes in the enterprise. Money changes hands, employees come and go, get salary rises, change jobs, and so on. A language that supports variables is said to be an imperative language (and one that does not is a functional language). The term “imperative” appeals to the notion of commands that such a language needs for purposes such as updating variables. A command is an instruction, written in some computer language, to tell the system to do something. The terms statement (very commonly) and imperative (rarely) are used instead of command. In this book I use statement quite frequently, bowing to common usage, but I really prefer command because it is more appropriate; also, in normal discourse statement refers to a sentence of the very important kind described in Chapter 3 and does not instruct anybody to do anything.  Figure 2.1 shows a simple commandthe assignment, Y := X + 1dissected into its component parts. The annotations show the terms we use for those components.  It is important to distinguish carefully between the concepts and the language constructs that represent (denote) those concepts. It is the distinction between what is written and what it meanssyntax and semantics.  Each annotated component in Figure 1 is an example of a certain language construct. The annotation shows the term used for the language construct and also the term for the concept it denotes. Honouring this distinction at all times can lead to laborious prose. Furthermore, we don’t always have distinct terms for the language construct and the corresponding concept. For example, there is no single-word term for an expression denoting an argument. We can write “argument expression” when we need to be absolutely clear and there is any danger of ambiguity, but normally we would just say, for example, that X+1 is an argument to that invocation of the operator “:=” shown in Figure 2.1. (The real argument is the result of evaluating X+1.) The update operator “:=” is known as assignment. The command Y := X+1 is an invocation of assignment, often referred to as just an assignment. The effect of that assignment is to evaluate the expression X+1, yielding some numerical result r and then to assign r to the variable Y. Subsequent references to Y therefore yield r (until some command is given to assign something else to Y). Note the two operands of the assignment: Y is the target, X+1 the source. The terms target and source here are names for the parameters of the operator. In the example, the argument expression Y is substituted for the parameter target and the argument expression X+1 is substituted for the parameter source. We say that target is subject to update, meaning that any argument expression substituted for it must denote a variable. The other parameter, source, is not subject to update, so any argument expression substituted must denote a value, not a variable. Y denotes a variable and X+1 denotes a value. When the assignment is evaluated (or, as we sometimes say of commands, executed), the variable denoted by Y becomes the argument substituted for target, and the current value of X+1 becomes the argument substituted for source. Whereas the Y in Y := X + 1 denotes a variable, as I have explained, the X in Y := X + 1 does not, as I am about to explain. So now let’s analyse the expression X+1. It is an invocation of the read-only operator +, which has two parameters, perhaps named a and b. Neither a nor b is subject to update. A read-only operator is one that has no parameter that is subject to update. Evaluation of an invocation of a read-only operator yields a value and updates nothing. The arguments to the invocation, in this example denoted by the expressions X and 1, are the values denoted by those two expressions. 1 is a literal, denoting the numerical value that it always denotes; X is a variable reference, denoting the value currently assigned to X. A literal is an expression that denotes a value and does not contain any variable references. But we do not use that term for all such expressions: for example, the expression 1+2, denoting the number 3, is not a literal. I defer a precise definition of literal to later in the present chapter.  The following very important distinctions emerge from the previous section and should be firmly taken on board: •Syntax versus semantics •Value versus variable •Variable versus variable reference •Update operator versus read-only operator •Operator versus invocation •Parameter versus argument •Parameter subject to update versus parameter not subject to update Each of these distinctions is illustrated in Figure 2.1, as follows: •Value versus variable: Y denotes a variable, X denotes the value currently assigned to the variable X. 1 denotes a value. Although X and Y are both symbols referencing variables, what they denote depends in the context in which those references appear. Y appears as an update target and thus denotes the variable of that name, whereas X appears where an expression denoting a value is expected and that position denotes the current value of the referenced variable. Note that variables, by definition, are subject to change (in value) from time to time. A value, by contrast, exists independently of time and space and is not subject to change. ﻿In Chapter 1 I defined a database to be “… an organised, machine-readable collection of symbols, to be interpreted as a true account of some enterprise.” I also gave this example (extracted from Figure 1.1): I suggested that those green symbols, organised as they are with respect to the blue ones, might be understood to mean: “Student S1, named Anne, is enrolled on course C1.” In this chapter I explain exactly how such an interpretation can be justified. In fact, I describe the general method under which data organized in the form of relations is to be interpretedto yield information, as some people say. This method of interpretation is firmly based in the science of logic. Relational database theory is based very directly on logic. Predicates and propositions are the fundamental concepts that logic deals with. Fortunately, we need to understand only the few basic principles on which logic is founded. You may well already have a good grasp of the principles in question, but even if you do, please do not skip this chapter. For one thing, the textbooks on logic do not all use exactly the same terminology and I have chosen the terms and definitions that seem most suitable for the purpose at hand. For another, I do of course concentrate on the points that are particularly relevant to relational theory; you need to know which points those are and to understand exactly why they are so relevant. Predicates, one might say, are what logic is all about. And yet the textbooks do not speak with one voice when it comes to pinning down exactly what the term refers to! I choose the definition that appears to me to fit best, so to speak, with relational database theory. We start by looking again at that possible interpretation of the symbols S1, Anne, and C1, placed the way they are in Figure 1.1: “Student S1, named Anne, is enrolled on course C1.” This is a sentence. Sentences are what human beings typically use to communicate with each other, using language. We express our interpretations of the data using sentences in human language and we use relations to organize the data to be interpreted. Logic bridges the gap between relations and sentences. Our example sentence can be recast into two simpler sentences, “Student S1 is named Anne” and “Student S1 is enrolled on course C1”. Let’s focus on the second: The symbols S1 and C1 appear both in this sentence and in the data whose meaning it expresses. Because they each designate, or refer to, a particular thingS1 a particular student, C1 a particular coursethey are called designators. The word Anne is another designator, referring to a particular forename. “An Introduction to Relational Database Theory” is also a designator, referring to a particular book, and so is, for example, -7, referring to a particular number. Now, suppose we replace S1 and C1 in Example 3.1 by another pair of symbols, taken from the same columns of Figure 1.1 but a different row. Then we might obtain A pattern is clearly emerging. For every row in Figure 1.1, considering just the columns headed StudentId and CourseId, we can obtain a sentence in the form of Examples 3.1 and 3.2. The words “Student … is enrolled on course …” appear in that order in each case and in each case the gaps indicated by …sometimes called placeholdersare replaced by appropriate designators. If we now replace each placeholder by the name given in the heading of the column from which the appropriate designator is to be drawn, we obtain this: Example 3.3 succinctly expresses the way in which the named columns in each row of Figure 1.1 are probably to be interpreted. And we now know that those names, StudentId and CourseId, in the column headings are the names of two of the attributes of the relation that Figure 1.1 depicts in tabular form. Now, the sentences in Examples 3.1 and 3.2 are in fact statements. They state something of which it can be said, “That is true”, or “That is not true”, or “I believe that”, or “I don’t believe that”. Not all sentences are statements. A good informal test, in English, to determine whether a sentence is a statement is to place “Is it true that” in front of it. If the result is a grammatical English question, then the original sentence is indeed a statement; otherwise it is not. Here are some sentences that are not statements: •“Let’s all get drunk.” •“Will you marry me?” •“Please pass me the salt.” •“If music be the food of love, play on.” They each fail the test. In fact one of them is a question itself and the other three are imperatives, but we have no need of such sentences in our interpretation of relations because we seek only information, in the form of statementsstatements that we are prepared to believe are statements of fact; in other words, statements we believe to be true. We do not expect a database to be interpreted as asking questions or giving orders. We expect it to be stating facts (or at least what are believed to be facts). As an aside, I must own up to the fact that some sentences that would be accepted as statements in English don’t really pass the test as they stand. Here are two cases in point, from Shakespeare: •“O for a muse of fire that would ascend the highest heaven of invention.” •“To be or not to bethat is the question.” The first appears to lack a verb, but we know that “O for a …” is a poetical way of expressing a wish for something on the part of the speaker, so we can paraphrase it fairly accurately by replacing “O” by “I wish”, and the sentence thus revised passes the test. In the second case we have only to delete the word “that”, whose presence serves only for emphasis (and scansion, of course!), and alter the punctuation slightly: “It is true that ‘to be or not to be?’ is the question.” Now, a statement is a sentence that is declarative in form: it declares something that is supposed to be true. Example 3.3, “Student StudentId is enrolled on course CourseId”, is not a statementit does not pass the test. It does, however, have the grammatical form of a statement. We can say that, like a statement, it is declarative in form. ﻿I should make it clear right away that to most people the term “normalization”, in the context of databases, means projection-join normalization specifically. But the term does seem to be equally appropriate in connection with other kinds of design choice, hence my decision to use it in the first two sections of this chapter. I promised a discussion of 1NF in this section. It is here, but later. Some examples in this chapter refer to the relvars COURSE and EXAM_MARK introduced at the beginning of Chapter 5. Their current values are repeated here in Figure 8.1. And here again are the relvar definitions for COURSE and EXAM_MARK: In Chapter 5, Example 5.12 illustrates the Tutorial D operator GROUP. The expression EXAM_MARK GROUP ( { StudentId, Mark } AS ExamResult ), operating on the current value of EXAM_MARK, yields the relation shown in Figure 5.6, repeated here as Figure 8.2 for convenience: Recall also that we can reverse the process by use of UNGROUP. Thus, a relvar defined as in Example 8.1 might be considered as a valid alternative to EXAM_MARK, but note the constraints needed to make the “grouped” design genuinely equivalent. The first of those two constraints reflects the fact that EXAM_MARK by itself cannot accommodate a course for which nobody sat the exam. It would probably make better sense to disregard that putative requirement and drop the constraint. The second constraint is a key constraint on the attribute values for ExamResult, a logical consequence of KEY { StudentId, CourseId } specified for EXAM_MARK. The KEY shorthand could be considered for relation-valued attributes of relvars but it is not included in Tutorial D, one reason being that such a design is in general contraindicated and should be discouraged. Here are some points against it: 1. The particular grouping chosen is arbitrary. Why not group on { CourseId, Mark } instead? 2. The constraints are more complicated, even if we drop the one requiring ExamResult values to be nonempty. 3. Updating can be particularly awkward. Consider how to write a Tutorial D UPDATE statement to change student S1’s mark in course C1. Consider how to write an INSERT statement to record that mark in the first place. 4. The playing field for queries, as with non-5NF relvars, is not level. True, some aggregation queries are slightly simplified (try obtaining the average exam mark for each course), but many others become quite complex unless a preliminary invocation of UNGROUP is injected (try obtaining all of student S1’s exam marks). In short, the asymmetric structure illustrated in Example 8.1 leads to asymmetric queries, asymmetric constraints, and asymmetric updates. Now, C_ER is in 5NF, as you can easily verify, and it exhibits no redundancy. But see, in Figure 8.3, what happens if we apply a similar treatment to our original non-5NF relvar, ENROLMENT, having the attribute Name in place of EXAM_MARK’s Mark, giving the relvar C_ES with relation-valued attribute EnrolledStudents. C_ES is in 5NF but exhibits exactly the same redundancy that ENROLMENT exhibits: in the current value of C_ES, student S1’s name is recorded twice. For these reasons, perhaps a group-ungroup normal form (GUNF?) could be usefully defined: relvar r is in GUNF if and only if no attribute of r is relation valued; but as far as the present author is aware no such definition is to be found in the literature (apart from this book). Codd proposed a normal form that he called first normal form (1NF), and he included a requirement for 1NF in his definitions for 2NF, 3NF, and subsequently BCNF. Under 1NF as he defined it, relation-valued attributes were “outlawed”; that is to say, a relvar having such an attribute was not in 1NF. However, certain examples do exist where to avoid a relation valued attribute we have to resort to artifice. Consider, for example, the relvar, in the catalog, to record the keys of all the relvars in the database. The straightforward definition, shown in Example 8.2, involves a relation-valued attribute. We cannot obtain an equivalent design by ungrouping, because a relvar can have several keys. A tuple appearing in the ungrouping, simply pairing relvar name r with attribute name a tells, us only that a is a member of some key of r. A truly equivalent design in GUNF is unachievable. The best we can do is probably as shown in Example 8.3, where we have to introduce an extra attribute. Moreover, this design does not admit relvars with empty keys (specified by KEY { })those would have to be represented by a separate relvar in the catalog. Having to number the keys of each relvar is artificial and burdensome. Most of the noted disadvantages of relation-valued attributes are not so relevant here because we expect catalog relvars to be maintained by the DBMS. The natural, non-GUNF design of Example 8.2 is probably preferable. Now, Codd’s definition of 1NF attempted similarly to outlaw attributes of certain other types too, because relation types are not the only types that, if used for attributes of relvars, give rise to the problems identified with relation types, as we shall now see. Instead of defining C_ES with its relation-valued attribute EnrolledStudents, derived from ENROLMENT using GROUP, we could apply WRAP to derive the relvar C_EST, as shown in Example 8.4. Like C_ER, C_EST is in 5NF and yet still exhibits the same redundancy as ENROLMENT. Codd’s definition of 1NF precluded tuple-typed attributes too. Perhaps a “wrap-unwrap” normal form (WUNF?) could be usefully defined along similar lines to the GUNF previously mooted. But even outlawing relation and tuple types wasn’t sufficient for the purpose at hand. A similar effect can be obtained with userdefined types, as Example 8.5 shows. Codd attempted to preclude the use of such types in his definition of 1NF, but unfortunately this definition appealed to an unclear notion of atomicity. To be in 1NF, a relvar’s attributes all had to be of types consisting of “atomic” values only. However, he did not give a clear definition of what it means for a value to be atomic and we now believe the notion has no absolute meaning. Suffice it just to say that the relational database designer should generally avoid futile attempts, such as those shown in this section, to obtain 5NF without achieving the elimination of redundancy that 5NF is supposed to achieve. In particular, stick, where possible, to GUNF and WUNF.$$$﻿I should make it clear right away that to most people the term “normalization”, in the context of databases, means projection-join normalization specifically. But the term does seem to be equally appropriate in connection with other kinds of design choice, hence my decision to use it in the first two sections of this chapter. I promised a discussion of 1NF in this section. It is here, but later. Some examples in this chapter refer to the relvars COURSE and EXAM_MARK introduced at the beginning of Chapter 5. Their current values are repeated here in Figure 8.1. And here again are the relvar definitions for COURSE and EXAM_MARK: In Chapter 5, Example 5.12 illustrates the Tutorial D operator GROUP. The expression EXAM_MARK GROUP ( { StudentId, Mark } AS ExamResult ), operating on the current value of EXAM_MARK, yields the relation shown in Figure 5.6, repeated here as Figure 8.2 for convenience: Recall also that we can reverse the process by use of UNGROUP. Thus, a relvar defined as in Example 8.1 might be considered as a valid alternative to EXAM_MARK, but note the constraints needed to make the “grouped” design genuinely equivalent. The first of those two constraints reflects the fact that EXAM_MARK by itself cannot accommodate a course for which nobody sat the exam. It would probably make better sense to disregard that putative requirement and drop the constraint. The second constraint is a key constraint on the attribute values for ExamResult, a logical consequence of KEY { StudentId, CourseId } specified for EXAM_MARK. The KEY shorthand could be considered for relation-valued attributes of relvars but it is not included in Tutorial D, one reason being that such a design is in general contraindicated and should be discouraged. Here are some points against it: 1. The particular grouping chosen is arbitrary. Why not group on { CourseId, Mark } instead? 2. The constraints are more complicated, even if we drop the one requiring ExamResult values to be nonempty. 3. Updating can be particularly awkward. Consider how to write a Tutorial D UPDATE statement to change student S1’s mark in course C1. Consider how to write an INSERT statement to record that mark in the first place. 4. The playing field for queries, as with non-5NF relvars, is not level. True, some aggregation queries are slightly simplified (try obtaining the average exam mark for each course), but many others become quite complex unless a preliminary invocation of UNGROUP is injected (try obtaining all of student S1’s exam marks). In short, the asymmetric structure illustrated in Example 8.1 leads to asymmetric queries, asymmetric constraints, and asymmetric updates. Now, C_ER is in 5NF, as you can easily verify, and it exhibits no redundancy. But see, in Figure 8.3, what happens if we apply a similar treatment to our original non-5NF relvar, ENROLMENT, having the attribute Name in place of EXAM_MARK’s Mark, giving the relvar C_ES with relation-valued attribute EnrolledStudents. C_ES is in 5NF but exhibits exactly the same redundancy that ENROLMENT exhibits: in the current value of C_ES, student S1’s name is recorded twice. For these reasons, perhaps a group-ungroup normal form (GUNF?) could be usefully defined: relvar r is in GUNF if and only if no attribute of r is relation valued; but as far as the present author is aware no such definition is to be found in the literature (apart from this book). Codd proposed a normal form that he called first normal form (1NF), and he included a requirement for 1NF in his definitions for 2NF, 3NF, and subsequently BCNF. Under 1NF as he defined it, relation-valued attributes were “outlawed”; that is to say, a relvar having such an attribute was not in 1NF. However, certain examples do exist where to avoid a relation valued attribute we have to resort to artifice. Consider, for example, the relvar, in the catalog, to record the keys of all the relvars in the database. The straightforward definition, shown in Example 8.2, involves a relation-valued attribute. We cannot obtain an equivalent design by ungrouping, because a relvar can have several keys. A tuple appearing in the ungrouping, simply pairing relvar name r with attribute name a tells, us only that a is a member of some key of r. A truly equivalent design in GUNF is unachievable. The best we can do is probably as shown in Example 8.3, where we have to introduce an extra attribute. Moreover, this design does not admit relvars with empty keys (specified by KEY { })those would have to be represented by a separate relvar in the catalog. Having to number the keys of each relvar is artificial and burdensome. Most of the noted disadvantages of relation-valued attributes are not so relevant here because we expect catalog relvars to be maintained by the DBMS. The natural, non-GUNF design of Example 8.2 is probably preferable. Now, Codd’s definition of 1NF attempted similarly to outlaw attributes of certain other types too, because relation types are not the only types that, if used for attributes of relvars, give rise to the problems identified with relation types, as we shall now see. Instead of defining C_ES with its relation-valued attribute EnrolledStudents, derived from ENROLMENT using GROUP, we could apply WRAP to derive the relvar C_EST, as shown in Example 8.4. Like C_ER, C_EST is in 5NF and yet still exhibits the same redundancy as ENROLMENT. Codd’s definition of 1NF precluded tuple-typed attributes too. Perhaps a “wrap-unwrap” normal form (WUNF?) could be usefully defined along similar lines to the GUNF previously mooted. But even outlawing relation and tuple types wasn’t sufficient for the purpose at hand. A similar effect can be obtained with userdefined types, as Example 8.5 shows. Codd attempted to preclude the use of such types in his definition of 1NF, but unfortunately this definition appealed to an unclear notion of atomicity. To be in 1NF, a relvar’s attributes all had to be of types consisting of “atomic” values only. However, he did not give a clear definition of what it means for a value to be atomic and we now believe the notion has no absolute meaning. Suffice it just to say that the relational database designer should generally avoid futile attempts, such as those shown in this section, to obtain 5NF without achieving the elimination of redundancy that 5NF is supposed to achieve. In particular, stick, where possible, to GUNF and WUNF.
EN03	0	﻿As its name implies control engineering involves the design of an engineering product or system where a requirement is to accurately control some quantity, say the temperature in a room or the position or speed of an electric motor. To do this one needs to know the value of the quantity being controlled, so that being able to measure is fundamental to control. In principle one can control a quantity in a so called open loop manner where ‘knowledge’ has been built up on what input will produce the required output, say the voltage required to be input to an electric motor for it to run at a certain speed. This works well if the ‘knowledge’ is accurate but if the motor is driving a pump which has a load highly dependent on the temperature of the fluid being pumped then the ‘knowledge’ will not be accurate unless information is obtained for different fluid temperatures. But this may not be the only practical aspect that affects the load on the motor and therefore the speed at which it will run for a given input, so if accurate speed control is required an alternative approach is necessary. This alternative approach is the use of feedback whereby the quantity to be controlled, say C, is measured, compared with the desired value, R, and the error between the two, E = R - C used to adjust C. This gives the classical feedback loop structure of Figure 1.1. In the case of the control of motor speed, where the required speed, R, known as the reference is either fixed or moved between fixed values, the control is often known as a regulatory control, as the action of the loop allows accurate speed control of the motor for the aforementioned situation in spite of the changes in temperature of the pump fluid which affects the motor load. In other instances the output C may be required to follow a changing R, which for example, might be the required position movement of a robot arm. The system is then often known as a servomechanism and many early textbooks in the control engineering field used the word servomechanism in their title rather than control. The use of feedback to regulate a system has a long history [1.1, 1.2], one of the earliest concepts, used in Ancient Greece, was the float regulator to control water level, which is still used today in water tanks. The first automatic regulator for an industrial process is believed to have been the flyball governor developed in 1769 by James Watt. It was not, however, until the wartime period beginning in 1939, that control engineering really started to develop with the demand for servomechanisms for munitions fire control and guidance. With the major improvements in technology since that time the applications of control have grown rapidly and can be found in all walks of life. Control engineering has, in fact, been referred to as the ‘unseen technology’ as so often people are unaware of its existence until something goes wrong. Few people are, for instance, aware of its contribution to the development of storage media in digital computers where accurate head positioning is required. This started with the magnetic drum in the 50’s and is required today in disk drives where position accuracy is of the order of 1μm and movement between tracks must be done in a few ms. Feedback is, of course, not just a feature of industrial control but is found in biological, economic and many other forms of system, so that theories relating to feedback control can be applied to many walks of life. The book is concerned with theoretical methods for continuous linear feedback control system design, and is primarily restricted to single-input single-output systems. Continuous linear time invariant systems have linear differential equation mathematical models and are always an approximation to a real device or system. All real systems will change with time due to age and environmental changes and may only operate reasonably linearly over a restricted range of operation. There is, however, a rich theory for the analysis of linear systems which can provide excellent approximations for the analysis and design of real world situations when used within the correct context. Further simulation is now an excellent means to support linear theoretical studies as model errors, such as the affects of neglected nonlinearity, can easily be assessed. There are total of 11 chapters and some appendices, the major one being Appendix A on Laplace transforms. The next chapter provides a brief description of the forms of mathematical model representations used in control engineering analysis and design. It does not deal with mathematical modelling of engineering devices, which is a huge subject and is best dealt with in the discipline covering the subject, since the devices or components could be electrical, mechanical, hydraulic etc. Suffice to say that one hopes to obtain an approximate linear mathematical model for these components so that their effect in a system can be investigated using linear control theory. The mathematical models discussed are the linear differential equation, the transfer function and a state space representation, together with the notations used for them in MATLAB. Chapter 3 discusses transfer functions, their zeros and poles, and their responses to different inputs. The following chapter discusses in detail the various methods for plotting steady state frequency responses with Bode, Nyquist and Nichols plots being illustrated in MATLAB. Hopefully sufficient detail, which is brief when compared with many textbooks, is given so that the reader clearly understands the information these plots provide and more importantly understands the form of frequency response expected from a specific transfer function. The material of chapters 2-4 could be covered in other courses as it is basic systems theory, there having been no mention of control, which starts in chapter 5. The basic feedback loop structure shown in Figure 1.1 is commented on further, followed by a discussion of typical performance specifications which might have to be met in both the time and frequency domains. Steady state errors are considered both for input and disturbance signals and the importance and properties of an integrator are discussed from a physical as well as mathematical viewpoint. The chapter concludes with a discussion on stability and a presentation of several results including the Mikhailov criterion, which is rarely mentioned in English language texts. ﻿Control systems exist in many fields of engineering so that components of a control system may be electrical, mechanical, hydraulic etc. devices. If a system has to be designed to perform in a specific way then one needs to develop descriptions of how the outputs of the individual components, which make up the system, will react to changes in their inputs. This is known as mathematical modelling and can be done either from the basic laws of physics or from processing the input and output signals in which case it is known as identification. Examples of physical modelling include deriving differential equations for electrical circuits involving resistance, inductance and capacitance and for combinations of masses, springs and dampers in mechanical systems. It is not the intent here to derive models for various devices which may be used in control systems but to assume that a suitable approximation will be a linear differential equation. In practice an improved model might include nonlinear effects, for example Hooke’s Law for a spring in a mechanical system is only linear over a certain range; or account for time variations of components. Mathematical models of any device will always be approximate, even if nonlinear effects and time variations are also included by using more general nonlinear or time varying differential equations. Thus, it is always important in using mathematical models to have an appreciation of the conditions under which they are valid and to what accuracy. Starting therefore with the assumption that our model is a linear differential equation then in general it will have the form:- where D denotes the differential operator d/dt. A(D) and B(D) are polynomials in D with Di d i / dt i , the ith derivative, u(t) is the model input and y(t) its output. So that one can write where the a and b coefficients will be real numbers. The orders of the polynomials A and B are assumed to be n and m, respectively, with n m. Thus, for example, the differential equation with the dependence of y and u on t assumed can be written  In order to solve an nth order differential equation, that is determine the output y for a given input u, one must know the initial conditions of y and its first n-1 derivatives. For example if a projectile is falling under gravity, that is constant acceleration, so that D2y= constant, where y is the height, then in order to find the time taken to fall to a lower height, one must know not only the initial height, normally assumed to be at time zero, but the initial velocity, dy/dt, that is two initial conditions as the equation is second order (n = 2). Control engineers typically study solutions to differential equations using either Laplace transforms or a state space representation. A short introduction to the Laplace transformation is given in Appendix A for the reader who is not familiar with its use. It is an integral transformation and its major, but not sole use, is for differential equations where the independent time variable t is transformed to the complex variable s by the expression Since the exponential term has no units the units of s are seconds-1, that is using mks notation s has units of s-1. If denotes the Laplace transform then one may write [f(t)] = F(s) and -1[F(s)] = f(t). The relationship is unique in that for every f(t), [F(s)], there is a unique F(s), [f(t)]. It is shown in Appendix A that when the n-1 initial conditions, Dn-1y(0) are zero the Laplace transform of Dny(t) is snY(s). Thus the Laplace transform of the differential equation (2.1) with zero initial conditions can be written with the assumed notation that signals as functions of time are denoted by lower case letters and as functions of s by the corresponding capital letter. If equation (2.8) is written then this is known as the transfer function, G(s), between the input and output of the ‘system’, that is whatever is modelled by equation (2.1). B(s), of order m, is referred to as the numerator polynomial and A(s), of order n, as the denominator polynomial and are from equations (2.2) and (2.3) Since the a and b coefficients of the polynomials are real numbers the roots of the polynomials are either real or complex pairs. The transfer function is zero for those values of s which are the roots of B(s), so these values of s are called the zeros of the transfer function. Similarly, the transfer function will be infinite at the roots of the denominator polynomial A(s), and these values are called the poles of the transfer function. The general transfer function (2.9) thus has m zeros and n poles and is said to have a relative degree of n-m, which can be shown from physical realisation considerations cannot be negative. Further for n > m it is referred to as a strictly proper transfer function and for n m as a proper transfer function. When the input u(t) to the differential equation of (2.1) is constant the output y(t) becomes constant when all the derivatives of the output are zero. Thus the steady state gain, or since the input is often thought of as a signal the term d.c. gain (although it is more often a voltage than a current!) is used, and is given by If the n roots of A(s) are i , i = 1….n and of B(s) are j, j = 1….m, then the transfer function may be written in the zero-pole form When the transfer function is known in the zero-pole form then the location of its zeros and poles can be shown on an s plane zero-pole plot, where the zeros are marked with a circle and the poles by a cross. The information on this plot then completely defines the transfer function apart from the gain K. In most instances engineers prefer to keep any complex roots in quadratic form, thus for example writing  ﻿As mentioned previously a major reason for wishing to obtain a mathematical model of a device is to be able to evaluate the output in response to a given input. Using the transfer function and Laplace transforms provides a particularly elegant way of doing this. This is because for a block with input U(s) and transfer function G(s) the output Y(s) = G(s)U(s). When the input, u(t), is a unit impulse which is conventionally denoted by (t), U(s) = 1 so that the output Y(s) = G(s). Thus in the time domain, y(t) = g(t), the inverse Laplace transform of G(s), which is called the impulse response or weighting function of the block. The evaluation of y(t) for any input u(t) can be done in the time domain using the convolution integral (see Appendix A, theorem (ix)) but it is normally much easier to use the transform relationship Y(s) = G(s)U(s). To do this one needs to find the Laplace transform of the input u(t), form the product G(s)U(s) and then find its inverse Laplace transform. G(s)U(s) will be a ratio of polynomials in s and to find the inverse Laplace transform, the roots of the denominator polynomial must be found to allow the expression to be put into partial fractions with each term involving one denominator root (pole). Assuming, for example, the input is a unit step so that U(s) = 1/s then putting G(s)U(s) into partial fractions will result in an expression for Y(s) of the form where in the transfer function G(s) = B(s)/A(s), the n poles of G(s) [zeros of A(s)] are i, i = 1…n and the coefficients C0 and Ci, i = 1…n, will depend on the numerator polynomial B(s), and are known as the residues at the poles. Taking the inverse Laplace transform yields The first term is a constant C0, sometimes written C0u0(t) because the Laplace transform is defined for t 0, where u0(t) denotes the unit step at time zero. Each of the other terms is an exponential, which provided the real part of i is negative will decay to zero as t becomes large. In this case the transfer function is said to be stable as a bounded input has produced a bounded output. Thus a transfer function is stable if all its poles lie in the left hand side (lhs) of the s plane zero-pole plot illustrated in Figure 2.1. The larger the negative value of i the more rapidly the contribution from the ith term decays to zero. Since any poles which are complex occur in complex pairs, say of the form 1, 2 = ± j , then the corresponding two residues C1 and C2 will be complex pairs and the two terms will combine to give a term of the form Ce t sin( t ) . This is a damped oscillatory exponential term where , which will be negative for a stable transfer function, determines the damping and the frequency [strictly angular frequency] of the oscillation. For a specific calculation most engineers, as mentioned earlier, will leave a complex pair of roots as a quadratic factor in the partial factorization process, as illustrated in the Laplace transform inversion example given in Appendix A. For any other input to G(s), as with the step input, the poles of the Laplace transform of the input will occur in a term of the partial fraction expansion (3.2), [as for the C0/s term above], and will therefore produce a bounded output for a bounded input. In control engineering the major deterministic input signals that one may wish to obtain responses to are a step, an impulse, a ramp and a constant frequency input. The purpose of this section is to discuss step responses of specific transfer functions, hopefully imparting an understanding of what can be expected from a knowledge of the zeros and poles of the transfer function without going into detailed mathematics. A transfer function with a single pole is s a G s K  ( ) 1 , which may also be written in the socalled time constant form sT G s K   1 ( ) , where K K / a 1 and T 1/ a The steady state gainG(0) K , that is the final value of the response, and T is called the time constant as it determines the speed of the response. K will have units relating the input quantity to the output quantity, for example °C/V, if the input is a voltage and the output temperature. T will have the same units of time as s-1, normally seconds. The output, Y(s), for a unit step input is given by Taking the inverse Laplace transform gives the result The larger the value of T (i.e. the smaller the value of a), the slower the exponential response. It can easily be shown that y(T) 0.632K , T dt dy(0) and y(5T) 0.993K or in words, the output reaches 63.2% of the final value after a time T, the initial slope of the response is T and the response has essentially reached the final value after a time 5T. The step response in MATLAB can be obtained by the command step(num,den). The figure below shows the step response for the transfer function with K = 1 on a normalised time scale. Here the transfer function G(s) is often assumed to be of the form It has a unit steady state gain, i.e G(0) = 1, and poles at   1 2 o o s j , which are complex when  1. For a unit step input the output Y(s), can be shown after some algebra, which has been done so that the inverse Laplace transforms of the second and third terms are damped cosinusoidal and sinusoidal expressions, to be given by Taking the inverse Laplace transform it yields, again after some algebra, where  cos 1 . is known as the damping ratio. It can also be seen that the angle to the negative real axis from the origin to the pole with positive imaginary part is tan 1 (1 2 )1/ 2 /  cos 1  .  ﻿The frequency response of a transfer function G(j ) was introduced in the last chapter. As G(j ) is a complex number with a magnitude and argument (phase) if one wishes to show its behaviour over a frequency range then one has 3 parameters to deal with the frequency, , the magnitude, M, and the phase . Engineers use three common ways to plot the information, which are known as Bode diagrams, Nyquist diagrams and Nichols diagrams in honour of the people who introduced them. All portray the same information and can be readily drawn in MATLAB for a system transfer function object G(s). One diagram may prove more convenient for a particular application, although engineers often have a preference. In the early days when computing facilities were not available Bode diagrams, for example, had some popularity because of the ease with which they could, in many instances, be rapidly approximated. All the plots will be discussed below, quoting many results without going into mathematical detail, in the hope that the reader will obtain enough knowledge to know whether MATLAB plots obtained are of the general shape expected. A Bode diagram consists of two separate plots the magnitude, M, as a function of frequency and the phase as a function of frequency. For both plots the frequency is plotted on a logarithmic (log) scale along the x axis. A log scale has the property that the midpoint between two frequencies 1 and 2 is the frequency 1 2  . A decade of frequency is from a value to ten times that value and an octave from a value to twice that value. The magnitude is plotted either on a log scale or in decibels (dB), where dB M 10 20log . The phase is plotted on a linear scale. Bode showed that for a transfer function with no right hand side (rhs) s-plane zeros the phase is related to the slope of the magnitude characteristic by the relationship It can be further shown from this expression that a relatively good approximation is that the phase at any frequency is 15° times the slope of the magnitude curve in dB/octave. This was a useful concept to avoid drawing both diagrams when no computer facilities were available. For two transfer functions G1 and G2 in series the resultant transfer function, G, is their product, this means for their frequency response which in terms of their magnitudes and phases can be written Thus since a log scale is used on the magnitude of a Bode diagram this means Bode magnitude plots for two transfer functions in series can be added, as also their phases on the phase diagram. Hence a transfer function in zero-pole form can be plotted on the magnitude and phase Bode diagrams simple by adding the individual contributions from each zero and pole. It is thus only necessary to know the Bode plots of single roots and quadratic factors to put together Bode plots for a complicated transfer function if it is known in zero-pole form. The single pole transfer function is normally considered in time constant form with unit steady state gain, that is It is easy to show that this transfer function can be approximated by two straight lines, one constant at 0 dB, as G(0) = 1, until the frequency, 1/T, known as the break point, and then from that point by a line with slope -6dB/octave. The actual curve and the approximation are shown in Figure 4.1 together with the phase curve. The differences between the exact magnitude curve and the approximation are symmetrical, that is a maximum at the breakpoint of 3dB, 1dB one octave each side of the breakpoint, 0.3 dB two octaves away etc. The phase changes between 0° and - 90° again with symmetry about the breakpoint phase of -45°. Note a steady slope of -6 dB/octave has a corresponding phase of -90° The Bode magnitude plot of a single zero time constant, that is is simply a reflection in the 0 dB axis of the pole plot. That is the approximate magnitude curve is flat at 0 dB until the break point frequency, 1/T, and then increases at 6 dB/octave. Theoretically as the frequency tends to infinity so does its gain so that it is not physically realisable. The phase curve goes from 0° to +90° The transfer function of an integrator, which is a pole at the origin in the zero-pole plot, is 1/s. It is sometimes taken with a gain K, i.e.K/s. Here K will be replaced by 1/T to give the transfer function On a Bode diagram the magnitude is a constant slope of -6 dB/octave passing through 0 dB at the frequency 1/T. Note that on a log scale for frequency, zero frequency where the integrator has infinite gain (the transfer function can only be produced electronically by an active device) is never reached. The phase is -90° at all frequencies. A differentiator has a transfer function of sT which gives a gain characteristic with a slope of 6 dB/octave passing through 0dB at a frequency of 1/T. Theoretically it produces infinite gain at infinite frequency so again it is not physically realisable. It has a phase of +90° at all frequencies. The quadratic factor form is again taken for two complex poles with < 1 as in equation (3.7), that is Again G(0) = 1 so the response starts at 0 dB and can be approximated by a straight line at 0 dB until o and by a line from o at -12 dB/octave. However, this is a very coarse approximation as the behaviour around o is highly dependent on . It can be shown that the magnitude reaches a maximum value of which is approximately 1/2 for small , at a frequency of This frequency is thus always less than o and only exists for < 0.707. The response with = 0.707 always has magnitude, M < 1. The phase curve goes from 0° to - 180° as expected from the original and final slopes of the magnitude curve, it has a phase shift of -90° at the frequency o independent of and changes more rapidly near o for smaller , as expected due to the more rapid change in the slope of the corresponding magnitude curve. ﻿Just as for movement in the previous chapter, our body, mind and senses form the basis of many common metaphors used in human language. Our understanding as humans is derived from fundamental concepts related to our physical embodiment in the real world, and the senses we use to perceive it. We often express and understand abstract concepts that we cannot sense directly based on these fundamental concepts. Table 5.1 provides a list of common phrases that illustrates this.  Traditionally, using a classification first attributed to Aristotle, the five main senses are considered to be sight, hearing, smell, taste and touch (as included in Table 5.1). Sometimes people also refer to a ‘sixth sense’ (also sometimes called extra sensory perception or ESP) – we can consider the term as a metaphor that people use in natural language for describing unconscious thought processes in terms of another sense. It is a mistake to consider a ‘sixth sense’ as being real, as there is very little scientific evidence to support it. However, our bodies do in fact have much more than just the five basic senses as shown by the selection in Table 5.2.  From a design perspective, we can consider every agent to be embodied in some manner, using senses to gain information about its environment. We can define embodiment in the following manner. An autonomous agent is embodied through some manifestation that allows it to sense its environment. Its embodiment concerns its physical body that it uses to move around its environment, its sensors that it uses to gain information about itself in relation to the environment, and its brain that it uses to process sensory information. The agent exists as part of the environment, and its perception and actions are determined by the way it interacts with the environment and other agents through its embodiment in a dynamic process.  The agent’s embodiment may consist of a physical manifestation in the real world, such as a human, industrial robot or an autonomous vehicle, or it can have a simulated or artificial manifestation in a virtual environment. If it deals exclusively with abstract information, for example a web crawling agent such as Googlebot, then its environment can be considered from a design perspective to be represented by an n-dimensional space, and its sensing capabilities relate to its movement around that space. Likewise, a real environment for a human or robotic agent can be considered to be an n-dimensional space, and the agent’s embodiment relates to its ability to gain information about the real environment as it moves around. We can consider an agent’s body as a sensory input-capturing device. For example, the senses in a human body is dictated by its embodiment – that is, the entire body itself has numerous sensors that occur throughout the body, enabling our mind to get a ‘picture’ of the whole body as it interacts with the environment (see the image at the beginning of this chapter). Pfeiffer and Scheier (1999) stress the important role that embodiment and sensory-motor coordination has to play in determining intelligent behaviour (see quote also at the beginning of this chapter). Craig Reynolds (1999) describes further features of autonomous agents that can be used to distinguish various classes of autonomous agents such as situatedness and reactive behaviour. We can define situatedness in the following manner. An autonomous agent is situated within its environment, sharing it with other agents and objects. Therefore its behaviour is determined from a first-person perspective by its agent-to-agent interactions and agent-to-environment interactions. Autonomous agents exhibit a range of behaviours, from purely reactive to more deliberative or cognitive.  An autonomous agent is said to be situated in an environment shared by other agents and/or other objects. An agent can be isolated existing by itself, for example a data mining agent searching for patterns in a database, situated in an abstract n-dimensional space that represents the database environment. A situated agent can have a range of behaviours, from purely reactive where the agent is governed by the stimulus it receives from its senses, to cognitive, where the agent deliberates on the stimulus it receives and decides on appropriate courses of actions. With a purely reactive approach, there is less need for the agent to maintain a representation of the environment – the environment is its own database that it can simply ‘look up’ by interacting directly with it. A cognitive agent, on the other hand, uses representations of what is happening in the environment in order to make decisions. (We can think of these as maps as discussed in the previous chapter). Reynolds notes that combinations of the attributes embodied, situated and reactive define distinct classes of autonomous agents, some of which are shown in Table 5.3. However, unlike Reynolds, we will consider that all agents are both situated in some environment (be it a real, virtual or an abstract environment, represented by some n-dimensional space) and embodied with the ability to sense and interact with its environment in some manner, whether explicitly or implicitly.  We can design NetLogo turtle agents introduced in the previous chapter to include sensing capabilities using the embodied, situated design perspective. One reason for doing this is that we would like to use turtle agents to search mazes (and not just to draw them) in order to demonstrate searching behaviour using animated mapping techniques (see next section 5.4 and also Chapter 8). Searching is an important process that an agent must perform in order to adequately carry out many tasks. However, before the agent is able to search, it must first have the ability to sense objects and other agents that exist in the environment in order to find the object or agent it is searching for.  We could easily adopt a disembodied solution to maze searching. In this approach, the turtle agent doing the searching simply follows paths whose movement has been pre-defined within the program (like with the maze drawing models described in the previous chapter). An alternative, embodied approach is that the turtle agent is programmed with the ability to ‘look’ at the maze environment, interpret what it ‘sees’, and then choose which path it wants to take, not being restricted to the pre-defined paths that were required for the disembodied solution.$$$﻿Just as for movement in the previous chapter, our body, mind and senses form the basis of many common metaphors used in human language. Our understanding as humans is derived from fundamental concepts related to our physical embodiment in the real world, and the senses we use to perceive it. We often express and understand abstract concepts that we cannot sense directly based on these fundamental concepts. Table 5.1 provides a list of common phrases that illustrates this.  Traditionally, using a classification first attributed to Aristotle, the five main senses are considered to be sight, hearing, smell, taste and touch (as included in Table 5.1). Sometimes people also refer to a ‘sixth sense’ (also sometimes called extra sensory perception or ESP) – we can consider the term as a metaphor that people use in natural language for describing unconscious thought processes in terms of another sense. It is a mistake to consider a ‘sixth sense’ as being real, as there is very little scientific evidence to support it. However, our bodies do in fact have much more than just the five basic senses as shown by the selection in Table 5.2.  From a design perspective, we can consider every agent to be embodied in some manner, using senses to gain information about its environment. We can define embodiment in the following manner. An autonomous agent is embodied through some manifestation that allows it to sense its environment. Its embodiment concerns its physical body that it uses to move around its environment, its sensors that it uses to gain information about itself in relation to the environment, and its brain that it uses to process sensory information. The agent exists as part of the environment, and its perception and actions are determined by the way it interacts with the environment and other agents through its embodiment in a dynamic process.  The agent’s embodiment may consist of a physical manifestation in the real world, such as a human, industrial robot or an autonomous vehicle, or it can have a simulated or artificial manifestation in a virtual environment. If it deals exclusively with abstract information, for example a web crawling agent such as Googlebot, then its environment can be considered from a design perspective to be represented by an n-dimensional space, and its sensing capabilities relate to its movement around that space. Likewise, a real environment for a human or robotic agent can be considered to be an n-dimensional space, and the agent’s embodiment relates to its ability to gain information about the real environment as it moves around. We can consider an agent’s body as a sensory input-capturing device. For example, the senses in a human body is dictated by its embodiment – that is, the entire body itself has numerous sensors that occur throughout the body, enabling our mind to get a ‘picture’ of the whole body as it interacts with the environment (see the image at the beginning of this chapter). Pfeiffer and Scheier (1999) stress the important role that embodiment and sensory-motor coordination has to play in determining intelligent behaviour (see quote also at the beginning of this chapter). Craig Reynolds (1999) describes further features of autonomous agents that can be used to distinguish various classes of autonomous agents such as situatedness and reactive behaviour. We can define situatedness in the following manner. An autonomous agent is situated within its environment, sharing it with other agents and objects. Therefore its behaviour is determined from a first-person perspective by its agent-to-agent interactions and agent-to-environment interactions. Autonomous agents exhibit a range of behaviours, from purely reactive to more deliberative or cognitive.  An autonomous agent is said to be situated in an environment shared by other agents and/or other objects. An agent can be isolated existing by itself, for example a data mining agent searching for patterns in a database, situated in an abstract n-dimensional space that represents the database environment. A situated agent can have a range of behaviours, from purely reactive where the agent is governed by the stimulus it receives from its senses, to cognitive, where the agent deliberates on the stimulus it receives and decides on appropriate courses of actions. With a purely reactive approach, there is less need for the agent to maintain a representation of the environment – the environment is its own database that it can simply ‘look up’ by interacting directly with it. A cognitive agent, on the other hand, uses representations of what is happening in the environment in order to make decisions. (We can think of these as maps as discussed in the previous chapter). Reynolds notes that combinations of the attributes embodied, situated and reactive define distinct classes of autonomous agents, some of which are shown in Table 5.3. However, unlike Reynolds, we will consider that all agents are both situated in some environment (be it a real, virtual or an abstract environment, represented by some n-dimensional space) and embodied with the ability to sense and interact with its environment in some manner, whether explicitly or implicitly.  We can design NetLogo turtle agents introduced in the previous chapter to include sensing capabilities using the embodied, situated design perspective. One reason for doing this is that we would like to use turtle agents to search mazes (and not just to draw them) in order to demonstrate searching behaviour using animated mapping techniques (see next section 5.4 and also Chapter 8). Searching is an important process that an agent must perform in order to adequately carry out many tasks. However, before the agent is able to search, it must first have the ability to sense objects and other agents that exist in the environment in order to find the object or agent it is searching for.  We could easily adopt a disembodied solution to maze searching. In this approach, the turtle agent doing the searching simply follows paths whose movement has been pre-defined within the program (like with the maze drawing models described in the previous chapter). An alternative, embodied approach is that the turtle agent is programmed with the ability to ‘look’ at the maze environment, interpret what it ‘sees’, and then choose which path it wants to take, not being restricted to the pre-defined paths that were required for the disembodied solution.
EN02	1	﻿There’s an old joke, well known in database circles, to the effect that what users really want (and always have wanted, ever since database systems were first invented) is for somebody to implement the go faster! command. Well, I’m glad to be able to tell you that, as of now, somebody finally has ... This book is all about a radically new database implementation technology, a technology that lets us build database management systems (DBMSs) that are “blindingly fast”—certainly orders of magnitude faster than any previous system. As explained in the preface, that technology is known as The TransRelationaltm Model, or the TR model for short (the terms TR technology and, frequently, just TR are also used). As also explained in the preface, the technology is the subject of a United States patent (U.S. Patent No. 6,009,432, dated December 28th, 1999), listed as reference [63] in Appendix B at the back of this book; however, that reference is usually known more specifically as the Initial Patent, because several follow-on patent applications have been applied for at the time of writing. This book covers material from the Initial Patent and from certain of those follow-on patents as well. The TR model really is a breakthrough. To say it again, it allows us to build DBMSs that are orders of magnitude faster than any previous system. And when I say “any previous system,” I don’t just mean previous relational systems. It’s an unfortunate fact that many people still believe that the fastest relational system will never perform as well as the fastest nonrelational system. Indeed, it’s exactly that belief that accounts in large part for the continued existence and use of older, nonrelational systems such as IMS [25,57] and IDMS [14,25], despite the fact that—as is well known—relational systems are far superior from the point of view of usability, productivity, and the like. However, a relational system implemented using TR technology should dramatically outperform even the fastest of those older nonrelational systems, finally giving the lie to those old performance arguments and making them obsolete (not before time, either). I must also make it clear that I don’t just mean that queries should be faster under TR (despite the traditional emphasis in relational systems on queries in particular)—updates should be faster as well. Nor do I mean that TR is suitable only for decision support systems—it’s eminently suitable for transaction processing systems, too (though it’s probably fair to say that TR is particularly suitable for systems in which read-only operations predominate, such as data warehouse and data mining systems). And one last preliminary remark: You’re probably thinking that the performance advantages I’m claiming must surely come at a cost: perhaps poor usability, or less functionality, or something (there’s no free lunch, right?). Well, I’m pleased to be able to tell you that such is not the case. The fact is, TR actually provides numerous additional benefits, over and above the performance benefit—for example, in the areas of database and system administration. Thus, I certainly don’t want you to think that performance is the only argument in favor of TR. We’ll take a look at some of those additional benefits in Chapters 2 and 15, and elsewhere in passing. (In fact, a detailed summary of all of the TR benefits appears in Chapter 15, in Section 15.4. You might like to take a quick look at that section right now, just to get an idea of how much of a breakthrough the TR model truly is.) As I said in the preface, I believe TR technology is one of the most significant advances—quite possibly the most significant advance—in the data management field since E. F. Codd first invented the relational model (which is to say, since the late 1960s and early 1970s; see references [5 7], also reference [35]). As I also said in the preface, TR represents among other things a highly effective way to implement the relational model, as I hope to show in this book. In fact, the TR model—or, rather, the more general technology of which the TR model is just one specific but important manifestation—represents an effective way to implement data management systems of many different kinds, including but not limited to the following: ■■SQL DBMSs ■■Data warehouse systems ■■Information access tools ■■Data mining tools ■■Object/relational DBMSs ■■Web search engines ■■Main-memory DBMSs ■■Temporal DBMSs ■■Business rule systems ■■Repository managers ■■XML document storage and retrieval systems ■■Enterprise resource planning tools as well as relational DBMSs in particular. Informally, we could say we’re talking about a backend technology that’s suitable for use with many different frontends. In planning this book, however, I quickly decided that my principal focus should be on the application of the technology to implementing the relational model specifically. Here are some of my reasons for that decision: ■Concentrating on one particular application should make the discussions and examples more concrete and therefore, I hope, easier to follow and understand. ■■More significantly, the relational model is of fundamental importance; it’s rock solid, and it will endure. After all, it really is the best contender, so far as we know, for the role of “proper theoretical foundation” for the entire data management field. One hundred years from now, I fully expect database systems still to be firmly based on Codd’s relational model—even if they’re advertised as “object/relational,” or “temporal,” or “spatial,” or whatever. See Chapter 15 for further discussion of such matters. ■■If your work involves data management in any of its aspects, then you should already have at least a nodding acquaintance with the basic ideas of the relational model. Though I feel bound to add that if that “nodding acquaintance” is based on a familiarity with SQL specifically, then you might not know as much as you should about the model as such, and you might know “some things that ain’t so.” I’ll come back to this point in a few moments. ■■The relational model is an especially good fit with TR ideas; I mean, it’s a very obvious candidate for implementation using those ideas. Why? Because the relational model is at a uniform, and high, level of abstraction; it’s concerned purely with what a database system is supposed to look like to the user, and has absolutely nothing to say about what the system might look like internally. As many people would put it, the relational model is logical, not physical. ﻿The main purpose of this chapter is to explain in more detail some of the problems that arise in connection with what the lawyers call “prior art”—meaning, in the case at hand, systems that use the traditional direct-image approach to implementation. Of course, you can skip this material if you’re already familiar with conventional implementation technology. However, this first section does also introduce a few simple relational ideas, and you might at least want to make sure you’re familiar with those and fully understand them. Consider Fig. 2.1, which depicts a relation called S (“suppliers”). Observe that each supplier has a supplier number (S#), unique to that supplier;1 a supplier name (SNAME), not necessarily unique (though in fact the sample names shown in the figure do happen to be unique); a rating or status value (STATUS); and a location (CITY). I’ll use this example to remind you of a few of the most fundamental relational terms and concepts. ■First of all, a relation can, obviously enough, be pictured as a table. However, a relation is not a table.2 A picture of a thing isn’t the same as the thing! In fact, the difference between a thing and a picture of that thing is another of the great logical differences (see the remarks on this latter notion in Chapter 1, near the beginning of Section 1.3). One problem with thinking of a relation as a table is that it suggests that certain properties of tables—for example, the property that the rows are in a certain top-to-bottom order—apply to relations too, when in fact they don’t (see below). ■■Each of the five suppliers is represented by a tuple (pronounced as noted in Chapter 1 to rhyme with “couple”). Tuples are depicted as rows in figures like Fig. 2.1, but tuples aren’t rows. ■■Each supplier tuple contains four values, called attribute values; that is, the suppliers relation involves four attributes, called S#, SNAME, STATUS, and CITY. Attributes are depicted as columns in figures like Fig. 2.1, but attributes aren’t columns. ■■Attributes are defined over data types (types for short, also known as domains), meaning that every value of the attribute in question is required to be a value of the type in question. Types can be either system-defined (built in) or user-defined. For example, attribute STATUS might be defined over the system-defined type INTEGER (STATUS values are integers), while attribute SNAME might be defined over the user-defined type NAME (SNAME values are names). Note: For definiteness, I’ll assume these specific types throughout what follows, where it makes any difference. I’ll also assume that attribute S# is defined over a user-defined type with the same name (that is, S#), and attribute CITY is defined over the system-defined type CHAR (meaning character strings of arbitrary length). ■■The tuples of a relation are all distinct. In fact, relations never contain duplicate tuples—the tuples of a relation form a mathematical set, and sets in mathematics don’t contain duplicate elements. Note: People often complain about this aspect of the relational model, but in fact there are good practical reasons for not permitting duplicate tuples. A detailed discussion of the point is beyond the scope of this book; see any of references [13], [20], or [33] if you want to pursue the matter. ■■There’s no top-to-bottom ordering to the tuples of a relation. Although figures like Fig. 2.1 clearly suggest there is such an ordering, there really isn’t—to say it again, the tuples of a relation form a mathematical set, and sets in mathematics have no ordering to their elements. Note: It follows from this point that we could draw several different pictures that would all represent the same relation. An analogous remark applies to the point immediately following. ■There’s no left-to-right ordering to the attributes of a relation. Again, figures like Fig. 2.1 clearly suggest there is such an ordering, but there really isn’t; like the tuples, the attributes of a relation form a set, and thus have no ordering. (By the same token, there’s no left-to-right ordering to the components of a tuple, either.) No relation can have two or more attributes with the same name. ■■The suppliers relation is in fact a base relation specifically. In general, we distinguish between base and derived relations; a derived relation is one that is derived from, or defined in terms of, other relations, and a base relation is one that isn’t derived in this sense. Loosely speaking, in other words, the base relations are the “given” ones—they’re the ones that make up the actual database—while the derived ones are views, snapshots, query results, and the like [33]. For example, given the base relation of Fig. 2.1, the result of the query “Get suppliers in London” is a derived relation that looks like this: Another way to think about the distinction is that base relations exist in their own right, while derived ones don’t—they’re existence-dependent on the base relations. ■■Every relation has at least one candidate key (or just key for short), which serves as a unique identifier for the tuples of that relation. In the case of the suppliers relation (and the derived relation just shown as well), there’s just one key, namely {S#}, but relations can have any number of keys, in general. Note: It’s important to understand that keys are always sets of attributes (though the set in question might well contain just a single attribute). For this reason, in this book I’ll always show key attributes enclosed in braces, as in the case at hand—braces being used by convention to bracket the elements that make up a set. ■■As you probably know, it’s customary (though not obligatory) to choose, for any given relation, one of that relation’s candidate keys—possibly its sole candidate key—as primary; thus, for example, we might say in the case of the suppliers relation that {S#} is not just a key but the “primary” key. In figures like Fig. 2.1, I’ll follow the convention of identifying primary key attributes by double underlining. ■■Finally, relations can be operated on by a variety of relational operators. In general, a relational operator is an operator that takes zero or more relations as input and produces a relation as output. Examples include the well-known operators restrict, project, join, and so on. ﻿In order to understand the TR approach to implementing the relational model, it’s necessary to be very clear over three distinct levels of the system, which I’ll refer to as the three levels of abstraction (since each level is an abstraction of the one below, loosely speaking). The three levels, or layers, are: 1. The relational (or user) level 2. The file level 3. The TR level ■Level 1, which corresponds to the database as seen by the user, is the relational level. At this level, the data is perceived as relations, including, perhaps, the suppliers relation S discussed in Section 2.1 (and illustrated in Fig. 2.1) in the previous chapter. ■■Level 3 is the fundamental TR implementation level. At this level, data is represented by means of a variety of internal structures called tables. Please note immediately that those TR tables are NOT tables in the SQL sense and do NOT correspond directly to relations at the user level. ■Level 2 is a level of indirection between the other two. Relations at the user or relational level are mapped to files at this level, and those files are then mapped to tables at the TR level. Of course, the mappings go both ways; that is, tables at the TR level map to files at the next level up, and those files then map to relations at the top level. Note: As I’m sure you know, map is a synonym for transform (and I’ll be using the term in that sense throughout this book); thus, we’re already beginning to touch on the TR transforms that were mentioned in Chapter 1. However, there’s a great deal more to it, as we’ll soon see. Please now observe that each level has its own terminology: relational terms at the user level, file terms at the file level, and table terms at the TR level. Using different terms should, I hope, help you keep the three levels distinct and separate in your mind; for that reason, I plan to use the three sets of terms consistently and systematically throughout the rest of this book. Having said that, I now need to say too that I’m well aware that some readers might object to my choice of terms—perhaps even find them confusing—for at least the following two reasons: ■■First, the industry typically uses the terminology of tables, not relations, at the user level—almost exclusively so, in fact. But I’ve already explained some of my rationale for wanting to use relational terms at that level (see the previous chapter, Section 2.1), and I’m going to give some additional reasons in the next section. ■Second, the industry also typically tends to think of files as a fairly “physical” construct. In fact, I did the same thing myself in the previous chapter, somewhat, though I was careful in that chapter always to be quite clear that the files I was talking about were indeed physically stored files specifically. By contrast, the files I’ll be talking about in the rest of the book are not physically stored; instead, they’re an abstraction of what’s physically stored, and hence a “logical” construct, not a physical one. (Though it wouldn’t be wrong to think of them as “slightly more physical” than the user-level relations, if you like.) If you still think my terms are confusing, then I’m sorry, but for better or worse they’re the terms I’m going to use. One final point: When I talk of three levels, or layers, of abstraction, I don’t mean that each of those levels is physically materialized in any concrete sense—of course not. The relational level is only a way of looking at the file level, a way in which certain details are ignored (that’s what “level of abstraction” means). Likewise, the file level in turn is only a way of looking at the TR level. Come to that, the TR level in turn is only a way of looking at the bits and bytes that are physically stored; that is, the TR level is itself—as already noted in Chapter 1, Section 1.2—still somewhat abstract. In a sense, the bits-and-bytes level is the only level that’s physically materialized.1 Since the focus of this book is on the use of TR technology to implement the relational model specifically, the topmost (user) level is relational by definition. In other words, the user sees the database as a set of relations, made up of attributes and tuples as explained in Chapter 2. For simplicity, I’m going to assume those relations are all base relations specifically (again, see Chapter 2); that is, I’ll simply assume, barring explicit statements to the contrary, that any relation that’s named and is included in the database is in fact a base relation specifically, and I won’t usually bother to use the “base” qualifier. Also, of course, the user at the relational level has available a set of relational operators—restrict, project, join, and so forth—for querying the relations in the database, as well as the usual INSERT, DELETE, and UPDATE operators for updating them. Note: If I wanted to be more precise here, I’d have to get into the important distinction between relation values and relation variables. Relational operators like join operate on relation values, while update operators like INSERT operate on relation variables. Informally, however, it’s usual to call them all just relations, and—somewhat against my better judgment—I’ve decided to follow that common usage (for the most part) in the present book. For further discussion of such matters, see either reference [32] or reference [40]. Now, given the current state of the IT industry, the user level in a real database system will almost certainly be based on SQL, not on the relational model. As a consequence, users will typically tend to think, not in terms of relational concepts as such, but rather in terms of SQL analogs of those concepts. For example, there isn’t any explicit project operator, as such, in SQL; instead, such an operation has to be formulated in terms of SQL’s SELECT and FROM operators, and the user has to think in terms of those SQL operators, as in this example (“Project suppliers over supplier number and city name”): ﻿Now (at last) I can begin to explain the TR model in detail. As I mentioned several times in Part I, TR is indeed still a model, and thus, like the relational model, still somewhat abstract. At the same time, however, it’s at a much lower level of abstraction than the relational model; it can be thought of as being closer to the physical implementation level (“closer to the metal”), and accordingly more oriented toward issues of performance. In particular, it relies heavily on the use of pointers—a concept deliberately excluded from the relational model, of course, for reasons discussed in references [9], [30], [40], and many other places—and its operators are much more procedural in nature than those of the relational model. (What I mean by this latter remark is that code that makes use of those operators is much more procedural than relational code is, or is supposed to be.) What’s more, reference [63] includes detailed, albeit still somewhat abstract, algorithms for implementing those operators. Note: These remarks aren’t meant to be taken as criticisms, of course; I’m just trying to capture the essence of the TR model by highlighting some of its key features. Despite its comparatively low-level nature, the fact remains that, to say it again, TR is indeed a model, and thus capable of many different physical realizations. In what follows, I’ll talk for much of the time in terms of just one possible realization—it’s easier on the reader to be concrete and definite—but I’ll also mention some alternative implementation schemes on occasion. Note that the alternatives in question have to do with the implementation of both data structures and corresponding access algorithms. In particular, bear in mind that both main-memory and secondary-storage implementations are possible. Now, this book is meant to be a tutorial; accordingly, I want to focus on showing the TR model in action (as it were)—that is, showing how it works in terms of concrete examples—rather than on describing the abstract model as such. Also, many TR features are optional, in the sense that they might or might not be present in any given implementation or application of the model, and it’s certainly not worth getting into all of those optional features in a book of this kind. Nor for the most part is it worth getting into the optionality or otherwise of those features that are discussed—though I should perhaps at least point out that options do imply a need for decisions: Given some particular option X, some agency, at some time, has to decide whether or not X should be exercised. For obvious reasons, I don’t want to get into a lot of detail on this issue here, either. Suffice it to say that I don’t think many of those decisions, if any at all, should have to be made at database design time (by some human being) or at run time (by the system itself); in fact, I would expect most of them to be made during the process of designing the DBMS that is the specific TR implementation in question. In other words, I don’t think the fact that those decisions do have to be made implies that a TR implementation will therefore suffer from the same kinds of problems that arise in connection with direct-image systems, as discussed in Chapter 2. It follows from all of the above that this book is meant as an introduction only; many topics are omitted and others are simplified, and I make no claims of completeness of any kind. Now let’s get down to business. In this chapter and the next,1 we’ll be looking at what are clearly the most basic TR constructs of all: namely, the Field Values Table and the Record Reconstruction Table, both of which were mentioned briefly in the final section of the previous chapter. These two constructs are absolutely fundamental—everything else builds on them, and I recommend as strongly as I can that you familiarize yourself with their names and basic purpose before you read much further. Just to remind you: ■■The Field Values Table contains the field values from a given file, rearranged in a way to be explained in Section 4.3. ■■The Record Reconstruction Table contains information that allows records of the given file to be reconstructed from the Field Values Table, in a way to be explained in Section 4.4. In subsequent chapters I’ll consider various possible refinements of those core concepts. Note: Those refinements might be regarded in some respects as “optional extras” or “frills,” but some of them are very important—so much so, that they’ll almost certainly be included in any concrete realization of the TR model, as we’ll see. Let r be some given record within some given file at the file level. Then the crucial insight underlying the TR model can be characterized as follows: The stored form of r involves two logically distinct pieces, a set of field values and a set of “linkage” information that ties those field values together, and there’s a wide range of possibilities for physically storing each piece. In direct-image systems, the two pieces (the field values and the linkage information) are kept together, of course; in other words, the linkage information in such systems is represented by physical contiguity. In TR, by contrast, the two pieces are kept separate; to be specific, the field values are kept in the Field Values Table, and the linkage information is kept in the Record Reconstruction Table. That separation makes TR strikingly different from virtually all previous approaches to implementing the relational model (see Chapters 1 and 2), and is the fundamental source of the numerous benefits that TR technology is capable of providing. In particular, it means that TR data representations are categorically not a direct image of what the user sees at the relational level. Note: One immediate advantage of the separation is that the Field Values Table and the Record Reconstruction Table can both be physically stored in a way that is highly efficient in terms of storage space and access time requirements. However, we’ll see many additional advantages as well, both in this chapter and in subsequent ones. ﻿This chapter continues our examination of the core constructs of the TR model (principally the Field Values and Record Reconstruction Tables). However, the chapter is rather more of a potpourri than the previous one. Its structure is as follows. Following this short introductory section, Section 5.2 offers some general observations regarding performance. Section 5.3 then briefly surveys the TR operators, and Sections 5.4 and 5.5 take another look at how the Record Reconstruction Table is built and how record reconstruction is done. Sections 5.6 and 5.7 describe some alternative perspectives on certain of the TR constructs introduced in Chapter 4. Finally, Section 5.6 takes a look at some alternative ways of implementing some of the TR structures and algorithms also first described in that previous chapter. It seems to me undeniable that the mechanisms described in the previous chapter for representing and reconstructing records and files are vastly different from those found in conventional DBMSs, and I presume you agree with this assessment. At the same time, however, they certainly look pretty complicated ... How does all of that complexity square with the claims I made in Chapter 1 regarding good performance? Let me remind you of some of the things I said there: Well, let me say a little more now regarding query performance specifically (I haven’t really discussed updates yet, so I’ll have to come back to the question of update performance later—actually in the next chapter). Now, any given query involves two logically distinct processes: a) Finding the data that’s required, and then b) Retrieving that data. TR is designed to exploit this fact. Precisely because it separates field value information and linkage information, it can treat these two processes more or less independently. To find the data, it uses the Field Values Table; to retrieve it, it uses the Record Reconstruction Table. (These characterizations aren’t 100 percent accurate, but they’re good to a first approximation—good enough for present purposes, at any rate.) And the Field Values Table in particular is designed to make the finding of data very efficient (for example, via binary search), as we saw in Chapter 4. Of course, it’s true that subsequent retrieval of that data then involves the record reconstruction process, and this latter process in turn involves a lot of pointer chasing, but: ■■Even in a disk-based implementation, the system will do its best to ensure that pertinent portions of both the Field Values Table and the Record Reconstruction Table are kept in main memory at run time, as we’ll see in Part III. Assuming this goal is met, the reconstruction will be done at main-memory speeds. ■■The “frills” to be discussed in Chapters 7 9 (as well as others that are beyond the scope of this book) have the effect, among other things, of dramatically improving the performance of various aspects of the reconstruction process. ■■Most important of all: Almost always, finding the data that’s wanted is a much bigger issue than returning that data to the user is. In a sense, the design of the TR internal structures is biased in favor of the first of these issues at the expense of the second. Observe the implication: The more complex the query, the better TR will perform—in comparison with traditional approaches, that is. (Of course, I don’t mean to suggest by these remarks that record reconstruction is slow or inefficient—it isn’t—nor that TR performs well on complex queries but not on simple ones. I just want to stress the relative importance of finding the data in the first place, that’s all.) I’d like to say more on this question of query performance. In 1969, in his very first paper on the relational model [5], Codd had this to say: Once aware that a certain relation exists, the user will expect to be able to exploit that relation using any combination of its attributes as “knowns” and the remaining attributes as “unknowns,” because the information (like Everest) is there. This is a system feature (missing from many current information systems) which we shall call (logically) symmetric exploitation of relations. Naturally, symmetry in performance is not to be expected. —E. F. Codd Note: I’ve reworded Codd’s remarks just slightly here. In particular, the final sentence (the caveat concerning performance) didn’t appear in the original 1969 paper [5] but was added in the expanded 1970 version [6]. Anyway, the point I want to make is that the TR approach gives us symmetry in performance, too—or, at least, it comes much closer to doing so than previous approaches ever did. This is because, as we saw in Chapter 4, the separation of field values from linkage information effectively allows the data to be physically stored in several different sort orders simultaneously. When Codd said “symmetry in performance is not to be expected,” he was tacitly assuming a direct-image style of implementation, one involving auxiliary structures like those described in Chapter 2. However, as I said in that chapter: [Auxiliary structures such as pointer chains and] indexes can be used to impose different orderings on a given file and thus (in a sense) “level the playing field” with respect to different processing sequences; all of those sequences are equally good from a logical point of view. But they certainly aren’t equally good from a performance point of view. For example, even if there’s a city index, processing suppliers in city name sequence will involve (in effect) random accesses to storage, precisely because the supplier records aren’t physically stored in city name sequence but are scattered all over the disk. —from Chapter 2 As we’ve seen, however, these remarks simply don’t apply to the TR data representation. And now I can address another issue that might possibly have been bothering you. We’ve seen that the TR model relies heavily on pointers. Now, the CODASYL “network model” [14,25] also relies heavily on pointers—as the “object model” [3,4,28,29] and “hierarchic model” [25,56] both do also, as a matter of fact—and I and many other writers have criticized it vigorously in the past on exactly that score (see, for example, references [10], [21], and [37]). So am I arguing out of both sides of my mouth here? How can TR pointers be good while CODASYL pointers are bad? ﻿This is the last chapter in this part of the book. In it, I want to describe a rather different approach to the problem of implementing the TR model on disk: more specifically, to the problem of minimizing disk seeks. Note immediately, therefore, that the approach in question can be regarded in part as an alternative to file banding as discussed in Chapter 13—but only in part, because in fact file banding can be used in combination with the approach to be described, as we’ll see in Section 14.4. Note too that, as with the discussion of file banding in Chapter 13, we’re primarily concerned here with how to deal with the “large file” that remains after file factoring has been used to get all of the “small files” into memory. But first things first. As we know, the basic problem with TR on the disk is that if we’re not careful, the zigzags can splay out all over the disk. Well, if the splay problem is caused by the zigzags, then let’s get rid of the zigzags! Recall from Chapter 5 (Section 5.8) that the linkage information that lets us reconstruct records doesn’t have to be implemented as zigzags specifically—other possibilities exist, with (of course) different performance characteristics. The approach to be described in this chapter exploits this idea; essentially, what it does is replace the zigzags by a different kind of structure called a star.  Let me illustrate this idea right away. Fig. 14.1 shows the Field Values Table and corresponding Record Reconstruction Table from Figs. 13.2 and 13.3 in Chapter 13—except that, for pedagogic reasons, I’ve shown the Field Values Table in uncondensed form. Fig. 14.2 then highlights one particular zigzag from Fig. 14.1 (actually the one for part P7), and Fig. 14.3 shows what happens if we replace that zigzag by a star.  As you can see, where Fig. 14.2 has a ring of pointers (implemented within the Record Reconstruction Table and conceptually superimposed on the Field Values Table), Fig. 14.3 has a star of pointers instead. Cell [7,1], which corresponds to the P# value P7, serves as the center or core of that star. Three pointers emanate from that core and point to cells [6,2], [8,3], and [4,4], respectively; those cells correspond to the PNAME value Nut, the WEIGHT value 19.0, and the CC# value cc1, respectively. Those three pointers, which (as Fig. 14.3 indicates) are all two-way and can therefore be traversed in either direction, serve as the spokes or rays of the star. Now, the star in the figure clearly does support reconstruction of the record for the part in question (part P7). To be specific: a) If we start at the core, we can simply follow the three spoke pointers outward to obtain the other three field values. b) If we start at any other point, we can follow the corresponding spoke pointer inward to the core and then proceed as under a) above—with the exception that, if we get to the core by following spoke pointer sp inward, then of course there’s no need to follow that particular spoke sp outward again. Note: As a matter of fact, we never need to follow a spoke outward from the core within the Record Reconstruction Table as such; we only need to be able to go from the core outward to cells within the Field Values Table. Now, you might have already realized that, for any given zigzag, there are several distinct but equivalent stars—it just depends on which field we choose as the core. I’ll return to this point in Section 14.3. You might also have realized that the record reconstruction algorithm as just outlined displays asymmetric performance—access via the core field will be faster than access via any other field, because stars (unlike zigzags) are an inherently asymmetric structure—and I’ll return to this point in Section 14.5. The structure of the chapter is as follows. Following this introductory section, Section 14.2 gives a simple example to illustrate the basic ideas behind star structures. Section 14.3 elaborates on and generalizes that example. Section 14.4 shows how the ideas from the first three sections work on the disk (those previous sections are principally concerned with a memory-based implementation only). Finally, Section 14.5 discusses the use of controlled redundancy in connection with star structures.  As in the previous chapter, the basic problem we’re trying to deal with is how to get the best possible performance out of the “large” Record Reconstruction Table in a disk-based system. So I’ll base my discussions on the same running example as in that previous chapter; to be specific, I’ll assume once again that we’ve factored the parts file into large and small files that look like this:  However, we’re interested here in the large file exclusively. Fig. 14.4 shows a sample value for that file (extracted from Fig. 13.1 in Chapter 13). And we’ve already seen a Field Values Table and a zigzag-based Record Reconstruction Table for that file in Fig. 14.1 above. Note: While the file shown in Fig. 14.4 is obviously not very large, let me remind you that we’re really supposed to be dealing with files of millions or even billions of records, and the data in those files isn’t supposed to display any “statistical clumpiness” at all.  Now, despite the fact that we’re really supposed to be talking about a disk implementation, it’s convenient to pretend for the time being that everything’s in memory, and I’ll adopt that pretense until further notice. So how do we proceed? Well, since (as we’ve already seen) stars are asymmetric, the first thing we have to do is decide what the core’s going to be; in other words, we first have to choose a core field (much as we had to choose a characteristic field in connection with with banding in the previous chapter).1 Suppose we choose field P#. Then Fig. 14.5 shows a corresponding star-based Record Reconstruction Table for the file of Fig. 14.4. Note: From this point forward, for convenience, I’ll abbreviate the term “star-based Record Reconstruction Table” to just star table, and similarly for zigzag table.$$$﻿This is the last chapter in this part of the book. In it, I want to describe a rather different approach to the problem of implementing the TR model on disk: more specifically, to the problem of minimizing disk seeks. Note immediately, therefore, that the approach in question can be regarded in part as an alternative to file banding as discussed in Chapter 13—but only in part, because in fact file banding can be used in combination with the approach to be described, as we’ll see in Section 14.4. Note too that, as with the discussion of file banding in Chapter 13, we’re primarily concerned here with how to deal with the “large file” that remains after file factoring has been used to get all of the “small files” into memory. But first things first. As we know, the basic problem with TR on the disk is that if we’re not careful, the zigzags can splay out all over the disk. Well, if the splay problem is caused by the zigzags, then let’s get rid of the zigzags! Recall from Chapter 5 (Section 5.8) that the linkage information that lets us reconstruct records doesn’t have to be implemented as zigzags specifically—other possibilities exist, with (of course) different performance characteristics. The approach to be described in this chapter exploits this idea; essentially, what it does is replace the zigzags by a different kind of structure called a star.  Let me illustrate this idea right away. Fig. 14.1 shows the Field Values Table and corresponding Record Reconstruction Table from Figs. 13.2 and 13.3 in Chapter 13—except that, for pedagogic reasons, I’ve shown the Field Values Table in uncondensed form. Fig. 14.2 then highlights one particular zigzag from Fig. 14.1 (actually the one for part P7), and Fig. 14.3 shows what happens if we replace that zigzag by a star.  As you can see, where Fig. 14.2 has a ring of pointers (implemented within the Record Reconstruction Table and conceptually superimposed on the Field Values Table), Fig. 14.3 has a star of pointers instead. Cell [7,1], which corresponds to the P# value P7, serves as the center or core of that star. Three pointers emanate from that core and point to cells [6,2], [8,3], and [4,4], respectively; those cells correspond to the PNAME value Nut, the WEIGHT value 19.0, and the CC# value cc1, respectively. Those three pointers, which (as Fig. 14.3 indicates) are all two-way and can therefore be traversed in either direction, serve as the spokes or rays of the star. Now, the star in the figure clearly does support reconstruction of the record for the part in question (part P7). To be specific: a) If we start at the core, we can simply follow the three spoke pointers outward to obtain the other three field values. b) If we start at any other point, we can follow the corresponding spoke pointer inward to the core and then proceed as under a) above—with the exception that, if we get to the core by following spoke pointer sp inward, then of course there’s no need to follow that particular spoke sp outward again. Note: As a matter of fact, we never need to follow a spoke outward from the core within the Record Reconstruction Table as such; we only need to be able to go from the core outward to cells within the Field Values Table. Now, you might have already realized that, for any given zigzag, there are several distinct but equivalent stars—it just depends on which field we choose as the core. I’ll return to this point in Section 14.3. You might also have realized that the record reconstruction algorithm as just outlined displays asymmetric performance—access via the core field will be faster than access via any other field, because stars (unlike zigzags) are an inherently asymmetric structure—and I’ll return to this point in Section 14.5. The structure of the chapter is as follows. Following this introductory section, Section 14.2 gives a simple example to illustrate the basic ideas behind star structures. Section 14.3 elaborates on and generalizes that example. Section 14.4 shows how the ideas from the first three sections work on the disk (those previous sections are principally concerned with a memory-based implementation only). Finally, Section 14.5 discusses the use of controlled redundancy in connection with star structures.  As in the previous chapter, the basic problem we’re trying to deal with is how to get the best possible performance out of the “large” Record Reconstruction Table in a disk-based system. So I’ll base my discussions on the same running example as in that previous chapter; to be specific, I’ll assume once again that we’ve factored the parts file into large and small files that look like this:  However, we’re interested here in the large file exclusively. Fig. 14.4 shows a sample value for that file (extracted from Fig. 13.1 in Chapter 13). And we’ve already seen a Field Values Table and a zigzag-based Record Reconstruction Table for that file in Fig. 14.1 above. Note: While the file shown in Fig. 14.4 is obviously not very large, let me remind you that we’re really supposed to be dealing with files of millions or even billions of records, and the data in those files isn’t supposed to display any “statistical clumpiness” at all.  Now, despite the fact that we’re really supposed to be talking about a disk implementation, it’s convenient to pretend for the time being that everything’s in memory, and I’ll adopt that pretense until further notice. So how do we proceed? Well, since (as we’ve already seen) stars are asymmetric, the first thing we have to do is decide what the core’s going to be; in other words, we first have to choose a core field (much as we had to choose a characteristic field in connection with with banding in the previous chapter).1 Suppose we choose field P#. Then Fig. 14.5 shows a corresponding star-based Record Reconstruction Table for the file of Fig. 14.4. Note: From this point forward, for convenience, I’ll abbreviate the term “star-based Record Reconstruction Table” to just star table, and similarly for zigzag table.
EN33	1	﻿A regular feature in the New Scientist magazine is Enigma, a weekly puzzle entry which readers are invited to solve. In the 8 February 2003 issue [1] the following puzzle was published. First, draw a chessboard. Now number the horizontal rows 1, 2, ..., 8, from top to bottom and number the vertical columns 1, 2, ..., 8, from left to right.You have to put a whole number in each of the sixty-four squares, subject to the following: 1. No two rows are exactly the same. 2. Each row is equal to one of the columns, but not to the column with the same number as the row. 3. If N is the largest number you write on the chessboard then you must also write 1, 2, ...,N −1 on the chessboard. The sum of the sixty-four numbers you write on the chessboard is called your total. What is the largest total you can obtain? We are going to solve this puzzle here using Prolog. The solution to be described will illustrate two techniques: unification and generate-and-test. Unification is a built-in pattern matching mechanism in Prolog which has been used in [9]; for example, the difference list technique essentially depended on it. For our approach here, unification will again be crucial in that the proposed method of solution hinges on the availability of built-in unification. It will be used as a kind of concise symbolic pattern generating facility without which the current approach wouldn’t be viable. Generate-and-test is easily implemented in Prolog. Prolog’s backtracking mechanism is used to generate candidate solutions to the problem which then are tested to see whether certain of the problem-specific constraints are satisfied. Fig. 1.1 shows a board arrangement with all required constraints satisfied. It is seen that the first requirement is satisfied since the rows are all distinct. The second condition is also seen to hold whereby rows and columns are interrelated in the following fashion: We use the permutation to denote the corresponding column–to–row transformation. The board also satisfies the latter part of the second condition since no row is mapped to a column in the same position. In terms of permutations, this requirement implies that no entry remains fixed; these are those permutations which in our context are permissible. 2 The third condition is obviously also satisfied with N = 6. The board’s total is 301, not the maximum, which, as we shall see later, is 544. The solution scheme described below in i–v is based on first generating all feasible solutions (an example of which was seen in Sect. 1.2) and then choosing a one with the maximum total. i. Take an admissible permutation, such as π in (1.1). ii. Find an 8 ×8 matrix with symbolic entries whose rows and columns are interrelated by the permutation in i. As an example, let us consider for the permutation π two such matrices, M1 and M2, with M1 and M2 both satisfy conditions 1 and 2. We also observe that the pattern of M2 may be obtained from that of M1 by specialization (by matching the variables X1 and X6). Thus, any total achievable for M2 is also achievable for M1. For any given permissible permutation, we can therefore concentrate on the most general pattern of variables, M. (We term a pattern of variables most general if it cannot be obtained by specialization from a more general one.) All this is reminiscent of ‘unification’ and the ‘most general unifier’, and we will indeed be using Prolog’s unification mechanism in this step. iii. Verify condition 1 for the symbolic matrix M. 3 Once this test is passed, we are sure that also the latter part of condition 2 is satisfied. 4 iv. We now evaluate the pattern M. If N symbols have been used in M, assign the values 1, ...,N to them in reverse order by first assigning N to the most frequently occurring symbol, N − 1 to the second most frequently occurring symbol etc. The total thus achieved will be a maximum for the given pattern M. v. The problem is finally solved by generating and evaluating all patterns according to i–iv and selecting a one with the maximum total. The original formulation from the New Scientist uses a chessboard but the problem can be equally set with a square board of any size. In our implementation, we shall allow for any board size since this will allow the limitations of the method employed to be explored. We write matrices in Prolog as lists of their rows which themselves are lists. Permutations will be represented by the list of the bottom entries of their two-line representation; thus, [2, 3, 1, 5, 6, 7, 8, 4] stands for π in (1.1). First, we want to generate all permutations of a list. Let us assume that we want to do this by the predicate permute(+List,-Perm) and let us see how List = [1, 2, 3, 4] might be permuted. A permuted list, Perm = [3, 4, 1, 2] say, may be obtained by • Removing from List the entry E = 3, leaving the reduced list R = [1, 2, 4] • Permuting the reduced list R to get P = [4, 1, 2] • Assembling the permuted list as [E|P] = [3, 4, 1, 2] . Lists with a single entry are left unchanged. This gives rise to the definition with the predicate remove one(+List,?Entry,?Reduced) defined by (Here we remove either the head or an entry from the tail.) For a permutation to be admissible, all entries must have changed position. We implement this by To generate a list of N unbound variables, L, we use var list(+N,-L) which is defined in terms of length(-L,+N) By Matrices with distinct symbolic entries may now be produced by mapping; for example, a 3 × 2 matrix is obtained by It is now that Prolog shows its true strength: we use unification to generate symbolic square matrices with certain patterns.5 For example, we may produce a 3 × 3 symmetric matrix thus ﻿Many problems in Artificial Intelligence (AI) can be formulated as network search problems. The crudest algorithms for solving problems of this kind, the so called blind search algorithms, use the network’s connectivity information only. We are going to consider examples, applications and Prolog implementations of blind search algorithms in this chapter. Since implementing solutions of problems based on search usually involves code of some complexity, modularization will enhance clarity, code reusability and readibility. In preparation for these more complex tasks in this chapter, Prolog’s module system will be discussed in the next section. In some (mostly larger) applications there will be a need to use several input files for a Prolog project. We have met an example thereof already in Fig. 3.5 of [9, p. 85] where consult/1 was used as a directive to include in the database definitions of predicates from other than the top level source file. As a result, all predicates thus defined became visible to the user: had we wished to introduce some further predicates, we would have had to choose the names so as to avoid those already used. Clearly, there are situations where it is preferable to make available (that is, to export ) only those predicates to the outside world which will be used by other non-local predicates and to hide the rest. This can be achieved by the built-in predicates module/2 and use module/1 . As an illustrative example, consider the network in Fig. 2.1.1 The network connectivity in links.pl is defined by the predicate link/2 which uses the auxiliary predicate connect/2 (Fig. 2.2). The first line of links.pl is the module directive indicating that the module name is edges and that the predicate link/2 is to be exported. All other predicates defined in links.pl (here: connect/2) are local to the module and (normally) not visible outside this module. Suppose now that in some other source file, link/2 is used in the definition of some new predicate (Fig. 2.3). Then, the (visible) predicates from links.pl will be imported by means of the directive The new predicate thus defined may be used as usual: In our example, the predicate connect/2 will not be available for use (since it is local to the module edges that resides in links.pl). A local predicate may be accessed, however, by prefixing its name by the module name in the following fashion:3 Let us assume that for the network in Fig. 2.1 we want to find a path from the start node s to the goal node g. The search may be conducted by using the (associated) search tree shown in Fig. 2.4. It is seen that the search tree is infinite but highly repetitive. The start node s is at the root node (level 0). At level 1, all tree nodes are labelled by those network nodes which can be reached in one step from the start node. In general, a node labelled n in the tree at level _ has successor (or child ) nodes labelled s1, s2, . . . if the nodes s1, s2, . . . in the network can be reached in one step from node n. These successor nodes are said to be at level _ + 1. The node labelled n is said to be a parent of the nodes s1, s2, . . .. In Fig. 2.4, to avoid repetition, those parts of the tree which can be generated by expanding a node from some level above have been omitted. Some Further Terminology • The connections between the nodes in a network are called links. • The connections in a tree are called branches. • In a tree, a node is said to be the ancestor of another if there is a chain of branches (upwards) which connects the latter node to the former. In a tree, a node is said to be a descendant of another node if the latter is an ancestor of the former. In Fig. 2.5 we show, for later reference, the fully developed (and ’pruned’) search tree. It is obtained from Fig. 2.4 by arranging that in any chain of branches (corresponding to a path in the network) there should be no two nodes with the same label (implying that in the network no node be visited more than once). All information pertinent to the present problem is recorded thus in the file links.pl (Fig. 2.2) by link/2. Notice that the order in which child nodes are generated by link/2 will govern the development of the trees in Figs. 2.4 and 2.5: children of the same node are written down from left to right in the order as they would be obtained by backtracking; for example, the node labelled d at level 1 in Fig. 2.4 is expanded by (The same may be deduced, of course, by inspection from links.pl, Fig. 2.2.) link/2 will serve as input to the implementations of the search algorithms to be discussed next. The most concise and easy to remember illustration of Depth First is by the conduit model (Fig. 2.6). We start with the search tree in Fig. 2.5 which is assumed to be a network of pipes with inlet at the root node s. The tree is rotated by 90◦ counterclockwise and connected to a valve which is initially closed. The valve is then opened and the system is observed as it gets flooded under the influence of gravity. The order in which the nodes are wetted corresponds to Depth First. We may be tempted to use Prolog’s backtracking mechanism to furnish a solution by recursion; our attempt is shown in Fig. 2.7.4 However, it turns out that the implementation does not work due to cycling in the network. The query shown below illustrates the problems arising. We implement Depth First search incrementally using a new approach. The idea is keeping track of the nodes to be visited by means of a list, the so called list of open nodes, also called the agenda. This book–keeping measure will turn out to be amenable to generalization; in fact, it will be seen that the various search algorithms differ only in the way the agenda is updated. ﻿In this chapter we are going to discuss graph search algorithms and applications thereof for finding a minimum cost path from a start node to the goal node. The network search problem in Sect. 2.2 (Fig. 2.1) was devoid of any cost information. Let us now assume that the costs to traverse the edges of the graph in Fig. 2.1 are as indicated in Fig. 3.1. There are two possible interpretations of the figures in Fig. 3.1: they can be thought of as costs of edge traversal or, alternatively, as edge lengths. (We prefer the latter interpretation in which case, of course, Fig. 3.1 is not to scale.) The task is to determine a minimum length path connecting s and g, or, more generally, minimum length paths connecting any two nodes. The algorithms considered in this chapter assume the knowledge of an heuristic distance measure, H, between nodes. Values of H for the network in Fig. 3.1 are shown in Table 3.1. They are taken to be the estimated straight line distances between nodes and may be obtained by drawing the network in Fig. 3.1 to scale and taking measurements. Three algorithms will be introduced here: the A–Algorithm, Iterative Deepening A∗ and Iterative Deepening A∗–_. An estimated overall cost measure, calculated by the heuristic evaluation function F, will be attached to every path; it is represented as where G is the actual cost incurred thus far by travelling from the start node to the current node and H, the heuristic, is the estimated cost of getting from the current node to the goal node. Assume, for example, that in the network shown in Fig. 3.1 we start in d and want to end up in c. Equation (3.1) then reads for the path d → s → a (with obvious notation) as follows We know from Chap. 2 that for blind search algorithms the updating of the agenda is crucial: Breadth First comes about by appending the list of extended paths to the list of open paths; Depth First requires these lists to be concatenated the other way round. For the A–Algorithm, the updating of the agenda is equally important. The new agenda is obtained from the old one in the steps 1 _ and 2 _ below. 1 _ Extend the head of the old agenda to get a list of successor paths. An intermediate, ‘working’ list will be formed by appending the tail of the old agenda to this list. 2 _ The new agenda is obtained by sorting the paths in the working list from 1 _ in ascending order of their F–values. 3 _ The steps 1 _ and 2 _ are iterated until the path at the head of the agenda leads to the goal node. In the example shown in Fig. 3.2, the paths are prefixed by their respective F–values and postfixed by their respective G–values. Using this notation and the cost information, the example path in (3.2) is now denoted by 242 − [a, s, d] − 147. Notice that this path also features in Fig. 3.2. It can be shown (e.g. [23]) that if the heuristic H is admissible, i.e. it never overestimates the actual minimum distance travelled between two nodes, the A–Algorithm will deliver a minimum cost path if such a path exists.1In this case the A–Algorithm is referred to as an A∗–Algorithm and is termed admissible. (As the straight line distance is a minimum, the heuristic defined by Table 3.1 is admissible.) The predicate a search(+Start,+Goal,-PathFound) in asearches.pl implements the A–Algorithm. A few salient features of a search/3 will be discussed only; for details, the reader is referred to the source code which broadly follows the pattern of implementation of the blind search algorithms (Fig. 2.15, p. 65 and Fig. 2.20, p. 69). The implementation of the A–Algorithm in asearches.pl uses the built-in predicate keysort/2 to implement step 2 _ (see inset on p. 108). The module invoking a search/3 should have defined (or imported) the following predicates. • The connectivity predicate link/2 . For the network search problem, this is imported from links.pl (Fig. 2.2, p. 49). • The estimated cost defined by e cost/3 . For the network search problem, this is defined in graph a.pl by with dist/3 essentially implementing Table 3.1, The actual edge costs defined by edge cost/3 . For the network search problem, this is defined in graph a.pl by Application of the A–Algorithm to a more substantial example in Sect. 3.2 will reveal that the A–Algorithm may fail due to excessive memory requirements.2 Clearly, there is scope for improvement. In the mid 1980s, a new algorithm was conceived by Korf [20] combining the idea of Iterative Deepening (Sect. 2.6) with a heuristic evaluation function; the resulting algorithm is known as Iterative Deepening A∗ (IDA∗).3 The underlying idea is as follows. • Use Depth First as the ‘core’ of the algorithm. • Convert the core into a kind of Bounded Depth First Search with the bound (the horizon) now not being imposed on the length of the paths but on their F-values. • Finally, imbed this ‘modified’ Bounded Depth First Search into a framework which repeatedly invokes it with a sequence of increasing bounds. The corresponding sequence of bounds in Iterative Deepening was defined as a sequence of multiples of some constant increment; a unit increment in the model implementation. The approach here is more sophisticated. Now, in any given phase of the iteration, the next value of the bound is obtained as the minimum of the F-values of all those paths which had to be ignored in the present phase. This approach ensures that in the new iteration cycle the least number of paths is extended. The pseudocode of IDA∗ won’t be given here; it should be possible to reconstruct it from the above informal description. It can be shown that IDA∗ is admissible under the same assumptions as A∗. The so-called _–admissible version of IDA∗ (IDA∗–_) is a generalization of IDA∗. It is obtained by extending the F-horizon to ﻿Whereas the problems considered thus far were taken from Artificial Intelligence, we are going now to apply Prolog to problems in text processing. The present chapter is in three parts. First, the Prolog implementation is described of a tool for removing from a file sections of text situated between marker strings. (The tool is therefore a primitive static program slicer; [32] and [12].) This tool then is used in a practical context for removing sample solutions from the LATEX source code of a solved exam script. It is also shown in this context how SWI-Prolog code can be embedded into a Linux shell script. The second part addresses the question of how Prolog can be used to generate LATEX code for drawing parametric curves. Some new features of Prolog will thereby also be introduced. The final part comprises a sequence of solved Prolog exercises, implementing a tool for drawing families of parametric curves in LATEX. The exercises are of increasing complexity and finally describe how SWI-Prolog can interact with Linux through a shell script. I use LATEX on Linux for preparing examination papers. This is done in the following steps. 1. Create a LATEX source file in a text editor. 2. Translate the LATEX file into a a DVI file. 3. Translate the DVI file into a PDF file. 4. View the PDF file. These steps are performed for exam.tex by running the Linux commands in Fig. 4.1.1 Upon execution of the last line in Fig. 4.1, a new window will pop up and the exam paper may be viewed. External examiners require examination papers with model answers. I create therefore a PDF file with model solutions in the first instance where answers are appended to each subquestion. The answers are placed between some marker strings enabling me eventually to locate and remove all text between them when creating the final LATEX source leading to the printed PDF for students. It is this text removal process which is automated by the Prolog implementation to be discussed here. Write a predicate sieve(+Infile,-Outfile,+Startmarker,+Endmarker) of arity 4 for removing all text in the file named in Infile in between all occurrences of lines starting with text in Startmarker and those starting with text in Endmarker. The result should be saved in the file named in Outfile . Outfile is without marker lines. If Outfile already exists, its old version should be overwritten, if it does not exist, it should be newly created. The file shown in Fig. 4.2 is an example of Infile with the marker phrases ‘water st’ and ‘water e’, say. (The file comprises a random collection of geographical names.) After the Prolog query the file without_waters will have been created. This is shown in Fig. 4.3. The main predicate sieve/4 is defined in terms of sieve/2 , both are shown in (P-4.1). The predicates get line/1 (and its auxiliary get line/2 ), switch off/1 and switch on/1 are defined in (P-4.2). For the SWI-Prolog built-ins atom chars/2 and atom codes/2 , the reader is referred respectively to pages 126 and 19 of [9]. Noteworthy are three more built-in predicates used here: the standard Prolog predicates see/1 , seen/0 (respectively for directing the input stream to a file and redirecting it) and get char/1 for reading a character; the example below illustrates their use by reading the first three characters of the file with_waters in Fig. 4.2. The predicate get line/1 in (P-4.2) is defined in terms of get line/2 by the accumulator technique. It reads into its argument the next line from the input stream. Example: The following observations apply. 1. It is seen from the above query that a line read by get line/1 is represented as a list of the characters it is composed of. 2. By definition the last character of each line in a file is the new line character ‘\n’. That explains the line break seen in the above query. 3. Finally (not demonstrated here), each file ends with the end-of-file marker ‘end_of_file’. The one-entry list [end_of_file] is deemed to be the last line of every file by the definition in (P-4.2). • The switches switch off/0 and switch on/0 are used, writing respectively switch(off) and switch(on) in the Prolog database, respectively for removal and retention of lines from the input file. • The main predicates are sieve/4 and sieve/2 in (P-4.1), the latter defined by recursion and called by the former. sieve/4 : this is the top level predicate. 1. Line 2 opens the input file. 2. The goals in lines 3-4 in (P-4.1) make sure that the earlier version of the output file (if there is such a file) is deleted. 3. In line 5, the new output stream is opened via append/1 3. 4. In line 6, the switch is set to the position (‘off’), anticipating that initially lines will be retained. 5. In line 7, sieve/2 is invoked and processing is carried out. 6. Lines 8 and 9 close respectively output and input. sieve/2 : this is called from sieve/4 . 1. Lines 14 and 18 contain the most interesting feature of this predicate: append/3 is used in them for pattern matching. For example, the goal succeeds if the initial segment of the list Line is Start_List. 2. atom chars/2 is used in sieve/2 to disassemble the start and end markers into lists in preparation for pattern matching. 3. Notice that the built-in predicate atom codes/2 can be used in two roles as the interactive session below demonstrates. In line 16 of (P-4.1), atom codes/2 is used in its first role, i.e. to convert a list of characters to an atom. This atom is the current line, it is written to the output file. 4. Recursion is stopped in sieve/2 (and control is returned to line 8 of sieve/4 ) when the end-of-file marker is read (line 15). Imbed the Prolog implementation from Sect. 4.1.3 into a Linux shell script for providing the same functionality as the predicate sieve/4 does. The application obtained thereby will run without explicitly having to use the SWI-Prolog system. The intended behaviour of the script is illustrated in Fig. 4.4. ﻿Conciseness and accessibility of source code through declarative reading are Prolog’s major strengths. It is therefore relatively easy to appreciate the workings of someone else’s implementation, while it is much harder independently to arrive at one’s own solution to the same problem. In this chapter, we illustrate a practical methodology which is intended to overcome this discrepancy: it is a software development style that is interactive, incremental, exploratory and allows Prolog code to be arrived at in a relatively effortless manner. The task is to write a Prolog predicate rhyme/0 which displays on the screen the well-known nursery rhyme This is the House that Jack Built ([11]): In our implementation of rhyme/0 we want to exploit the rhyme’s repetitive structure and the fact that all essential information is contained in its last verse. We record the last verse in the database by verse/1 as shown in (P-4.1). The rhyme is seen roughly to match the simplified pattern shown in Fig. 4.1. Knowing the rhyme’s last verse and the above structure will allow (up to some finer detail) the rhyme to be fully reconstructed. With a view to a simplified preliminary Prolog implementation, we therefore define the following Prolog fact in the database The first task is now to define a predicate rhyme prel/2 which should enable us to obtain the skeleton rhyme’s structure in the following manner. Taking this as an informal specification of rhyme prel/2 , we want to arrive at its definition by a series of interactive experiments. What could be the least ambitious first step in implementing rhyme prel/2 ? We may for example create a list whose only entry is the last entry of the above list-of-lists. (This will correspond to reproducing the last verse.) This we do by Still interactively, a list comprising the last two entries of the target list-of-lists may be generated by Here we unify T1 with the tail of V and position it in front of V to form the new list (of lists). How do we now generate the next larger list (comprising the last three entries of the target list-of-lists)? We proceed as before except that we assemble R from the entries T2 , T1 and V (in that order!) where T2 is unified with the tail of T1 . Since our aim is to identify a recursive pattern in the above interactive session, we recast the inputs slightly by observing that [a1, • • • , an−1, an] and [a1|[a2|[a3| • • • |[an−1|[an]] • • • ]] are equivalent representations of the same list. Let’s have a look at the last two queries again. The annotated lists suggest the following pseudocode (using Prolog’s list-notation) for one single recursive step. Notice that by equations (4.1) and (4.2) we may replace the latter by The base case for the recursion is given by A straightforward implementation of the recursive step is by the (auxiliary) predicate rhyme aux/3 in (P-4.2). In the first argument of rhyme aux/3 the most recent version of the rhyme is accumulated; its second argument is a counter which is decremented from an initial value until it reaches unity at which point the third argument is instantiated to the first. It is noteworthy in the definition of rhyme aux/3 that, as a consequence of using the accumulator technique, reference to the more complex case in the recursive step is found in the rule’s body. (In this sense, as opposed to the familiar situation from imperative programming, progression is from right to left.) We find out by an experiment what the counter should be initialized to. It is seen that the second argument of rhyme aux/3 (the counter) will have to be initialized to the length of (what stands for) the last verse. This gives rise to the following first version of the predicate rhyme prel/2 which then behaves as specified on p. 119. Even though the solution thus obtained is perfectly acceptable, there is scope for improvement. Counters are commonly used in imperative programming for verifying a stopping criterion. The corresponding task in declarative programming is best achieved by pattern matching. There is indeed no need for a counter here since the information for when not to apply the recursive step (any more) can be gleaned from the pattern of the first argument of rhyme aux/3 : For the recursion to stop, the head of the list-of-lists (in the first argument) should itself be a list with exactly one entry. (The complete rhyme will have been arrived at when the first verse comprises a single line!) This idea gives rise in (P-4.3) to a new, improved (and more concise) version of the auxiliary predicate, now called rhyme aux/3 . rhyme aux 2/3 behaves as intended: The definition of a second, improved version of the preliminary rhyme predicate now simplifies to To complete the ‘skeleton version’ of the rhyme, we display the above by with the predicate show rhyme/1 defined by There is still scope for further improvement leading to an even more concise version of the auxiliary predicate. We may replace in the definition of rhyme aux 2/2 all occurrences of Head Old by [H|T], say, accounting for the fact that Head Old will be unified with a list. But then, by virtue of the first goal in the body of this rule we may replace all occurrences of Head by T. Subsequently, the first goal may be dropped. Overall, we obtain in (P-4.4) a third, even more concise version of the auxiliary predicate. There is hardly any room for improvement left save perhaps a minor simplification of the first clause. We derive an alternative boundary case by first completing the interactive session from p. 120 and then carrying out one more step: The first query suggests that we are finished if the (partially) completed skeleton rhyme’s head is a singleelement list; this condition gave rise to the earlier boundary case. On the other hand, in the second query the variable R is unified with a list whose head is empty and whose tail is the full skeleton rhyme. This suggests the following alternative first clause for rhyme aux 3/2 ,$$$﻿Conciseness and accessibility of source code through declarative reading are Prolog’s major strengths. It is therefore relatively easy to appreciate the workings of someone else’s implementation, while it is much harder independently to arrive at one’s own solution to the same problem. In this chapter, we illustrate a practical methodology which is intended to overcome this discrepancy: it is a software development style that is interactive, incremental, exploratory and allows Prolog code to be arrived at in a relatively effortless manner. The task is to write a Prolog predicate rhyme/0 which displays on the screen the well-known nursery rhyme This is the House that Jack Built ([11]): In our implementation of rhyme/0 we want to exploit the rhyme’s repetitive structure and the fact that all essential information is contained in its last verse. We record the last verse in the database by verse/1 as shown in (P-4.1). The rhyme is seen roughly to match the simplified pattern shown in Fig. 4.1. Knowing the rhyme’s last verse and the above structure will allow (up to some finer detail) the rhyme to be fully reconstructed. With a view to a simplified preliminary Prolog implementation, we therefore define the following Prolog fact in the database The first task is now to define a predicate rhyme prel/2 which should enable us to obtain the skeleton rhyme’s structure in the following manner. Taking this as an informal specification of rhyme prel/2 , we want to arrive at its definition by a series of interactive experiments. What could be the least ambitious first step in implementing rhyme prel/2 ? We may for example create a list whose only entry is the last entry of the above list-of-lists. (This will correspond to reproducing the last verse.) This we do by Still interactively, a list comprising the last two entries of the target list-of-lists may be generated by Here we unify T1 with the tail of V and position it in front of V to form the new list (of lists). How do we now generate the next larger list (comprising the last three entries of the target list-of-lists)? We proceed as before except that we assemble R from the entries T2 , T1 and V (in that order!) where T2 is unified with the tail of T1 . Since our aim is to identify a recursive pattern in the above interactive session, we recast the inputs slightly by observing that [a1, • • • , an−1, an] and [a1|[a2|[a3| • • • |[an−1|[an]] • • • ]] are equivalent representations of the same list. Let’s have a look at the last two queries again. The annotated lists suggest the following pseudocode (using Prolog’s list-notation) for one single recursive step. Notice that by equations (4.1) and (4.2) we may replace the latter by The base case for the recursion is given by A straightforward implementation of the recursive step is by the (auxiliary) predicate rhyme aux/3 in (P-4.2). In the first argument of rhyme aux/3 the most recent version of the rhyme is accumulated; its second argument is a counter which is decremented from an initial value until it reaches unity at which point the third argument is instantiated to the first. It is noteworthy in the definition of rhyme aux/3 that, as a consequence of using the accumulator technique, reference to the more complex case in the recursive step is found in the rule’s body. (In this sense, as opposed to the familiar situation from imperative programming, progression is from right to left.) We find out by an experiment what the counter should be initialized to. It is seen that the second argument of rhyme aux/3 (the counter) will have to be initialized to the length of (what stands for) the last verse. This gives rise to the following first version of the predicate rhyme prel/2 which then behaves as specified on p. 119. Even though the solution thus obtained is perfectly acceptable, there is scope for improvement. Counters are commonly used in imperative programming for verifying a stopping criterion. The corresponding task in declarative programming is best achieved by pattern matching. There is indeed no need for a counter here since the information for when not to apply the recursive step (any more) can be gleaned from the pattern of the first argument of rhyme aux/3 : For the recursion to stop, the head of the list-of-lists (in the first argument) should itself be a list with exactly one entry. (The complete rhyme will have been arrived at when the first verse comprises a single line!) This idea gives rise in (P-4.3) to a new, improved (and more concise) version of the auxiliary predicate, now called rhyme aux/3 . rhyme aux 2/3 behaves as intended: The definition of a second, improved version of the preliminary rhyme predicate now simplifies to To complete the ‘skeleton version’ of the rhyme, we display the above by with the predicate show rhyme/1 defined by There is still scope for further improvement leading to an even more concise version of the auxiliary predicate. We may replace in the definition of rhyme aux 2/2 all occurrences of Head Old by [H|T], say, accounting for the fact that Head Old will be unified with a list. But then, by virtue of the first goal in the body of this rule we may replace all occurrences of Head by T. Subsequently, the first goal may be dropped. Overall, we obtain in (P-4.4) a third, even more concise version of the auxiliary predicate. There is hardly any room for improvement left save perhaps a minor simplification of the first clause. We derive an alternative boundary case by first completing the interactive session from p. 120 and then carrying out one more step: The first query suggests that we are finished if the (partially) completed skeleton rhyme’s head is a singleelement list; this condition gave rise to the earlier boundary case. On the other hand, in the second query the variable R is unified with a list whose head is empty and whose tail is the full skeleton rhyme. This suggests the following alternative first clause for rhyme aux 3/2 ,
